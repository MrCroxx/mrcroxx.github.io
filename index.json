[{"categories":["深入浅出LevelDB"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:0:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"0. 引言 正如Rebalance与Spill之于B+Tree，Compaction操作是LSM-Tree的核心。 本节将介绍并分析LevelDB中LSM-Tree的Compaction操作的实现。 ","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:1:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"1. Compaction的类型 LevelDB中LSM-Tree的Compaction操作分为两类，分别是Minor Compaction与Major Compaction。 Minor Compaction（Immutable MemTable -\u003e SSTable）：将Immutable MemTable转储为level-0 SSTable写入。 Major Compaction（Low-level SSTable -\u003e High-level SSTable）：合并压缩第i层的SSTable，生成第i+1层的SSTable。 在LevelDB中，Major Compaction还可以按照触发条件分为三类： Size Compaction：根据每层总SSTable大小触发（level-0根据SSTable数）的Major Compaction。 Seek Compaction：根据SSTable的seek miss触发的Major Compaction。 Manual Compaction：LevelDB使用者通过接口void CompactRange(const Slice* begin, const Slice* end)手动触发。 下面我们具体分析各种Compaction的触发时机。 ","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:2:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"2. Compaction的触发 在介绍LevelDB中Compaction的触发时机前，我们先来了解一下LevelDB的后台线程。 ","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:3:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"2.1 后台线程 为了防止Compaction执行时阻塞LevelDB的正常读写，LevelDB的所有Compaction都通过一个后台线程执行。LevelDB的后台线程的实现依赖系统环境，因此其接口定义在了include/leveldb/env.h中，在不同环境中的实现分别位于util/env_windows.cc与env_posix.cc中。本文只考虑其在POSIX环境下的实现。 如果需要Compaction，LevelDB会通过如下代码调度后台线程： void DBImpl::MaybeScheduleCompaction() { //... ... env_-\u003eSchedule(\u0026DBImpl::BGWork, this); } void DBImpl::BGWork(void* db) { reinterpret_cast\u003cDBImpl*\u003e(db)-\u003eBackgroundCall(); } 该方法调用了include/leveldb/env.h中定义的Schedule接口，我们先来看看其接口定义上的注释： // Arrange to run \"(*function)(arg)\" once in a background thread. // // \"function\" may run in an unspecified thread. Multiple functions // added to the same Env may run concurrently in different threads. // I.e., the caller may not assume that background work items are // serialized. virtual void Schedule(void (*function)(void* arg), void* arg) = 0; 从该接口上的注释可以看出，该接口会安排后台线程执行一次传入的方法。且该接口既不保证后台线程仅单线程执行，也不传入的方法保序执行。 下面我们来分析Schedule及其相关方法在POSIX环境下的实现。 void PosixEnv::Schedule( void (*background_work_function)(void* background_work_arg), void* background_work_arg) { background_work_mutex_.Lock(); // Start the background thread, if we haven't done so already. if (!started_background_thread_) { started_background_thread_ = true; std::thread background_thread(PosixEnv::BackgroundThreadEntryPoint, this); background_thread.detach(); } // If the queue is empty, the background thread may be waiting for work. if (background_work_queue_.empty()) { background_work_cv_.Signal(); } background_work_queue_.emplace(background_work_function, background_work_arg); background_work_mutex_.Unlock(); } 该方法首先检测后台线程是否创建，如果没有创建创建后台线程。接下来会将任务放入后台线程的任务队列中，并通过信号量唤醒后台线程执行。创建后台线程与操作任务队列都需要通过锁来保护，因此该方法全程加锁。 下面是后台线程的逻辑： // ... ... static void BackgroundThreadEntryPoint(PosixEnv* env) { env-\u003eBackgroundThreadMain(); } // ... ... void PosixEnv::BackgroundThreadMain() { while (true) { background_work_mutex_.Lock(); // Wait until there is work to be done. while (background_work_queue_.empty()) { background_work_cv_.Wait(); } assert(!background_work_queue_.empty()); auto background_work_function = background_work_queue_.front().function; void* background_work_arg = background_work_queue_.front().arg; background_work_queue_.pop(); background_work_mutex_.Unlock(); background_work_function(background_work_arg); } } 后台线程会循环获取任务丢列中的任务，为了避免线程空转，在队列为空时通过信号量等待唤醒。如果队列中有任务，则获取该任务并将任务出队，然后执行任务。后台线程中操作队列的部分需要通过锁来保护，而执行任务时没有上锁，可以并行执行（但是LevelDB只使用了1个后台线程，因此Compaction仍是串行而不是并行的）。 ","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:3:1","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"2.2 Compaction优先级 LevelDB中Compaction具有优先级，其顺序为：Minor Compaction \u003e Manual Compaction \u003e Size Compaction \u003e Seek Compaction。本节将根据源码来分析这一优先级的体现。 无论是Minor Compaction还是Major Compaction，在设置了相应的参数后，都会通过DBImpl::MaybeScheduleCompaction方法来判断是否需要执行Compaction。该方法实现如下： void DBImpl::MaybeScheduleCompaction() { mutex_.AssertHeld(); if (background_compaction_scheduled_) { // Already scheduled } else if (shutting_down_.load(std::memory_order_acquire)) { // DB is being deleted; no more background compactions } else if (!bg_error_.ok()) { // Already got an error; no more changes } else if (imm_ == nullptr \u0026\u0026 manual_compaction_ == nullptr \u0026\u0026 !versions_-\u003eNeedsCompaction()) { // No work to be done } else { background_compaction_scheduled_ = true; env_-\u003eSchedule(\u0026DBImpl::BGWork, this); } } MaybeScheduleCompaction方法是需要在上锁时被调用的，因此其首先断言当前正持有着锁。接下来，其按照顺序做了如下判断： 当前是否正在进行Compaction的调度，如果正在调度则不再调度。这里的“调度”开始于Schedule调度后台线程前，结束于后台线程中BackgroundCompaction真正完成Compaction操作后。 数据库是否正在关闭，如果数据库已被关闭，则不再调度。 如果后台线程报告了错误，则不再调度。 如果此时还没有Immutable MemTable产生，也没有Major Compaction被触发，则不需要调度。 否则，通过Schedule方法开始新Compaction任务调度。 MaybeScheduleCompaction方法通过imm_是否为空判断是否需要Minor Compaction，通过manual_compaction判断是否需要Manual Compaction；而是否需要Size Compaction或Seek Compaction，则需要通过当前的VersionSet的NeedsCompaction方法来判断。该方法的实现如下： // Returns true iff some level needs a compaction. bool NeedsCompaction() const { Version* v = current_; return (v-\u003ecompaction_score_ \u003e= 1) || (v-\u003efile_to_compact_ != nullptr); } 该方法只检查了当前Version的两个字段：compaction_score_是否大于1或file_to_compact_是否不为空。其中compaction_score_字段用来计算是否需要触发Size Compaction，file_to_compact_用来计算是否需要触发Seek Compaction。关于这两个字段的计算会在下文介绍。 在了解LevelDB中Compaction整体的触发条件后，下面我们来分析每种Compaction具体的触发方式。 ","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:3:2","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"2.3 Minor Compaction的触发 Minor Compaction在MemTable大小超过限制时（默认为4MB）触发，LevelDB在写入变更前，首先会通过DBImpl::MakeRoomForWrite方法来在MemTable过大时将其转为Immutable MemTable，在该方法中，我们也能够找到尝试触发Compcation调度的调用。这里我们完整地看一下DBImpl::MakeRoomForWrite的实现： // REQUIRES: mutex_ is held // REQUIRES: this thread is currently at the front of the writer queue Status DBImpl::MakeRoomForWrite(bool force) { mutex_.AssertHeld(); assert(!writers_.empty()); bool allow_delay = !force; Status s; while (true) { if (!bg_error_.ok()) { // Yield previous error s = bg_error_; break; } else if (allow_delay \u0026\u0026 versions_-\u003eNumLevelFiles(0) \u003e= config::kL0_SlowdownWritesTrigger) { // We are getting close to hitting a hard limit on the number of // L0 files. Rather than delaying a single write by several // seconds when we hit the hard limit, start delaying each // individual write by 1ms to reduce latency variance. Also, // this delay hands over some CPU to the compaction thread in // case it is sharing the same core as the writer. mutex_.Unlock(); env_-\u003eSleepForMicroseconds(1000); allow_delay = false; // Do not delay a single write more than once mutex_.Lock(); } else if (!force \u0026\u0026 (mem_-\u003eApproximateMemoryUsage() \u003c= options_.write_buffer_size)) { // There is room in current memtable break; } else if (imm_ != nullptr) { // We have filled up the current memtable, but the previous // one is still being compacted, so we wait. Log(options_.info_log, \"Current memtable full; waiting...\\n\"); background_work_finished_signal_.Wait(); } else if (versions_-\u003eNumLevelFiles(0) \u003e= config::kL0_StopWritesTrigger) { // There are too many level-0 files. Log(options_.info_log, \"Too many L0 files; waiting...\\n\"); background_work_finished_signal_.Wait(); } else { // Attempt to switch to a new memtable and trigger compaction of old assert(versions_-\u003ePrevLogNumber() == 0); uint64_t new_log_number = versions_-\u003eNewFileNumber(); WritableFile* lfile = nullptr; s = env_-\u003eNewWritableFile(LogFileName(dbname_, new_log_number), \u0026lfile); if (!s.ok()) { // Avoid chewing through file number space in a tight loop. versions_-\u003eReuseFileNumber(new_log_number); break; } delete log_; delete logfile_; logfile_ = lfile; logfile_number_ = new_log_number; log_ = new log::Writer(lfile); imm_ = mem_; has_imm_.store(true, std::memory_order_release); mem_ = new MemTable(internal_comparator_); mem_-\u003eRef(); force = false; // Do not force another compaction if have room MaybeScheduleCompaction(); } } return s; } DBImpl::MakeRoomForWrite方法执行了以下功能： 通过断言确保当前持有着锁。 如果后台线程报错，退出执行。 如果当前level-0中的SSTable数即将超过最大限制（默认为8，而当level-0的SSTable数达到4时即可触发Minor Compaction），这可能是写入过快导致的。此时会开启流控，将每条写入都推迟1ms，以给Minor Compaction留出时间。如果调用该方法时参数force为true，则不会触发流控。 如果force为false且MemTable估算的大小没有超过限制（默认为4MB），则直接退出，不需要进行Minor Compaction。 如果此时有未完成Minor Compaction的Immutable MemTable，此时循环等待Minor Compaction执行完成再执行。 如果当前level-0层的SSTable数过多（默认为8），此时循环等待level-0层SSTable数低于该上限，以避免level-0层SSTable过多 否则，将当前的MemTable转为Immutable，并调用MaybeScheduleCompaction方法尝试通过后台线程调度Compcation执行（此时imm_会引用旧的MemTable，以让MaybeScheduleCompaction得知当前需要Minor Compaction）。 DBImpl::MakeRoomForWrite方法在判断是否需要进行Minor Compaction时，LevelDB通过流控与等待的方式，避免level-0层SSTable数过多。这是因为level-0层的key之间是有重叠的，因此当查询level-0层SSTable时，需要查找level-0层的所有SSTable。如果level-0层SSTable太多，会严重拖慢查询效率。 ","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:3:3","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"2.4 Size Compaction的触发 Size Compaction在非level-0层是根据该层的总SSTable大小触发的，而在level-0层是根据该层SSTable数触发的。也就是说，只有发生了Compaction，才有可能触发Size Compaction。因为Compaction的执行会导致Version的更新，因此LevelDB在VersionSet::LogAndApply方法更新Version后，让其调用VersionSet::Finalize方法来计算每层SSTable是否需要Size Compaction，并选出最需要进行Size Compaction的层作为下次Size Compaction的目标。 VersionSet::Finalize方法实现如下： void VersionSet::Finalize(Version* v) { // Precomputed best level for next compaction int best_level = -1; double best_score = -1; for (int level = 0; level \u003c config::kNumLevels - 1; level++) { double score; if (level == 0) { // We treat level-0 specially by bounding the number of files // instead of number of bytes for two reasons: // // (1) With larger write-buffer sizes, it is nice not to do too // many level-0 compactions. // // (2) The files in level-0 are merged on every read and // therefore we wish to avoid too many files when the individual // file size is small (perhaps because of a small write-buffer // setting, or very high compression ratios, or lots of // overwrites/deletions). score = v-\u003efiles_[level].size() / static_cast\u003cdouble\u003e(config::kL0_CompactionTrigger); } else { // Compute the ratio of current size to size limit. const uint64_t level_bytes = TotalFileSize(v-\u003efiles_[level]); score = static_cast\u003cdouble\u003e(level_bytes) / MaxBytesForLevel(options_, level); } if (score \u003e best_score) { best_level = level; best_score = score; } } v-\u003ecompaction_level_ = best_level; v-\u003ecompaction_score_ = best_score; } 该方法计算了每层需要Size Compaction的score，并选出score最大的层作为下次Size Compaction的目标（如果score小于1，会被MaybeScheduleCompaction方法忽略）。其计算依据为： 对于level-0，计算该层SSTable数与应触发level-0 Compaction的SSTable数的比值（默认为4）作为score。 对于非level-0，计算该层SSTable总大小与该层预设大小的比值作为score。level-1层的预设大小为10MB，之后每层依次*10。 计算完score后，需要等待Size Compaction的触发。Size Compaction的触发发生在后台线程调用的DBImpl::BackgroundCall方法中。该方法在完成Compaction操作后，会再次调用MaybeScheduleCompaction方法，来触发因上次Compaction而需要的Size Compaction操作。 void DBImpl::BackgroundCall() { // ... ... // Previous compaction may have produced too many files in a level, // so reschedule another compaction if needed. MaybeScheduleCompaction(); background_work_finished_signal_.SignalAll(); } ","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:3:4","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"2.5 Seek Compaction的触发 在介绍Seek Compaction触发条件前，我们先来看为什么需要Seek Compaction。 在LSM-Tree中，除了level-0外，虽然每个level的SSTable间相互没有overlap，但是level与level间的SSTable是可以有overlap的，如下图中的实例所示。 overlapoverlap \" overlap 在本例中，如果查找键18时在level-k前都没有命中，则查询会下推到level-k。在level-k层中，因为SSTable(k, i)的key范围覆盖了18，LevelDB会在该SSTable中查找是否存在要查找的key 18（实际上查找的是该SSTable在TableCache中的filter），该操作被称为“seek”。当LevelDB在level-k中没有找到要查找的key时，才会继续在level-(k+1)中查找。 seek missseek miss \" seek miss 在上图的示例中，每当LevelDB要查找key 18时，因为SSTable(k, i)的key范围覆盖了18，所以其每次都必须在该SSTable中seek，这一不必要的seek操作会导致性能下降。因此，在FileMetaData结构体中引入了allowed_seeks字段，该字段初始为文件大小与16KB的比值，不足100则取100；每次无效seek发生时LevelDB都会将该字段值减1。当某SSTable的allowed_seeks减为0时，会触发seek compaction，该SSTable会与下层部分SSTable合并。合并后的SSTable如下图所示。 matchmatch \" match 引文 allow_seeks字段初始值取值原因： // We arrange to automatically compact this file after // a certain number of seeks. Let's assume: // (1) One seek costs 10ms // (2) Writing or reading 1MB costs 10ms (100MB/s) // (3) A compaction of 1MB does 25MB of IO: // 1MB read from this level // 10-12MB read from next level (boundaries may be misaligned) // 10-12MB written to next level // This implies that 25 seeks cost the same as the compaction // of 1MB of data. I.e., one seek costs approximately the // same as the compaction of 40KB of data. We are a little // conservative and allow approximately one seek for every 16KB // of data before triggering a compaction. 合并后，当LevelDB需要查找key 18时，在level-k中便没有了覆盖key 18的SSTable，因此会直接在level-(k+1)中找到该key所在的SSTable。这样便避免这次无效的seek。 因为Seek Compcation的触发需要在SSTable上seek，因此我们从DBImpl::Get方法查找SSTable时开始分析。由于LevelDB的查找操作涉及到多层，笔者将在本系列的后续文章中详细介绍其流程，本文尽可能屏蔽目前不需要的细节。 Status DBImpl::Get(const ReadOptions\u0026 options, const Slice\u0026 key, std::string* value) { // ... ... Version::GetStats stats; // Unlock while reading from files and memtables { mutex_.Unlock(); // First look in the memtable, then in the immutable memtable (if any). LookupKey lkey(key, snapshot); if (mem-\u003eGet(lkey, value, \u0026s)) { // Done } else if (imm != nullptr \u0026\u0026 imm-\u003eGet(lkey, value, \u0026s)) { // Done } else { s = current-\u003eGet(options, lkey, value, \u0026stats); have_stat_update = true; } mutex_.Lock(); } if (have_stat_update \u0026\u0026 current-\u003eUpdateStats(stats)) { MaybeScheduleCompaction(); } } 当LevelDB查找key时，会记录一些统计信息。当在SSTable上发生查找时，会记录发生seek miss的 SSTable，这样会更新Version中其相应的FileMetaData中的allowed_seeks字段，并通过MaybeScheduleCompaction检查是否需要触发Seek Compaction。 // Lookup the value for key. If found, store it in *val and // return OK. Else return a non-OK status. Fills *stats. // REQUIRES: lock is not held struct GetStats { FileMetaData* seek_file; int seek_file_level; }; // ... ... Status Version::Get(const ReadOptions\u0026 options, const LookupKey\u0026 k, std::string* value, GetStats* stats) { stats-\u003eseek_file = nullptr; stats-\u003eseek_file_level = -1; struct State { Saver saver; GetStats* stats; const ReadOptions* options; Slice ikey; FileMetaData* last_file_read; int last_file_read_level; VersionSet* vset; Status s; bool found; static bool Match(void* arg, int level, FileMetaData* f) { State* state = reinterpret_cast\u003cState*\u003e(arg); if (state-\u003estats-\u003eseek_file == nullptr \u0026\u0026 state-\u003elast_file_read != nullptr) { // We have had more than one seek for this read. Charge the 1st file. state-\u003estats-\u003eseek_file = state-\u003elast_file_read; state-\u003estats-\u003eseek_file_level = state-\u003elast_file_read_level; } state-\u003elast_file_read = f; state-\u003elast_file_read_level = level; state-\u003es = state-\u003evset-\u003etable_cache_-\u003eGet(*state-\u003eoptions, f-\u003enumber, f-\u003efile_size, state-\u003eikey, \u0026state-\u003esaver, SaveValue); if (!state-\u003es.ok()) { state-\u003efound = true; return false; } switch (state-\u003esaver.state) { case kNotFound: return true; // Keep searching in other files case kFound: state-\u003efound = true; return false; case kDeleted: return false; case kCorrupt: state-\u003es = Status::Corruption(\"corrupted key for \", state-\u003esaver.user_key); state-\u003efound = true; return false; } // Not reached. Added to avoid false compilation warnings of // \"control reaches end of non-void function\". return false; } }; State st","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:3:5","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"2.6 Manual Compaction的触发 Manual Comapction的触发时机比较简单，当LevelDB的用户调用DB::CompactRange接口时，LevelDB会检查用户给定的Compact范围与当前状态，判断是否需要执行Manual Compaction。如果确定执行Manual Compaction，则设置manual_compaction_，再调用 MaybeScheduleCompaction方法以尝试触发Manual Compaction。 ","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:3:6","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"3. Compaction的范围 Compaction在触发后，首先要确定Compact的范围。本节，笔者将介绍并分析LevelDB中Comapction范围的确定。 LevelDB在确定Minor Compaction范围与Major Compaction范围的方法区别很大，因此这里分别介绍。 ","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:4:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"3.1 Minor Compaction范围的确定 在LST-Tree的基本概念中，Minor Compaction只需要将Immutable MemTable全量转储为SSTable，并将其推至level-0即可。而LevelDB对这一步骤进行了优化，其会将Minor Comapction生成的SSTable推至更高的层级。该优化的依据如下： 由于level 0中SSTable间可能存在overlap，发生在level 0=\u003e1的Major Compaction开销相对较大。为了尽可能避免level 0=\u003e1的Major Compaction开销并避免一些开销较大的Manifest文件操作，LevelDB会将Minor Comapction产生的MemTable尽可能推至更高level。 LevelDB也不会将Minor Compaction产生的SSTable的level推得过高。SSTable的level越高越难被Compaction，因此如果该SSTable中很多Record是override操作，如果不被Compaction会造成很大的空间浪费。 该优化不能破坏LSM-Tree结构。 因此计算Minor Compaction范围时需要且只需要确定其生成的SSTable所在的level。其通过Version::PickLevelForMemTableOutput方法实现： int Version::PickLevelForMemTableOutput(const Slice\u0026 smallest_user_key, const Slice\u0026 largest_user_key) { int level = 0; if (!OverlapInLevel(0, \u0026smallest_user_key, \u0026largest_user_key)) { // Push to next level if there is no overlap in next level, // and the #bytes overlapping in the level after that are limited. InternalKey start(smallest_user_key, kMaxSequenceNumber, kValueTypeForSeek); InternalKey limit(largest_user_key, 0, static_cast\u003cValueType\u003e(0)); std::vector\u003cFileMetaData*\u003e overlaps; while (level \u003c config::kMaxMemCompactLevel) { if (OverlapInLevel(level + 1, \u0026smallest_user_key, \u0026largest_user_key)) { break; } if (level + 2 \u003c config::kNumLevels) { // Check that file does not overlap too many grandparent bytes. GetOverlappingInputs(level + 2, \u0026start, \u0026limit, \u0026overlaps); const int64_t sum = TotalFileSize(overlaps); if (sum \u003e MaxGrandParentOverlapBytes(vset_-\u003eoptions_)) { break; } } level++; } } return level; } PickLevelForMemTableOutput方法最初将目标level置为0，并循环判断是否可以将该level推高一层至目标level。其判断条件如下： 目标level不能超过配置config::kMaxMemCompactLevel中限制的最大高度（默认为2）。 目标level不能与该level的其它SSTable有overlap。 目标level与其下一层level的overlap不能过多，其计算规则为：首先根据Immutable MemTable的key范围找出目标level的下一层level中与其存在overlap的所有文件；所有与之存在overlap的文件总大小不能超过LevelDB配置中max_file_size大小的10倍（默认为2MB）。 如果满足以上所有条件，则将目标level推至下一层并继续循环。 ","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:4:1","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"3.2 Major Compaction LevelDB在进行Major Compaction时，至少需要确定以下参数： 确定Compaction起始层级i。 确定level-i层SSTable input。 确定level-(i+1)层中与待Compact的SSTable有overlap的SSTable input。 Major Compation生成的SSTable的level即为level-(i+1)。 由于三种Major Compaction的起始条件与目标都不同，其确定这三个参数的方式稍有不同。本节笔者将介绍并分析各种Major Compaction确定Compaction范围的方法与实现。 3.2.1 Major Compaction元数据 LevelDB通过Compaction类（位于db/version_set.h）记录Major Compaction所需元数据： // A Compaction encapsulates information about a compaction. class Compaction { // ... ... private: friend class Version; friend class VersionSet; Compaction(const Options* options, int level); int level_; uint64_t max_output_file_size_; Version* input_version_; VersionEdit edit_; // Each compaction reads inputs from \"level_\" and \"level_+1\" std::vector\u003cFileMetaData*\u003e inputs_[2]; // The two sets of inputs // State used to check for number of overlapping grandparent files // (parent == level_ + 1, grandparent == level_ + 2) std::vector\u003cFileMetaData*\u003e grandparents_; size_t grandparent_index_; // Index in grandparent_starts_ bool seen_key_; // Some output key has been seen int64_t overlapped_bytes_; // Bytes of overlap between current output // and grandparent files // State for implementing IsBaseLevelForKey // level_ptrs_ holds indices into input_version_-\u003elevels_: our state // is that we are positioned at one of the file ranges for each // higher level than the ones involved in this compaction (i.e. for // all L \u003e= level_ + 2). size_t level_ptrs_[config::kNumLevels]; }; 本节中我们主要关注以下字段： level：Major Compaction的起始level（即上述level-i）。 input[0]：level-i层需要Compact的SSTable编号。 input[1]：level-(i+1)层需要Compact的SSTable编号。 3.2.2 Size Compaction与Seek Compaction LevelDB在触发Size Compaction时，已知Compaction的起始层级i；而LevelDB在触发Seek Compaction时，已知Compaction的起始层级i和level-i层的输入SSTable。LevelDB通过VersionSet::PickCompaction方法来计算其它参数： Compaction* VersionSet::PickCompaction() { Compaction* c; int level; // We prefer compactions triggered by too much data in a level over // the compactions triggered by seeks. const bool size_compaction = (current_-\u003ecompaction_score_ \u003e= 1); const bool seek_compaction = (current_-\u003efile_to_compact_ != nullptr); if (size_compaction) { level = current_-\u003ecompaction_level_; assert(level \u003e= 0); assert(level + 1 \u003c config::kNumLevels); c = new Compaction(options_, level); // Pick the first file that comes after compact_pointer_[level] for (size_t i = 0; i \u003c current_-\u003efiles_[level].size(); i++) { FileMetaData* f = current_-\u003efiles_[level][i]; if (compact_pointer_[level].empty() || icmp_.Compare(f-\u003elargest.Encode(), compact_pointer_[level]) \u003e 0) { c-\u003einputs_[0].push_back(f); break; } } if (c-\u003einputs_[0].empty()) { // Wrap-around to the beginning of the key space c-\u003einputs_[0].push_back(current_-\u003efiles_[level][0]); } } else if (seek_compaction) { level = current_-\u003efile_to_compact_level_; c = new Compaction(options_, level); c-\u003einputs_[0].push_back(current_-\u003efile_to_compact_); } else { return nullptr; } c-\u003einput_version_ = current_; c-\u003einput_version_-\u003eRef(); // Files in level 0 may overlap each other, so pick up all overlapping ones if (level == 0) { InternalKey smallest, largest; GetRange(c-\u003einputs_[0], \u0026smallest, \u0026largest); // Note that the next call will discard the file we placed in // c-\u003einputs_[0] earlier and replace it with an overlapping set // which will include the picked file. current_-\u003eGetOverlappingInputs(0, \u0026smallest, \u0026largest, \u0026c-\u003einputs_[0]); assert(!c-\u003einputs_[0].empty()); } SetupOtherInputs(c); return c; } 对于Size Compaction，level-i层的SSTable输入根据该level的Compaction Pointer（记录在Version中），选取上次Compaction后的第一个SSTable（如果该层还没发生过Compaction）。这是为了尽可能公平地为Size Compaction选取SSTable，避免某些SSTable永远不会被Compact。 对于Seek Compaction，该方法直接将触发Seek Compaction的SSTable加入到level-i层的输入中。 如果触发Compact的SSTable在level-0，PickCompaction方法会将level-0层中所有与该SSTable有overlap的SSTable加入level-0层的输入中。 在确定了input[0]后，PickCompcation方法会调用VersionSet::SetupOtherInputs方法。该方法首先扩展input[0]范围，并确定input[1]，即参与Major Compaction的level-(i+1)层的SSTable。同样，VersionSet::SetupOtherInputs也会扩展input[1]的范围。扩展input范围的目的是","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:4:2","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"x. Minor Compaction ","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:5:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"x. Major Compaction DBImpl::BackgroundCompaction DBImplCompactMemTable btw. Tier Compaction ( Tiering vs. Leveling ) ","date":"2021-03-11","objectID":"/posts/code-reading/leveldb-made-simple/8-compaction/:6:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x08 Compaction [施工中]","uri":"/posts/code-reading/leveldb-made-simple/8-compaction/"},{"categories":["深入浅出LevelDB"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2021-03-10","objectID":"/posts/code-reading/leveldb-made-simple/7-cache/:0:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x07 Cache","uri":"/posts/code-reading/leveldb-made-simple/7-cache/"},{"categories":["深入浅出LevelDB"],"content":"0. 引言 为了减少热点数据访问时磁盘I/O频繁导致的效率问题，LevelDB在访问SSTable时加入了缓存。LevelDB中使用的缓存从功能上可分为两种： BlockCache：缓存最近使用的SSTable中DataBlock数据。 TableCache：缓存最近打开的SSTable中的部分元数据（如索引等）。 无论是BlockCache还是TableCache，其内部的核心实现都是LRU缓存（Least-Recently-Used）。该LRU缓存实现了include/leveldb/cache.h定义的缓存接口。 本文主要介绍并分析LevelDB中Cache的设计与实现。关于BlockCache与TableCache的使用，将在本系列后续文章介绍LevelDB读写时介绍。 ","date":"2021-03-10","objectID":"/posts/code-reading/leveldb-made-simple/7-cache/:1:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x07 Cache","uri":"/posts/code-reading/leveldb-made-simple/7-cache/"},{"categories":["深入浅出LevelDB"],"content":"1.1 Cache接口 LevelDB的include/leveldb/cache.h定义了其内部使用的缓存接口，在介绍LevelDB中LRU缓存的实现前，我们首先关注该文件中定义的缓存接口： // Copyright (c) 2011 The LevelDB Authors. All rights reserved. // Use of this source code is governed by a BSD-style license that can be // found in the LICENSE file. See the AUTHORS file for names of contributors. // // A Cache is an interface that maps keys to values. It has internal // synchronization and may be safely accessed concurrently from // multiple threads. It may automatically evict entries to make room // for new entries. Values have a specified charge against the cache // capacity. For example, a cache where the values are variable // length strings, may use the length of the string as the charge for // the string. // // A builtin cache implementation with a least-recently-used eviction // policy is provided. Clients may use their own implementations if // they want something more sophisticated (like scan-resistance, a // custom eviction policy, variable cache sizing, etc.) #ifndef STORAGE_LEVELDB_INCLUDE_CACHE_H_ #define STORAGE_LEVELDB_INCLUDE_CACHE_H_ #include \u003ccstdint\u003e #include \"leveldb/export.h\"#include \"leveldb/slice.h\" namespace leveldb { class LEVELDB_EXPORT Cache; // Create a new cache with a fixed size capacity. This implementation // of Cache uses a least-recently-used eviction policy. LEVELDB_EXPORT Cache* NewLRUCache(size_t capacity); class LEVELDB_EXPORT Cache { public: Cache() = default; Cache(const Cache\u0026) = delete; Cache\u0026 operator=(const Cache\u0026) = delete; // Destroys all existing entries by calling the \"deleter\" // function that was passed to the constructor. virtual ~Cache(); // Opaque handle to an entry stored in the cache. struct Handle {}; // Insert a mapping from key-\u003evalue into the cache and assign it // the specified charge against the total cache capacity. // // Returns a handle that corresponds to the mapping. The caller // must call this-\u003eRelease(handle) when the returned mapping is no // longer needed. // // When the inserted entry is no longer needed, the key and // value will be passed to \"deleter\". virtual Handle* Insert(const Slice\u0026 key, void* value, size_t charge, void (*deleter)(const Slice\u0026 key, void* value)) = 0; // If the cache has no mapping for \"key\", returns nullptr. // // Else return a handle that corresponds to the mapping. The caller // must call this-\u003eRelease(handle) when the returned mapping is no // longer needed. virtual Handle* Lookup(const Slice\u0026 key) = 0; // Release a mapping returned by a previous Lookup(). // REQUIRES: handle must not have been released yet. // REQUIRES: handle must have been returned by a method on *this. virtual void Release(Handle* handle) = 0; // Return the value encapsulated in a handle returned by a // successful Lookup(). // REQUIRES: handle must not have been released yet. // REQUIRES: handle must have been returned by a method on *this. virtual void* Value(Handle* handle) = 0; // If the cache contains entry for key, erase it. Note that the // underlying entry will be kept around until all existing handles // to it have been released. virtual void Erase(const Slice\u0026 key) = 0; // Return a new numeric id. May be used by multiple clients who are // sharing the same cache to partition the key space. Typically the // client will allocate a new id at startup and prepend the id to // its cache keys. virtual uint64_t NewId() = 0; // Remove all cache entries that are not actively in use. Memory-constrained // applications may wish to call this method to reduce memory usage. // Default implementation of Prune() does nothing. Subclasses are strongly // encouraged to override the default implementation. A future release of // leveldb may change Prune() to a pure abstract method. virtual void Prune() {} // Return an estimate of the combined charges of all elements stored in the // cache. virtual size_t TotalCharge() const = 0; private: void LRU_Remove(Handle* e); void LRU_Append(Handle* e); void Unref(Handle* e); struct Rep; Rep* rep_; }; } // namespace","date":"2021-03-10","objectID":"/posts/code-reading/leveldb-made-simple/7-cache/:2:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x07 Cache","uri":"/posts/code-reading/leveldb-made-simple/7-cache/"},{"categories":["深入浅出LevelDB"],"content":"2. Cache的实现 ","date":"2021-03-10","objectID":"/posts/code-reading/leveldb-made-simple/7-cache/:3:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x07 Cache","uri":"/posts/code-reading/leveldb-made-simple/7-cache/"},{"categories":["深入浅出LevelDB"],"content":"2.1 LevelDB中LRU缓存设计 LevelDB中内建了一个Cache接口的实现，其位于util/cache.cc中。接下来，笔者将介绍LevelDB中Cache实现的设计与源码实现。 通过该文件开头的注释，我们能够对其实现有一个初步的认识： // LRU cache implementation // // Cache entries have an \"in_cache\" boolean indicating whether the cache has a // reference on the entry. The only ways that this can become false without the // entry being passed to its \"deleter\" are via Erase(), via Insert() when // an element with a duplicate key is inserted, or on destruction of the cache. // // The cache keeps two linked lists of items in the cache. All items in the // cache are in one list or the other, and never both. Items still referenced // by clients but erased from the cache are in neither list. The lists are: // - in-use: contains the items currently referenced by clients, in no // particular order. (This list is used for invariant checking. If we // removed the check, elements that would otherwise be on this list could be // left as disconnected singleton lists.) // - LRU: contains the items not currently referenced by clients, in LRU order // Elements are moved between these lists by the Ref() and Unref() methods, // when they detect an element in the cache acquiring or losing its only // external reference. LevelDB的Cache实现中有两个用来保存缓存项LRUHandle的链表：in-use链表和LRU链表。in-use链表上无序保存着在LRUCache中且正在被client使用的LRUHandle（该链表仅用来保持LRUHandle引用计数）；LRU链表上按照最近使用的顺序保存着当前在LRUCache中但目前没有被用户使用的LRUHandle。LRUHandle在两个链表间的切换由Ref和UnRef实现。 另外，在LRUCache的实现中，在Insert方法插入LRUHandle时，只会从LRU链表中逐出LRUHandle，相当于in-use链表中的LRUHandle会被LRUCache “Pin”住，永远都不会逐出。也就是说，对于LRUCache中的每个LRUHandle，其只有如下几种状态： 对于还没存入LRUCache的LRUHandle，不在任一链表上（显然）。 当前在LRUCache中，且正在被client使用的LRUHandle，在in-use链表上无序保存。 当前在LRUCache中，当前未被client使用的LRUHandle，在LRU链表上按LRU顺序保存。 之前在LRUCache中，但①被用户通过Erase方法从LRUCache中删除，或②用户通过Insert方法更新了该key的LRUHandle，或③LRUCache被销毁时，LRUHandle既不在in-use链表上也不在LRU链表上。此时，该LRUHandle在等待client通过Release方法释放引用计数以销毁。 LRUCache为了能够快速根据key来找到相应的LRUHandle，而不需要遍历链表，其还组装了一个哈希表HandleTable。LevelDB的哈希表与哈希函数都使用了自己的实现。 LRUCache其实已经实现了完整的LRU缓存的功能。但是LevelDB又将其封装为ShardedLRUCache，并让ShardedLRUCache实现了Cache接口。ShardedLRUCache中保存了若干个LRUCache，并根据插入的key的哈希将其分配到相应的LRUCache中。因为每个LRUCache有独立的锁，这种方式可以减少锁的争用，以优化并行程序的性能。 接下来，我们自底向上地介绍并分析LevelDB中LRUHandle、HandleTable、LRUCache、ShardedLRUCache的实现。 ","date":"2021-03-10","objectID":"/posts/code-reading/leveldb-made-simple/7-cache/:3:1","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x07 Cache","uri":"/posts/code-reading/leveldb-made-simple/7-cache/"},{"categories":["深入浅出LevelDB"],"content":"2.2 LRUHandle LRUHandle是表示缓存项的结构体，其源码如下： // An entry is a variable length heap-allocated structure. Entries // are kept in a circular doubly linked list ordered by access time. struct LRUHandle { void* value; void (*deleter)(const Slice\u0026, void* value); LRUHandle* next_hash; LRUHandle* next; LRUHandle* prev; size_t charge; // TODO(opt): Only allow uint32_t? size_t key_length; bool in_cache; // Whether entry is in the cache. uint32_t refs; // References, including cache reference, if present. uint32_t hash; // Hash of key(); used for fast sharding and comparisons char key_data[1]; // Beginning of key Slice key() const { // next_ is only equal to this if the LRU handle is the list head of an // empty list. List heads never have meaningful keys. assert(next != this); return Slice(key_data, key_length); } }; LRUHandle中有记录key（深拷贝）、value（浅拷贝）及相关哈希值、引用计数、占用空间、是否仍在LRUCache中等字段，这里不再赘述。我们主要关注LRUHandle中的三个LRUHandle*类型的指针。其中next指针与prev指针，用来实现LRUCache中的两个链表，而next_hash是哈希表HandleTable为了解决哈希冲突采用拉链法的链指针。 ","date":"2021-03-10","objectID":"/posts/code-reading/leveldb-made-simple/7-cache/:3:2","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x07 Cache","uri":"/posts/code-reading/leveldb-made-simple/7-cache/"},{"categories":["深入浅出LevelDB"],"content":"2.3 HandleTable 接下来我们来分析HandleTable的实现。HandleTable实现了一个可扩展哈希表。HandleTable中只有3个字段： private: // The table consists of an array of buckets where each bucket is // a linked list of cache entries that hash into the bucket. uint32_t length_; uint32_t elems_; LRUHandle** list_; length_字段记录了HandleTable中solt的数量，elems_字段记录了当前HandleTable中已用solt的数量，list_字段是HandleTable的bucket数组。 接下来我们简单分析一下HandleTable对可扩展哈希表的实现。 // Return a pointer to slot that points to a cache entry that // matches key/hash. If there is no such cache entry, return a // pointer to the trailing slot in the corresponding linked list. LRUHandle** FindPointer(const Slice\u0026 key, uint32_t hash) { LRUHandle** ptr = \u0026list_[hash \u0026 (length_ - 1)]; while (*ptr != nullptr \u0026\u0026 ((*ptr)-\u003ehash != hash || key != (*ptr)-\u003ekey())) { ptr = \u0026(*ptr)-\u003enext_hash; } return ptr; } FindPointer方法是根据key与其hash查找LRUHandle的方法。如果key存在则返回其LRUHandle的指针，如果不存在则返回指向可插入的solt的指针。 void Resize() { uint32_t new_length = 4; while (new_length \u003c elems_) { new_length *= 2; } LRUHandle** new_list = new LRUHandle*[new_length]; memset(new_list, 0, sizeof(new_list[0]) * new_length); uint32_t count = 0; for (uint32_t i = 0; i \u003c length_; i++) { LRUHandle* h = list_[i]; while (h != nullptr) { LRUHandle* next = h-\u003enext_hash; uint32_t hash = h-\u003ehash; LRUHandle** ptr = \u0026new_list[hash \u0026 (new_length - 1)]; h-\u003enext_hash = *ptr; *ptr = h; h = next; count++; } } assert(elems_ == count); delete[] list_; list_ = new_list; length_ = new_length; } Resize方法是扩展哈希表的方法。该方法会倍增solt大小，并重新分配空间。在重新分配solt的空间后，再对所有原有solt中的LRUHandle重哈希。最后释放旧的solt的空间。 LRUHandle* Lookup(const Slice\u0026 key, uint32_t hash) { return *FindPointer(key, hash); } LRUHandle* Insert(LRUHandle* h) { LRUHandle** ptr = FindPointer(h-\u003ekey(), h-\u003ehash); LRUHandle* old = *ptr; h-\u003enext_hash = (old == nullptr ? nullptr : old-\u003enext_hash); *ptr = h; if (old == nullptr) { ++elems_; if (elems_ \u003e length_) { // Since each cache entry is fairly large, we aim for a small // average linked list length (\u003c= 1). Resize(); } } return old; } LRUHandle* Remove(const Slice\u0026 key, uint32_t hash) { LRUHandle** ptr = FindPointer(key, hash); LRUHandle* result = *ptr; if (result != nullptr) { *ptr = result-\u003enext_hash; --elems_; } return result; } HandleTable公开的LookUp、Insert、Remove是对FindPointer与Resize的封装，这里不再赘述。 ","date":"2021-03-10","objectID":"/posts/code-reading/leveldb-made-simple/7-cache/:3:3","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x07 Cache","uri":"/posts/code-reading/leveldb-made-simple/7-cache/"},{"categories":["深入浅出LevelDB"],"content":"2.4 LRUCache class LRUCache { // ... ... private: // ... ... // Initialized before use. size_t capacity_; // mutex_ protects the following state. mutable port::Mutex mutex_; size_t usage_ GUARDED_BY(mutex_); // Dummy head of LRU list. // lru.prev is newest entry, lru.next is oldest entry. // Entries have refs==1 and in_cache==true. LRUHandle lru_ GUARDED_BY(mutex_); // Dummy head of in-use list. // Entries are in use by clients, and have refs \u003e= 2 and in_cache==true. LRUHandle in_use_ GUARDED_BY(mutex_); HandleTable table_ GUARDED_BY(mutex_); }; LRUCache中，除了容量capacity_外，其它字段都通过互斥锁mutex_来保护并发操作，这些字段包括：LRUCache的当前用量、LRU链表lru_、in-use链表in_use_、和哈希表table_。 void LRUCache::Ref(LRUHandle* e) { if (e-\u003erefs == 1 \u0026\u0026 e-\u003ein_cache) { // If on lru_ list, move to in_use_ list. LRU_Remove(e); LRU_Append(\u0026in_use_, e); } e-\u003erefs++; } void LRUCache::Unref(LRUHandle* e) { assert(e-\u003erefs \u003e 0); e-\u003erefs--; if (e-\u003erefs == 0) { // Deallocate. assert(!e-\u003ein_cache); (*e-\u003edeleter)(e-\u003ekey(), e-\u003evalue); free(e); } else if (e-\u003ein_cache \u0026\u0026 e-\u003erefs == 1) { // No longer in use; move to lru_ list. LRU_Remove(e); LRU_Append(\u0026lru_, e); } } void LRUCache::LRU_Remove(LRUHandle* e) { e-\u003enext-\u003eprev = e-\u003eprev; e-\u003eprev-\u003enext = e-\u003enext; } void LRUCache::LRU_Append(LRUHandle* list, LRUHandle* e) { // Make \"e\" newest entry by inserting just before *list e-\u003enext = list; e-\u003eprev = list-\u003eprev; e-\u003eprev-\u003enext = e; e-\u003enext-\u003eprev = e; } 除了LRU_Remove与LRU_Append方法用来操作链表外，LRUCache还提供了Ref与Unref方法，在操作链表的同时处理LRUHandle的引用计数。Ref方法将LRUHandle的引用计数加一，并将其从lru_链表中转移到in_use_链表中；Unref方法将引用计数减一，当LRUHandle的引用计数减为1时，将其从in_use_链表中归还给lru_链表（其最后一个引用为链表指针的引用），当LRUHandle的引用计数减为0时，通过其deleter销毁该对象。 LRUCache中其它的方法实现比较简单，这里不再赘述。 ","date":"2021-03-10","objectID":"/posts/code-reading/leveldb-made-simple/7-cache/:3:4","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x07 Cache","uri":"/posts/code-reading/leveldb-made-simple/7-cache/"},{"categories":["深入浅出LevelDB"],"content":"2.5 ShardedLRUCache SharedLRUCache是最终实现Cache接口的方法。正如前文所介绍的，ShardedLRUCache中保存了若干个LRUCache，并根据插入的key的哈希将其分配到相应的LRUCache中。因为每个LRUCache有独立的锁，这种方式可以减少锁的争用，以优化并行程序的性能。 class ShardedLRUCache : public Cache { private: LRUCache shard_[kNumShards]; port::Mutex id_mutex_; uint64_t last_id_; static inline uint32_t HashSlice(const Slice\u0026 s) { return Hash(s.data(), s.size(), 0); } static uint32_t Shard(uint32_t hash) { return hash \u003e\u003e (32 - kNumShardBits); } // ... ... } ShardedLRUCache通过HashSlice方法对key进行一次哈希，并通过Shard方法为其分配shard。ShardedLRUCache中其它方法都是对shard的操作与对LRUCache的封装，这里也不再赘述。 ","date":"2021-03-10","objectID":"/posts/code-reading/leveldb-made-simple/7-cache/:3:5","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x07 Cache","uri":"/posts/code-reading/leveldb-made-simple/7-cache/"},{"categories":["深入浅出LevelDB"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2021-03-07","objectID":"/posts/code-reading/leveldb-made-simple/6-version/:0:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x06 Version","uri":"/posts/code-reading/leveldb-made-simple/6-version/"},{"categories":["深入浅出LevelDB"],"content":"0. 引言 在介绍LevelDB中“Version”的设计前，我们先来了解LevelDB中的MVCC。 MVCC是存储系统中常用来优化“读读并发”与“读写并发”并行的设计，LevelDB也不例外。LevelDB的MVCC机制不但使其能够并行化“读读并发”与“读写并发”操作，还使其能够实现快照读（Snapshot Read）。LevelDB的用户可以通过其提供的接口要求其保留一定时间之前的快照，在用户释放快照前，该快照创建时LevelDB中存在的数据就不会被释放。 《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》一文介绍了MVCC在内存中的实现(这里放上笔者之前的翻译版《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译)。这篇综述剖析的MVCC实现中的层次与每个层次的常见方案。这里我们主要关注LevelDB的MVCC实现中的“版本存储”的设计。 想要实现MVCC，存储系统就要存储数据的多个版本。笔者将LevelDB的多版本存储设计分为了三个层次： 从key/value的角度： 每次变更操作的记录（Batch Writer可视为一次操作）都有不同且递增的SequenceNumber。对于一个UserKey，当存在SequenceNumber更高的的记录时，旧的记录不会被立即删除，至少要在该SequenceNumber之前的所有Snapshot都被释放后才能删除（具体删除时间与Compaction时间有关）。这是LevelDB实现Snapshot Read的基础。 从MemTable的角度： LevelDB中的MemTable通过引用计数来控制释放时间。在需要读取MemTable时（无论是Get操作还是Minor Compaction时），读取前会增大其引用计数，读取后减小引用计数。这样，即使MemTable已被通过Minor Compaction操作写入到Level-0文件，MemTable在被读取，它就不会被释放。 从数据库文件的角度： LevelDB的文件同样需要引用计数，当执行Major Compaction时，LevelDB不会立即删除已被合并的数据库文件，因为此时可能还有未完成的读取该文件的操作。 key/value的版本实际上也是依赖于内存与稳定存储，其分别在Compaction与Put/Get操作中体现，因此这里我们主要关注后两者。MemTable的多版本与Snapshot信息是不需要直接持久化的，因为数据库关闭时无法进行Snapshot Read，也就没有了Snapshot的概念，而最新的MemTable会通过WAL重建，旧的MemTable也不再会被依赖。而数据库文件则不同，LevelDB必须记录数据库文件的版本信息，否则在数据库重启时无法快速确定哪些文件是有效的（LevelDB提供了文件版本信息损坏时的修复机制）。而LevelDB中Version及相关概念就是为此设计的。 本文主要围绕LevelDB中Version、VersionEdit、VersionSet的设计与实现介绍与分析。 ","date":"2021-03-07","objectID":"/posts/code-reading/leveldb-made-simple/6-version/:1:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x06 Version","uri":"/posts/code-reading/leveldb-made-simple/6-version/"},{"categories":["深入浅出LevelDB"],"content":"1. 相关类型与文件 在LevelDB中，与Version相关的类有三个，分别是：VersionEdit、Version与VersionSet，而相关文件主要有Manifest和Current 正如引言中所述，LevelDB中Version相关信息记录的是LevelDB生成的文件的版本信息与相关元数据。LevelDB的版本信息是增量存储的，其存储方式与WAL相同，将版本的增量变化信息作为Record顺序写入Manifest文件中（详见本系列文章深入浅出LevelDB —— 0x03 Log）。 ","date":"2021-03-07","objectID":"/posts/code-reading/leveldb-made-simple/6-version/:2:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x06 Version","uri":"/posts/code-reading/leveldb-made-simple/6-version/"},{"categories":["深入浅出LevelDB"],"content":"1.1 VersionEdit、Manifest、Current LevelDB的版本增量数据在内存中的类型是VersionEdit，其EncodeTo与DecodeFrom方法分别用来序列化或反序列化VersionEdit，以便将其保存在文件中或从文件中读取。 目前VersionEdit类中需要持久化到Manifest文件中的数据共有8种： // Tag numbers for serialized VersionEdit. These numbers are written to // disk and should not be changed. enum Tag { kComparator = 1, kLogNumber = 2, kNextFileNumber = 3, kLastSequence = 4, kCompactPointer = 5, kDeletedFile = 6, kNewFile = 7, // 8 was used for large value refs kPrevLogNumber = 9 }; 这些Tag分别对应以下数据： Comparator Name：InternalKey比较器的名称字符串。 Log Number：当前Log文件编号。 Prev Log Number：前一个Log文件编号。 Last SequenceNumber：当前版本最后一个SequenceNumber的值（仅对于SSTable文件而言，在LevelDB掉电后恢复时，还需要从WAL中恢复MemTable的状态，WAL中的SequenceNumber比文件中的更高）。 Compact Pointers：(level, compaction key)记录某个level上次Compaction的位置，在Size Compaction中通过该字段来确定Compact的范围。 Deleted File：(level, file number)该版本中删除的元数据。 New File：(level, file number, file size, smallest key, largest key)该版本中新增文件的元数据。 已弃用 Prev Log Number：前一个Log文件编号。 因为VersionEdit是增量数据，因此并非每个VersionEdit中都有所有类型的数据，因此序列化VersionEdit的每种类型的数据前会将该类型对应的Tag以Varint32的编码方式写入到其数据之前。 每次LevelDB启动时，会创建一个新的Manifest文件，并创建一个当前状态的全量快照（首次调用LogAndApply方法时除了写入增量的Record，还会调用WriteSnapshot方法写入其余的全量数据，这两个方法位于version_set.h与version_set.cc中），以裁剪增量记录的长度。在创建新的Manifest文件同时，LevelDB还会修改Current文件，将其指向最新的Manifest文件。Current文件中只有一个内容，即当前Manifest文件名。 ","date":"2021-03-07","objectID":"/posts/code-reading/leveldb-made-simple/6-version/:2:1","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x06 Version","uri":"/posts/code-reading/leveldb-made-simple/6-version/"},{"categories":["深入浅出LevelDB"],"content":"1.2 VersionEdit、Version、VersionSet Manifest与Current文件是LevelDB保存在稳定存储中的文件版本信息，在LevelDB被打开后，其会先通过Current文件找到当前的Manifest文件，读取并反序列化其中数据，并在内存中维护文件版本信息，以便后续操作。 LevelDB在内存中将每个版本的文件信息封装为Version保存，Version保存了该版本每个level中都有哪些文件（及一些与Compaction有关的字段，笔者将在本系列后续文章中介绍），Version中文件元数据被按层组织为FileMetaData结构体数组的vector容器。FileMetaData中的字段如下： struct FileMetaData { FileMetaData() : refs(0), allowed_seeks(1 \u003c\u003c 30), file_size(0) {} int refs; int allowed_seeks; // Seeks allowed until compaction uint64_t number; uint64_t file_size; // File size in bytes InternalKey smallest; // Smallest internal key served by table InternalKey largest; // Largest internal key served by table }; LevelDB将Version以双向链表的形式组织，每个Version的next_指针指向下一个Version，prev_指针指向上一个Version。 LevelDB通过VersionSet结构来保存Version的双向链表，及其它不需要多版本记录的文件信息等（如db_name、last_sequence_、log_number_、table_cache_等）。VersionSet的dummy_versions_是Version双向链表的链头，其是一个无实际意义的Version对象，因此dummy_versions_的下一个Version是内存中保存的最旧的Version、dummy_versions_的上一个Version是最新的Version，VersionSet中还有一个current_指针指向链表中最新的Version。 VersionEdit、Version、VersionSet关系图VersionEdit、Version、VersionSet关系图 \" VersionEdit、Version、VersionSet关系图 在创建新版本时，LevelDB首先构造VersionEdit，然后通过VersionSet::LogAndApply方法，先将VersionEdit应用到Current Version，然后将增量的VersionEdit写入Manifest文件中。 ","date":"2021-03-07","objectID":"/posts/code-reading/leveldb-made-simple/6-version/:2:2","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x06 Version","uri":"/posts/code-reading/leveldb-made-simple/6-version/"},{"categories":["深入浅出LevelDB"],"content":"2. LevelDB重启后的恢复与修复 LevelDB重启时，无论是正常关闭还是异常退出，都需要恢复其状态。这个状态既包括内存中VersionSet的状态，也包括MemTable的状态。 在正常情况情况下，LevelDB可以通过Current文件找到当前的Manifest文件，并根据该文件恢复期VersionSet的状态；在通过WAL文件恢复其MemTable的状态。但是，如果数据库上一次关闭时异常退出（如掉电宕机等），且当时正在写Manifest文件或SSTable，可能导致Manifest或SSTable文件损坏。此时，需要用户通过db/Repairer.cc中提供的Repairer类的Run()方法来尝试手动修复LevelDB。 本节主要介绍并分析LevelDB恢复与修复的流程与实现。 ","date":"2021-03-07","objectID":"/posts/code-reading/leveldb-made-simple/6-version/:3:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x06 Version","uri":"/posts/code-reading/leveldb-made-simple/6-version/"},{"categories":["深入浅出LevelDB"],"content":"2.1 LevelDB的恢复 LevelDB被通过DB::Open方法打开时，在非常简单的初始化后，会调用DB::Recover方法来恢复自身状态。该方法会检查Lock文件是否被占用（LevelDB通过名为LOCK的文件避免多个LevelDB进程同时访问一个数据库）、目录是否存在、Current文件是否存在等。然后主要通过VersionSet::Recover与DBImpl::RecoverLogFile两个方法，分别恢复其VersionSet的状态与MemTable的状态。 VersionSet::Recover方法在恢复VersionSet时，会读取Current文件来找到当前的Manifest文件。随后，LevelDB会读取Manifest中的全量数据，并通过这些数据构造单个Version，而不是为每条VersionEdit都创建一个Version。接下来，LevelDB需要写入新的Manifest文件，如果当前的Manifest文件较小，LevelDB会复用旧的Manifest文件，否则创建新的Manifest文件。 DBImpl::RecoverLogFile方法在恢复MemTable的状态时，只需要顺序读取WAL，并根据其中Record构建MemTable中即可。需要注意的是，由于每次打开LevelDB时的Options可能不能，因此在RecoverLogFile期间，也可能出现MemTable超过write_buffer_size的情况。此时，LevelDB不会将MemTable转为Immutable MemTable，而是直接将其写入level-0。 如果LevelDB恢复不成功，其会报告错误。此时，用户必须通过Repairer来尝试手动修复LevelDB。 ","date":"2021-03-07","objectID":"/posts/code-reading/leveldb-made-simple/6-version/:3:1","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x06 Version","uri":"/posts/code-reading/leveldb-made-simple/6-version/"},{"categories":["深入浅出LevelDB"],"content":"2.2 LevelDB的修复 在Manifest或SSTable文件损坏时，LevelDB为用户提供了Repairer（位于db/repair.cc）来尝试修复数据库文件。Repaier类中只提供了一个可见方法Run。 Run方法会依次调用如下4个方法对LevelDB的数据进行修复： FindFiles：该方法会查找数据库目录中所有的Manifest、Log与SSTable文件。 ConvertLogFilesToTables：该方法会依次遍历所有的Log文件，并对每个Log文件通过ConvertLogToTable方法生成SSTable文件。这一过程不会产生新的VersionEdit，VersionEdit由后续流程生成。 ExtractMetaData：该方法会通过ScanTable方法扫描所有的SSTable，并生成其FileMetaData对象，以便恢复当前的Version。该方法遇到完全无法读取的SSTable时，会将其归档到lost/目录下；在遍历SSTable期间，会尽可能跳过无法解析的key；如果遇到无法解析并导致之后的文件都无法读取的key，则会通过RepairTable方法，将该key前能解析的key写入到新的SSTable中，并将原SSTable归档到lost/目录中。 WriteDescriptor：该方法会根据恢复的Version，重新生成编号为1的Manifest文件，并将旧的Manifest文件归档到lost/目录下。 ","date":"2021-03-07","objectID":"/posts/code-reading/leveldb-made-simple/6-version/:3:2","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x06 Version","uri":"/posts/code-reading/leveldb-made-simple/6-version/"},{"categories":["深入浅出LevelDB"],"content":"3. More 本文在介绍VersionEdit、Version、VersionSet和FileMetaData时，只介绍了其与文件版本信息相关的字段。除此之外其中还有一些其它的重要字段。本系列的后续文章会在使用这些字段时对其进行介绍。 ","date":"2021-03-07","objectID":"/posts/code-reading/leveldb-made-simple/6-version/:4:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x06 Version","uri":"/posts/code-reading/leveldb-made-simple/6-version/"},{"categories":["深入浅出LevelDB"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2021-03-06","objectID":"/posts/code-reading/leveldb-made-simple/5-sstable/:0:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x05 SSTable","uri":"/posts/code-reading/leveldb-made-simple/5-sstable/"},{"categories":["深入浅出LevelDB"],"content":"0. 引言 SSTable（Sorted-String Table）是LevelDB中数据在稳定存储中的格式。当memtable中的数据超过一定阈值时，LevelDB会将memtable转为immutable memtable，LevelDB的后台线程会将immutable memtable通过compaction操作将其以SSTable的格式写入到稳定存储。 本文主要介绍SSTable的格式，有关compaction操作会在本系列后续的文章中介绍。 ","date":"2021-03-06","objectID":"/posts/code-reading/leveldb-made-simple/5-sstable/:1:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x05 SSTable","uri":"/posts/code-reading/leveldb-made-simple/5-sstable/"},{"categories":["深入浅出LevelDB"],"content":"1. SSTable格式 ","date":"2021-03-06","objectID":"/posts/code-reading/leveldb-made-simple/5-sstable/:2:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x05 SSTable","uri":"/posts/code-reading/leveldb-made-simple/5-sstable/"},{"categories":["深入浅出LevelDB"],"content":"1.1 SSTable文件格式概览 SSTable的文件格式可表示为下图： sstable文件格式sstable文件格式 \" sstable文件格式 SSTable中的数据按照功能可以分为如下几块区： Data Block区：存放key/value数据。 Meta Block区：存放过滤器或当前SSTable相关的统计数据。 MetaIndex Block：仅有1个Block，该Block中存放了所有Meta Block的索引。 Index Block区：所有Data Block的索引。 Footer：大小固定的一个区域（48B），该区域中有两个Handle，分别标识了MetaIndex Block区和Index Block区的偏移量与大小；文件末尾的MagicNum用来标识该文件是LevelDB的SSTable文件；剩余空间被填充为Padding。 提示 Footer大小48B原因：Footer中有2个Handle和1个64bit的MagicNumber，每个Handle中有2个varint64编码的字段。varint64编码最大长度为10B，因最多需要 (10B + 10B) * 2 + 8 = 48B。 在SSTable中，无论是Footer中的Handler，还是各种索引中的Handler，都由两个varint64编码的字段组成：offset、size。这两个字段分别标识了指向的Block的偏移量与内容大小。每个Block除了其包含的内容的数据外，还有压缩类型标识符（1B）与校验和（4B）。Handle的size字段是不包含块尾元数据（1B+4B=5B）的大小。 Handle与Block格式Handle与Block格式 \" Handle与Block格式 其中，合法的压缩类型标识符共两种： 压缩类型 值 描述 kNoCompression 0x0 不压缩。 kSnappyCompression 0x1 采用Snappy算法压缩。 ","date":"2021-03-06","objectID":"/posts/code-reading/leveldb-made-simple/5-sstable/:2:1","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x05 SSTable","uri":"/posts/code-reading/leveldb-made-simple/5-sstable/"},{"categories":["深入浅出LevelDB"],"content":"1.2 Block格式 SSTable中所有的Block（content）都以下图格式组织： Block格式Block格式 \" Block格式 从功能上，Block中可分为三个区域： Entry区：保存每条数据条目（通过Restart方式压缩）。 Restart区：保存每条Restart索引（详见下文）。 Restart Num：Restart区索引数（Fixed32编码），读取时通过该值来找到Restart区的起点。 由于SSTable中Entry常有公共前缀（特别是在不清理无效版本的level-0中），因此LevelDB对Block中的Entry进行了简单的压缩：每隔一定数量的Entry设定一个Restart Point，Restart Point后的第一条Entry完整保存（下文称其为Restart Entry）。而对于该Restart Entry到下一个Restart Point中间的Entry，只保存其与Restart Entry公共前缀后的部分，与一些用来计算长度的元数据。 这里以Data Block为例，如下图所示： Restart压缩Restart压缩 \" Restart压缩 如图所示，每个Entry可分为5段，分别为：该Entry的Key与其相应的Restart Entry的公共前缀长度（Varint32编码）、该Entry的Key剩余的长度（Varint32编码）、该Entry的Value长度（Varint32）编码、该Entry的Key的非公共前缀数据（bytes）、该Entry的Value数据（bytes）。Restart区的Restart索引（Fixed32编码）分别指向每个Restart Entry的偏移量。当然，这种压缩方式适用于所有的Block，无论数据只有key还是拥有key/value，并非只有Data Block使用了这种方式。 通过这种方式，可以对频繁出现的公共前缀进行压缩。Restart Entry的间隔leveldb::Options.block_restart_interval默认为16，以平衡缓存局部性。 接下来关注SSTable中各类Block保存的数据（以下屏蔽Restart细节，仅关注Entry中的key/value）。 Block Type key value 描述 Data Block InternalKey Size + InternalKey Value Size + Value 完整数据与SkipList中Key的格式相同。 Meta Block - - Meta Block有多种类型，目前可分 Filter Meta Block与Stats Meta Block，分别保存当前SSTable的过滤器与统计数据。 Filter Meta Block filter.{{ Filter Name}} BlockHandle 该SSTable使用的过滤器名称及其索引。过滤器的实现详见下文。 Stats Meta Block 统计量名 统计量值 保存该SSTable的统计量。 MetaIndex Block Meta Block Name BlockHandle 用来索引所有的MetaBlock。 Index Block 相应Data Block的“最大”Key值（详见下文） BlockHandle 用来快速索引key在SSTable的哪个Data Block中。 在较早版本的LevelDB中，Index Block中的key为其相应的Data Block中最大的key。随后，LevelDB对Index Block中key的空间进行了优化。目前，该key为大于等于其相应Block的最大key，且小于下一个Data Block最小key的最短key值。对于SSTable中的最后一个Data Block的key，取大于其最后一个key的最短key值。 例如，下表给出了在一个SSTable中连续的若干Data Block的key范围（这里简化了key的格式，假设其只保存一个简单字符串）及其在Index Block中的key，其中Data Block n为该SStable的最后一个Data Block： Data Block min(key) max(key) index key Data Block k apple catspaw catq Data Block k+1 catsup dog dog Data Block k+2 dogecoin … … … … … Data Block n-1 CalvinWeirFields PaulDano R Data Block n RubyTiffanySparks YoeKazan Z 从上表中可以看出，通过这种方式可以进一步压缩Index Block中key的空间。 提示 取Index Block的key的两个方法分别通过FindShortestSeparator与FindShortSuccessor实现。 Filter Block中存放了当前SSTable的过滤器。Filter Block中的过滤器被分为n个段，第i个过滤器是为位于文件偏移量$[i*base,(i+1)*base)$的key生成的过滤器，$base$值位于Filter Block的最后一字节处，其单位为KB，默认为2KB。 Filter Block格式Filter Block格式 \" Filter Block格式 LevelDB使用的默认过滤器是布隆过滤器，其在MetaIndex Block中的名为filter.leveldb.BuiltinBloomFilter2，通过util/bloom.cc实现（哈希函数实现在util/hash.h与util/hash.cc中）。该实现生成的过滤器按最短为8字节。该布隆过滤器实现非常简单，这里不再赘述，感兴趣的读者可以自行阅读其源码。 ","date":"2021-03-06","objectID":"/posts/code-reading/leveldb-made-simple/5-sstable/:2:2","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x05 SSTable","uri":"/posts/code-reading/leveldb-made-simple/5-sstable/"},{"categories":["深入浅出LevelDB"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2021-03-05","objectID":"/posts/code-reading/leveldb-made-simple/4-memtable/:0:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x04 MemTable","uri":"/posts/code-reading/leveldb-made-simple/4-memtable/"},{"categories":["深入浅出LevelDB"],"content":"0. 引言 LSM-Tree中的MemTable既作为整合随机写入的buffer，又最为加速热数据读取的cache，是LSM-Tree的重要组件之一。 由于MemTable是保存在内存中的，其I/O开销比保存在稳定存储上的SSTable要小得多，因此LevelDB在实现MemTable时，查找结构采用的是跳表SkipList。 无论是MemTable还是Immutable MemTable，其实现均为leveldb::Memtable，当MemTable写满后，LevelDB会将其从DBImpl的mem_字段转移到imm_字段，不再对其进行写入。 本文主要介绍并分析LevelDB中MemTable的设计与实现。 相关文件：db/skiplist.h、db/memtable.h、db/memtable.cc、db/dbformat.h、db/dbformat.cc、util/arena.h、util/arena.cc。 ","date":"2021-03-05","objectID":"/posts/code-reading/leveldb-made-simple/4-memtable/:1:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x04 MemTable","uri":"/posts/code-reading/leveldb-made-simple/4-memtable/"},{"categories":["深入浅出LevelDB"],"content":"1. 内存分配器Arena SkipList几乎是MemTable中唯一需要在堆上分配内存的部分。为了尽可能将SkipList的内存连续分配以充分利用缓存局部性，LevelDB在分配SkipList链表节点的内存时，没有通过默认的new/delete实现，而是自己实现了一个内存分配器Arena。因此，在介绍SkipList与MemTable前，我们先来关注内存分配器Arena的实现。 相关文件：util/arena.h、util/arena.cc。 Arena是LevelDB的内存分配器，Arena会按需以“块”为单位申请内存，每个块是一个连续的内存空间。SkipList通过Arena分配内存可以避免其链表节点的内存在内存堆中过于分散，以便利用缓存局部性。 Arena在其生命周期中不会释放已获取的内存。此外，向Arena申请内存的数据结构也不会在Arena的生命周期中归还其占用的内存，这与数据结构的使用场景及使用相关。Arena的生命周期是与MemTable绑定的，每个MemTable都有自己的Arena。当MemTable销毁时，会将作为其字段的arena_一并销毁。 Arena对外提供了以下方法： class Arena { public: Arena(); Arena(const Arena\u0026) = delete; Arena\u0026 operator=(const Arena\u0026) = delete; ~Arena(); // Return a pointer to a newly allocated memory block of \"bytes\" bytes. char* Allocate(size_t bytes); // Allocate memory with the normal alignment guarantees provided by malloc. char* AllocateAligned(size_t bytes); // Returns an estimate of the total memory usage of data allocated // by the arena. size_t MemoryUsage() const { return memory_usage_.load(std::memory_order_relaxed); } // ... ... } Arena对外的分配方法有两种，区别在于是否按照机器位数对齐。Arena内部主要通过4个字段实现： class Arena { // ... ... private: char* AllocateFallback(size_t bytes); char* AllocateNewBlock(size_t block_bytes); // Allocation state char* alloc_ptr_; size_t alloc_bytes_remaining_; // Array of new[] allocated memory blocks std::vector\u003cchar*\u003e blocks_; // Total memory usage of the arena. // // TODO(costan): This member is accessed via atomics, but the others are // accessed without any locking. Is this OK? std::atomic\u003csize_t\u003e memory_usage_; }; std::vector\u003cchar*\u003e blocks_字段按block来保存已申请的内存空间，char* alloc_ptr_指向当前块中还未分配的内存地址，size_t alloc_bytes_remaining_记录了当前块中剩余的未分配空间大小，std::atomic\u003csize_t\u003e memory_usage_记录了Arena获取的总内存大小（包括了每个block的header大小）。注意，这里“当前块”并非Arena创建的最后一个块，因为Arena为了避免浪费，会为较大的请求分配单独的块（详见下文），这里的“当前块”是指除了这些单独分配的块外获得的最后一个块。 当LevelDB通过Allocate方法向Arena请求内存时，Arena首先会检查当前块的剩余空间，如果当前块剩余空间能够满足分配需求，则直接将剩余空间分配给调用者，并调整alloc_ptr与alloc_bytes_remaining： inline char* Arena::Allocate(size_t bytes) { // The semantics of what to return are a bit messy if we allow // 0-byte allocations, so we disallow them here (we don't need // them for our internal use). assert(bytes \u003e 0); if (bytes \u003c= alloc_bytes_remaining_) { char* result = alloc_ptr_; alloc_ptr_ += bytes; alloc_bytes_remaining_ -= bytes; return result; } return AllocateFallback(bytes); } 如果当前块剩余空间不足，Arena会调用内部的AllocateFallback方法： char* Arena::AllocateFallback(size_t bytes) { if (bytes \u003e kBlockSize / 4) { // Object is more than a quarter of our block size. Allocate it separately // to avoid wasting too much space in leftover bytes. char* result = AllocateNewBlock(bytes); return result; } // We waste the remaining space in the current block. alloc_ptr_ = AllocateNewBlock(kBlockSize); alloc_bytes_remaining_ = kBlockSize; char* result = alloc_ptr_; alloc_ptr_ += bytes; alloc_bytes_remaining_ -= bytes; return result; } AllocateFallback会判断需要分配的大小，如果需要分配的大小超过了默认块大小的$\\frac{1}{4}$，为了避免浪费当前块的剩余空间，Arena会为其单独分配一个大小等于需求的块，此时不需要调整alloc_ptr与alloc_bytes_remaining字段，这样做的另一个好处是这一逻辑也可以用于分配需求大于默认块大小的空间；如果需要分配的大小没有超过默认块大小的$\\frac{1}{4}$，此时不再使用当前块的剩余空间浪费也很小，因此直接申请一个默认大小的块，并从新块分配空间，同时调整alloc_ptr与alloc_bytes_remaining字段。 AllocateNewBlock会通过new关键字创建连续的内存块，并将获得的内存块保存到blocks_中，同时更新memory_usage_字段： char* Arena::AllocateNewBlock(size_t block_bytes) { char* result = new char[block_bytes]; blocks_.push_back(result); memory_usage_.fetch_add(block_bytes + sizeof(char*), std::memory_order_relaxed); return result; } 在计算memory_usage_时，使用的空间除了需求的空间大小block_bytes外，还要加上new关键字为数组分配空间时为数组加上的header大小（这样delete[]关键字才能知道需要释放的数组大小）。 ","date":"2021-03-05","objectID":"/posts/code-reading/leveldb-made-simple/4-memtable/:2:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x04 MemTable","uri":"/posts/code-reading/leveldb-made-simple/4-memtable/"},{"categories":["深入浅出LevelDB"],"content":"2. SkipList的实现 SkipList是一种多层链表查找结构，其实现较其它查找结构比简单很多。有关SkipList的概念本文不再赘述，不了解的读者可以自行查找其它资料。 LevelDB的跳表实现位于db/skiplist.h文件中，其对外提供了插入、判断键是否存在的功能，此外还提供了一个用来更细粒度访问跳表的迭代器，通过迭代器可以顺序地正反遍历跳表，或按照索引随机查找。 LevelDB中的SkipList只插入，不修改或删除，MemTable的修改或删除是通过插入有响应标识或序号的key实现的。 SkipList通过template可以实现自定义Key类型与Key的比较方式。自定义Comparator只需要实现include/comparator.h中的虚类Comparator即可。 leveldb::SkipList及其迭代器leveldb::SkipList::Iterator的声明如下： template \u003ctypename Key, class Comparator\u003e class SkipList { private: struct Node; public: // Create a new SkipList object that will use \"cmp\" for comparing keys, // and will allocate memory using \"*arena\". Objects allocated in the arena // must remain allocated for the lifetime of the skiplist object. explicit SkipList(Comparator cmp, Arena* arena); SkipList(const SkipList\u0026) = delete; SkipList\u0026 operator=(const SkipList\u0026) = delete; // Insert key into the list. // REQUIRES: nothing that compares equal to key is currently in the list. void Insert(const Key\u0026 key); // Returns true iff an entry that compares equal to key is in the list. bool Contains(const Key\u0026 key) const; // Iteration over the contents of a skip list class Iterator { public: // Initialize an iterator over the specified list. // The returned iterator is not valid. explicit Iterator(const SkipList* list); // Returns true iff the iterator is positioned at a valid node. bool Valid() const; // Returns the key at the current position. // REQUIRES: Valid() const Key\u0026 key() const; // Advances to the next position. // REQUIRES: Valid() void Next(); // Advances to the previous position. // REQUIRES: Valid() void Prev(); // Advance to the first entry with a key \u003e= target void Seek(const Key\u0026 target); // Position at the first entry in list. // Final state of iterator is Valid() iff list is not empty. void SeekToFirst(); // Position at the last entry in list. // Final state of iterator is Valid() iff list is not empty. void SeekToLast(); private: const SkipList* list_; Node* node_; // Intentionally copyable }; private: enum { kMaxHeight = 12 }; inline int GetMaxHeight() const { return max_height_.load(std::memory_order_relaxed); } Node* NewNode(const Key\u0026 key, int height); int RandomHeight(); bool Equal(const Key\u0026 a, const Key\u0026 b) const { return (compare_(a, b) == 0); } // Return true if key is greater than the data stored in \"n\" bool KeyIsAfterNode(const Key\u0026 key, Node* n) const; // Return the earliest node that comes at or after key. // Return nullptr if there is no such node. // // If prev is non-null, fills prev[level] with pointer to previous // node at \"level\" for every level in [0..max_height_-1]. Node* FindGreaterOrEqual(const Key\u0026 key, Node** prev) const; // Return the latest node with a key \u003c key. // Return head_ if there is no such node. Node* FindLessThan(const Key\u0026 key) const; // Return the last node in the list. // Return head_ if list is empty. Node* FindLast() const; // Immutable after construction Comparator const compare_; Arena* const arena_; // Arena used for allocations of nodes Node* const head_; // Modified only by Insert(). Read racily by readers, but stale // values are ok. std::atomic\u003cint\u003e max_height_; // Height of the entire list // Read/written only by Insert(). Random rnd_; }; SkipList的节点由leveldb::SkipList::Node类实现，Node的内存是在堆中分配的，其通过Arena分配器分配。有 Node的实现如下： // Implementation details follow template \u003ctypename Key, class Comparator\u003e struct SkipList\u003cKey, Comparator\u003e::Node { explicit Node(const Key\u0026 k) : key(k) {} Key const key; // Accessors/mutators for links. Wrapped in methods so we can // add the appropriate barriers as necessary. Node* Next(int n) { assert(n \u003e= 0); // Use an 'acquire load' so that we observe a fully initialized // version of the returned Node. return next_[n].load(std::memory_order_acquire); } void SetNext(int n, Node* x) { assert(n \u003e= 0); // Use a 'release store' so that anybody who reads through this // pointer observes a fully initialized version of the inserted node. next_[n].store(x, std::memory_order_release); } // No-barrier va","date":"2021-03-05","objectID":"/posts/code-reading/leveldb-made-simple/4-memtable/:3:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x04 MemTable","uri":"/posts/code-reading/leveldb-made-simple/4-memtable/"},{"categories":["深入浅出LevelDB"],"content":"3. MemTable的实现 ","date":"2021-03-05","objectID":"/posts/code-reading/leveldb-made-simple/4-memtable/:4:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x04 MemTable","uri":"/posts/code-reading/leveldb-made-simple/4-memtable/"},{"categories":["深入浅出LevelDB"],"content":"3.1 MemTable概览 Memtable对SkipList进行了封装，SkipList只能提供key的插入与查找，而Memtable并对外提供了key/value的增删改查操作。MemTable还提供了正向迭代器与反向迭代器，让使用者能够更细粒度地访问MemTable中的数据。 MemTable的声明如下： class InternalKeyComparator; class MemTableIterator; class MemTable { public: // MemTables are reference counted. The initial reference count // is zero and the caller must call Ref() at least once. explicit MemTable(const InternalKeyComparator\u0026 comparator); MemTable(const MemTable\u0026) = delete; MemTable\u0026 operator=(const MemTable\u0026) = delete; // Increase reference count. void Ref() { ++refs_; } // Drop reference count. Delete if no more references exist. void Unref() { --refs_; assert(refs_ \u003e= 0); if (refs_ \u003c= 0) { delete this; } } // Returns an estimate of the number of bytes of data in use by this // data structure. It is safe to call when MemTable is being modified. size_t ApproximateMemoryUsage(); // Return an iterator that yields the contents of the memtable. // // The caller must ensure that the underlying MemTable remains live // while the returned iterator is live. The keys returned by this // iterator are internal keys encoded by AppendInternalKey in the // db/format.{h,cc} module. Iterator* NewIterator(); // Add an entry into memtable that maps key to value at the // specified sequence number and with the specified type. // Typically value will be empty if type==kTypeDeletion. void Add(SequenceNumber seq, ValueType type, const Slice\u0026 key, const Slice\u0026 value); // If memtable contains a value for key, store it in *value and return true. // If memtable contains a deletion for key, store a NotFound() error // in *status and return true. // Else, return false. bool Get(const LookupKey\u0026 key, std::string* value, Status* s); private: friend class MemTableIterator; friend class MemTableBackwardIterator; struct KeyComparator { const InternalKeyComparator comparator; explicit KeyComparator(const InternalKeyComparator\u0026 c) : comparator(c) {} int operator()(const char* a, const char* b) const; }; typedef SkipList\u003cconst char*, KeyComparator\u003e Table; ~MemTable(); // Private since only Unref() should be used to delete it KeyComparator comparator_; int refs_; Arena arena_; Table table_; }; } // namespace leveldb MemTable的实例采用了引用计数，其初始计数为0，因此其构造方法的调用者需要手动调用其Ref方法将其初始引用计数置为1；在读取MemTable时或MemTable在Compact时，LevelDB会通过Ref方法增大MemTable的引用计数，避免其在读取过程中被回收而导致的无效内存访问，在操作完成后再通过Unref减小其引用计数；当调用Unref方法使其引用计数器减至0时，MemTable会自己销毁。 本节，我们主要关注MemTable是如何封装SkipList以实现key/value的增删改查的。 ","date":"2021-03-05","objectID":"/posts/code-reading/leveldb-made-simple/4-memtable/:4:1","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x04 MemTable","uri":"/posts/code-reading/leveldb-made-simple/4-memtable/"},{"categories":["深入浅出LevelDB"],"content":"3.2 MemTable的实现 3.2.1 key的封装 如前文所述，SkipList数据结构是一个只有key的查找结构，为了能够通过 SkipList同时保存key/value等数据，就要将key/value及其它数据封装成一个key。LevelDB中key的封装规则如下图所示。 key封装规则key封装规则 \" key封装规则 SkipList的key从大体上可以分为三部分（颜色不同的部分）：InternalKey Size、InternalKey、Value。其中，InternalKey是LevelDB为了在Insert-Only的SkipList上实现增删改查而封装的结构，也是SkipList中Node的默认排序依据；InternalKey Size即InternalKey的大小，通过varint32编码；Value由用户插入的value与其大小组成，其value size同样通过varint32编码实现。接下来，我们重点介绍InternalKey的设计。 InternalKey内部由3部分组成： key：用户插入的key，也叫做UserKey。 SequenceNumber：全局单调递增序号，当LevelDB更新数据时（增/删/改）递增，保证后发生操作的SequenceNumber值大于先发生的操作，MemTable通过该字段在Insert-Only的SkipList上实现MVCC的增删改查。 ValueType：用来表示操作类型枚举值，其值只有两种：kTypeDeletion与kTypeValue。其中kTypeDeletion表示该Key是删除操作，kTypeValue表示该Key是增/改操作。 3.2.2 增删改查的实现 在介绍了MemTable对SkipList的Key封装后，我们来分析MemTable如何通过这种封装来在Insert-Only的SkipList上实现key/value的增删改查操作。 在上一节中，笔者提到过InternalKey是SkipList中Node的默认排序依据。LevelDB中SkipList的默认排序是通过leveldb::InternalKeyComparator实现的，其声明与实现在db/dbformat.h与db/dbformat.cc中。 InternalKeyComparator的Compare方法按照如下优先级，依次对InternalKey进行排序： 按照UserKey升序排序； 按照SequenceNumber降序排序； 按照ValueType降序排序（由于SequenceNumber已经足以对Key排序，因此这条规则永远不会用到）。 通过InternalKeyComparator，SkipList可以保证对于同一key（UserKey），新的操作永远在旧的操作的前面。因此，只要找到key（UserKey）在SkipList中第一次出现的位置，即可保证得到的是该key最新的版本。 在分析MemTable如何实现查找key（UserKey）前，我们先来看一下MemTable实现增/删/查操作的实现： void MemTable::Add(SequenceNumber s, ValueType type, const Slice\u0026 key, const Slice\u0026 value) { // Format of an entry is concatenation of: // key_size : varint32 of internal_key.size() // key bytes : char[internal_key.size()] // value_size : varint32 of value.size() // value bytes : char[value.size()] size_t key_size = key.size(); size_t val_size = value.size(); size_t internal_key_size = key_size + 8; const size_t encoded_len = VarintLength(internal_key_size) + internal_key_size + VarintLength(val_size) + val_size; char* buf = arena_.Allocate(encoded_len); char* p = EncodeVarint32(buf, internal_key_size); std::memcpy(p, key.data(), key_size); p += key_size; EncodeFixed64(p, (s \u003c\u003c 8) | type); p += 8; p = EncodeVarint32(p, val_size); std::memcpy(p, value.data(), val_size); assert(p + val_size == buf + encoded_len); table_.Insert(buf); } 从Add方法中可以看出，MemTable不需要进行额外的操作，只需要将需要插入SkipList的Key按照上节中介绍的格式编码，然后直接插入到SkipList中即可。 而对于查找操作，由于在查找时MemTable无法得知需要查找的key（UserKey）最新的SequenceNumber或ValueType，因此在查找时，无法构造出恰好与SkipList中的InternalKey相等的查找键。但这并不影响MemTable查找UserKey的“当前”版本（注：由于LevelDB实现了MVCC，这里的“当前”版本只对该操作可见的最新版本，下文同理），根据InternalKey的排序顺序，只要构造出的查找键的UserKey与需要查找的UserKey相同，且SequenceNumber大于等于该UserKey当前SequenceNumber，MemTable即可根据查找键找到待查找的UserKey的最新版本可能出现的位置。然后判断该位置上的UserKey是否与我们要查找的UserKey相同，如果相同则说明我们找到了该UserKey的最新版本，如果不同则说明MemTable没有该UserKey的记录。 为了便于生成查找键，LevelDB定义了levelDB::LookupKey，其结构相当于InternalKey Size与InternalKey部分连接在一起，其中SequenceNumber部分为构造时的SequenceNumber，ValueType为1。在查找时，只需要传入UserKey及“当前”的SequenceNumber，即可构造出在SkipList中位于我们要查找的UserKey的最新版本可能出现的位置处的LookupKey。 MemTable中查找操作的实现如下： bool MemTable::Get(const LookupKey\u0026 key, std::string* value, Status* s) { Slice memkey = key.memtable_key(); Table::Iterator iter(\u0026table_); iter.Seek(memkey.data()); if (iter.Valid()) { // entry format is: // klength varint32 // userkey char[klength] // tag uint64 // vlength varint32 // value char[vlength] // Check that it belongs to same user key. We do not check the // sequence number since the Seek() call above should have skipped // all entries with overly large sequence numbers. const char* entry = iter.key(); uint32_t key_length; const char* key_ptr = GetVarint32Ptr(entry, entry + 5, \u0026key_length); if (comparator_.comparator.user_comparator()-\u003eCompare( Slice(key_ptr, key_length - 8), key.user_key()) == 0) { // Correct user key const uint64_t tag = DecodeFixed64(key_ptr + key_length - 8); switch (static_cast\u003cValueType\u003e(tag \u0026 0xff)) { case kTypeValue: { Slice v = GetLengthPrefixedSlice(key_ptr + key_length); value-\u003eassign(v.data(), v.size()); return true; } case kTypeDeletion: *s = Status::NotFound(Slice()); return true; } } } return false; } 正如上文中分析的那样，在根据LookupKey找到待查找的UserKey最新版本可能","date":"2021-03-05","objectID":"/posts/code-reading/leveldb-made-simple/4-memtable/:4:2","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x04 MemTable","uri":"/posts/code-reading/leveldb-made-simple/4-memtable/"},{"categories":["深入浅出LevelDB"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2021-03-05","objectID":"/posts/code-reading/leveldb-made-simple/3-log/:0:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x03 Log","uri":"/posts/code-reading/leveldb-made-simple/3-log/"},{"categories":["深入浅出LevelDB"],"content":"0. 引言 LevelDB在修改时，首先会将修改写入到保存在文件系统上的Log，以避免掉电时保存在内存中的数据丢失。由于Log是顺序写入的，其写入速度较快。因为Log的写入是在真正执行操作之前的，因此这一技术也叫做Write-Ahead Log。 本文主要分析LevelDB中Log的设计与实现。此外，本文的后半部分主要着眼于LevelDB如何保证WAL被安全地写入到稳定存储。 相关命名空间：leveldb::log。 相关文件：include/leveldb/env.h、db/log_format.h、db/log_writer.h、db/log_writer.cc、db/log_reader.h、db/log_reader.cc。 ","date":"2021-03-05","objectID":"/posts/code-reading/leveldb-made-simple/3-log/:1:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x03 Log","uri":"/posts/code-reading/leveldb-made-simple/3-log/"},{"categories":["深入浅出LevelDB"],"content":"1. Log的格式与设计 LevelDB的Log是由Record和一些为了对齐而填充的gap组成的文件。 LevelDB在读取Log文件时，为了减少I/O次数，每次读取都会读入一个32KB大小的块。因此，在写入Log文件时，LevelDB也将数据按照32KB对齐。 文件与块文件与块 \" 文件与块 由于，LevelDB中记录的长度是不确定的，如果想要与32KB块对齐，为了尽可能地利用空间，那么较长的记录可能会被拆分为多个段，以能够将其放入块的剩余空间中。LevelDB定义只有1个段的记录为FullType，由多个段组成的记录的首位段分别为FirstType与LastType，中间段为MiddleType。 记录与段记录与段 \" 记录与段 当块中剩余空间不足以放入完整记录时，LevelDB会将其按段拆分，直到该记录被完整保存： 段与块段与块 \" 段与块 记录的每个段由段头和数据组成，段头长度为固定的7字节，其中头4字节表示该段的CRC校验码、随后2字节表示该段长度、最后1字节标识该段的类型： 段结构段结构 \" 段结构 如果在写入时，与32KB对齐的剩余空间不足以放入7字节的header时，LevelDB会将剩余空间填充为0x00，并从下一个与32KB对齐处继续写入： 空白填充空白填充 \" 空白填充 ","date":"2021-03-05","objectID":"/posts/code-reading/leveldb-made-simple/3-log/:2:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x03 Log","uri":"/posts/code-reading/leveldb-made-simple/3-log/"},{"categories":["深入浅出LevelDB"],"content":"2. Log的实现 ","date":"2021-03-05","objectID":"/posts/code-reading/leveldb-made-simple/3-log/:3:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x03 Log","uri":"/posts/code-reading/leveldb-made-simple/3-log/"},{"categories":["深入浅出LevelDB"],"content":"2.1 WritableFile与SequentialFile 相关文件：include/leveldb/env.h、util/env_*.*。 在介绍LevelDB中Log的Writer与Reader前，我们首先看一下LevelDB中对Log文件的抽象。LevelDB对Log文件的抽象有WritableFile和SequentialFile，分别对应顺序写入与顺序读取的文件。 WritableFile与SequentialFile是抽象类，定义在include/leveldb/env/h中。env.h中声明了很多与环境无关的抽象，让使用者不需要关心这些类在不同操作系统环境下的具体实现，而这些抽象的实现在util/env_*.*中，对应不同环境下的实现。 WritableFile与SequentialFile的声明如下： // A file abstraction for sequential writing. The implementation // must provide buffering since callers may append small fragments // at a time to the file. class LEVELDB_EXPORT WritableFile { public: WritableFile() = default; WritableFile(const WritableFile\u0026) = delete; WritableFile\u0026 operator=(const WritableFile\u0026) = delete; virtual ~WritableFile(); virtual Status Append(const Slice\u0026 data) = 0; virtual Status Close() = 0; virtual Status Flush() = 0; virtual Status Sync() = 0; }; // A file abstraction for reading sequentially through a file class LEVELDB_EXPORT SequentialFile { public: SequentialFile() = default; SequentialFile(const SequentialFile\u0026) = delete; SequentialFile\u0026 operator=(const SequentialFile\u0026) = delete; virtual ~SequentialFile(); // Read up to \"n\" bytes from the file. \"scratch[0..n-1]\" may be // written by this routine. Sets \"*result\" to the data that was // read (including if fewer than \"n\" bytes were successfully read). // May set \"*result\" to point at data in \"scratch[0..n-1]\", so // \"scratch[0..n-1]\" must be live when \"*result\" is used. // If an error was encountered, returns a non-OK status. // // REQUIRES: External synchronization virtual Status Read(size_t n, Slice* result, char* scratch) = 0; // Skip \"n\" bytes from the file. This is guaranteed to be no // slower that reading the same data, but may be faster. // // If end of file is reached, skipping will stop at the end of the // file, and Skip will return OK. // // REQUIRES: External synchronization virtual Status Skip(uint64_t n) = 0; }; ","date":"2021-03-05","objectID":"/posts/code-reading/leveldb-made-simple/3-log/:3:1","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x03 Log","uri":"/posts/code-reading/leveldb-made-simple/3-log/"},{"categories":["深入浅出LevelDB"],"content":"2.2 Writer与Reader 相关文件：db/log_writer.h、db/log_writer.cc、db/log_reader.h、db/log_reader.cc。 leveldb::log::Writer是用来写入Log文件的类，其除了构造方法外只对外提供了一个追加记录的方法AddRecord，内部通过EmitPhysicalRecord方法用来将记录写入存储；leveldb::log::Reader是用来读取Log文件的类，其对外提供了ReadRecord方法，该方法会读取下一条记录，并将参数result与scratch指向下一条记录，内部方法ReadPhysicalRecord会通过unistd.h的read方法，读取文件的下一个块（32KB）到内部buffer（backing_store_），以减少I/O次数。 class Writer { public: // Create a writer that will append data to \"*dest\". // \"*dest\" must be initially empty. // \"*dest\" must remain live while this Writer is in use. explicit Writer(WritableFile* dest); // Create a writer that will append data to \"*dest\". // \"*dest\" must have initial length \"dest_length\". // \"*dest\" must remain live while this Writer is in use. Writer(WritableFile* dest, uint64_t dest_length); Writer(const Writer\u0026) = delete; Writer\u0026 operator=(const Writer\u0026) = delete; ~Writer(); Status AddRecord(const Slice\u0026 slice); private: Status EmitPhysicalRecord(RecordType type, const char* ptr, size_t length); // ... ... }; class Reader { public: // Interface for reporting errors. class Reporter { public: virtual ~Reporter(); // Some corruption was detected. \"size\" is the approximate number // of bytes dropped due to the corruption. virtual void Corruption(size_t bytes, const Status\u0026 status) = 0; }; // Create a reader that will return log records from \"*file\". // \"*file\" must remain live while this Reader is in use. // // If \"reporter\" is non-null, it is notified whenever some data is // dropped due to a detected corruption. \"*reporter\" must remain // live while this Reader is in use. // // If \"checksum\" is true, verify checksums if available. // // The Reader will start reading at the first record located at physical // position \u003e= initial_offset within the file. Reader(SequentialFile* file, Reporter* reporter, bool checksum, uint64_t initial_offset); Reader(const Reader\u0026) = delete; Reader\u0026 operator=(const Reader\u0026) = delete; ~Reader(); // Read the next record into *record. Returns true if read // successfully, false if we hit end of the input. May use // \"*scratch\" as temporary storage. The contents filled in *record // will only be valid until the next mutating operation on this // reader or the next mutation to *scratch. bool ReadRecord(Slice* record, std::string* scratch); // Returns the physical offset of the last record returned by ReadRecord. // // Undefined before the first call to ReadRecord. uint64_t LastRecordOffset(); private: // Extend record types with the following special values enum { kEof = kMaxRecordType + 1, // Returned whenever we find an invalid physical record. // Currently there are three situations in which this happens: // * The record has an invalid CRC (ReadPhysicalRecord reports a drop) // * The record is a 0-length record (No drop is reported) // * The record is below constructor's initial_offset (No drop is reported) kBadRecord = kMaxRecordType + 2 }; // Skips all blocks that are completely before \"initial_offset_\". // // Returns true on success. Handles reporting. bool SkipToInitialBlock(); // Return type, or one of the preceding special values unsigned int ReadPhysicalRecord(Slice* result); // Reports dropped bytes to the reporter. // buffer_ must be updated to remove the dropped bytes prior to invocation. void ReportCorruption(uint64_t bytes, const char* reason); void ReportDrop(uint64_t bytes, const Status\u0026 reason); // ... ... }; leveldb::log::Writer与leveldb::log::Reader中大部分是处理记录分段分块的代码，本文不再赘述。这里需要关注的是写入Log文件时数据的同步语义。 ","date":"2021-03-05","objectID":"/posts/code-reading/leveldb-made-simple/3-log/:3:2","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x03 Log","uri":"/posts/code-reading/leveldb-made-simple/3-log/"},{"categories":["深入浅出LevelDB"],"content":"2.3 WAL数据同步 Log（或Write-Ahead Log，WAL）的意义在于保证机器故障时数据不会因为内存掉电而丢失，只有record被执行前，被完全同步到稳定存储后，才能保证掉电后数据的完整性。然而，如果每条记录都要等待同步写入，其开销很高。 LevelDB提供了是否开启同步的选项WriteOptions，其定义在include/leveldb/options.h中： // Options that control write operations struct LEVELDB_EXPORT WriteOptions { WriteOptions() = default; // If true, the write will be flushed from the operating system // buffer cache (by calling WritableFile::Sync()) before the write // is considered complete. If this flag is true, writes will be // slower. // // If this flag is false, and the machine crashes, some recent // writes may be lost. Note that if it is just the process that // crashes (i.e., the machine does not reboot), no writes will be // lost even if sync==false. // // In other words, a DB write with sync==false has similar // crash semantics as the \"write()\" system call. A DB write // with sync==true has similar crash semantics to a \"write()\" // system call followed by \"fsync()\". bool sync = false; }; 如果在配置LevelDB时，将WriteOptions的sync字段置为true，LevelDB在写入WAL时会根据环境架构，通过适当的方式等待数据完全被写入到稳定存储。 接下来我们以支持POSIX的系统为例，分析LevelDB中WAL的同步写入过程。 leveldb::log::Writer的EmitPhysicalRecord方法是将Record写入到WAL中的方法： Status Writer::EmitPhysicalRecord(RecordType t, const char* ptr, size_t length) { assert(length \u003c= 0xffff); // Must fit in two bytes assert(block_offset_ + kHeaderSize + length \u003c= kBlockSize); // Format the header char buf[kHeaderSize]; buf[4] = static_cast\u003cchar\u003e(length \u0026 0xff); buf[5] = static_cast\u003cchar\u003e(length \u003e\u003e 8); buf[6] = static_cast\u003cchar\u003e(t); // Compute the crc of the record type and the payload. uint32_t crc = crc32c::Extend(type_crc_[t], ptr, length); crc = crc32c::Mask(crc); // Adjust for storage EncodeFixed32(buf, crc); // Write the header and the payload Status s = dest_-\u003eAppend(Slice(buf, kHeaderSize)); if (s.ok()) { s = dest_-\u003eAppend(Slice(ptr, length)); if (s.ok()) { s = dest_-\u003eFlush(); } } block_offset_ += kHeaderSize + length; return s; } 从其源码中可以看出，该方法通过Flush方法将用户态buffer中写入的内容刷入内核态buffer后便会返回，后续写入通过操作系统实现。如果掉电时，操作系统还没有将数据写入到稳定存储，数据仍会丢失。为了确保内核缓冲区中的数据会被写入到稳定存储，需要通过系统调用实现，在POSIX系统下常用的系统调用有fsync、fdatasync、msync等。 leveldb::log::Writer的AppendRecord方法最终是通过EmitPhysicalRecord实现的，因此我们追溯到LevelDB调用AppendRecord的位置，其位于db/db_impl.cc中DBImpl::Write方法的实现中： // ... ... status = log_-\u003eAddRecord(WriteBatchInternal::Contents(write_batch)); bool sync_error = false; if (status.ok() \u0026\u0026 options.sync) { status = logfile_-\u003eSync(); if (!status.ok()) { sync_error = true; } } if (status.ok()) { status = WriteBatchInternal::InsertInto(write_batch, mem_); } mutex_.Lock(); // ... ... 如果开启了WriteOptions.sync选项，LevelDB此处会在调用AppendRecord后调用WritableFile的Sync方法以保证数据被同步到了稳定存储中。在POSIX环境下，WritableFile的Sync方法实现最终会落到SyncFd方法中，该方法位于util/env_posix.cc文件中： // Ensures that all the caches associated with the given file descriptor's // data are flushed all the way to durable media, and can withstand power // failures. // // The path argument is only used to populate the description string in the // returned Status if an error occurs. static Status SyncFd(int fd, const std::string\u0026 fd_path) { #if HAVE_FULLFSYNC // On macOS and iOS, fsync() doesn't guarantee durability past power // failures. fcntl(F_FULLFSYNC) is required for that purpose. Some // filesystems don't support fcntl(F_FULLFSYNC), and require a fallback to // fsync(). if (::fcntl(fd, F_FULLFSYNC) == 0) { return Status::OK(); } #endif // HAVE_FULLFSYNC #if HAVE_FDATASYNC bool sync_success = ::fdatasync(fd) == 0; #else bool sync_success = ::fsync(fd) == 0; #endif // HAVE_FDATASYNC if (sync_success) { return Status::OK(); } return PosixError(fd_path, errno); } SyncFd会根据宏定义来检查编译环境下系统支持的系统调用，并在保证安全的条件下选择开销最小的系统调用实现。 ","date":"2021-03-05","objectID":"/posts/code-reading/leveldb-made-simple/3-log/:3:3","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x03 Log","uri":"/posts/code-reading/leveldb-made-simple/3-log/"},{"categories":["深入浅出LevelDB"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2021-03-04","objectID":"/posts/code-reading/leveldb-made-simple/2-basic-data-structure/:0:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x02 Bisic Data Format [施工中]","uri":"/posts/code-reading/leveldb-made-simple/2-basic-data-structure/"},{"categories":["深入浅出LevelDB"],"content":"0. 引言 为了便于后续的分析，本节将介绍LevelDB中常用的基本数据格式。 ","date":"2021-03-04","objectID":"/posts/code-reading/leveldb-made-simple/2-basic-data-structure/:1:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x02 Bisic Data Format [施工中]","uri":"/posts/code-reading/leveldb-made-simple/2-basic-data-structure/"},{"categories":["深入浅出LevelDB"],"content":"1.切片Slice 相关文件：include/leveldb/slice.h。 Slice是LevelDB中广泛使用的切片类。Slice的结构非常简单，其只有两个字段，分别保存切片指针与切片大小： class LEVELDB_EXPORT Slice { // ... ... private: const char* data_; size_t size_; }; Slice只关心切片的位置与大小，而不关心切片内容。因此。我们可以将Slice看做字节数组切片。 Slice有4种构造方法，其显式使用了默认的拷贝构造方法与拷贝构造运算符： class LEVELDB_EXPORT Slice { public: // Create an empty slice. Slice() : data_(\"\"), size_(0) {} // Create a slice that refers to d[0,n-1]. Slice(const char* d, size_t n) : data_(d), size_(n) {} // Create a slice that refers to the contents of \"s\" Slice(const std::string\u0026 s) : data_(s.data()), size_(s.size()) {} // Create a slice that refers to s[0,strlen(s)-1] Slice(const char* s) : data_(s), size_(strlen(s)) {} // Intentionally copyable. Slice(const Slice\u0026) = default; Slice\u0026 operator=(const Slice\u0026) = default; // ... ... } 从Slice的构造方法可以看出，在Slice实例构造时，LevelDB不会为其分配新的内存空间，而是直接将其指向需要表示的切片头位置。因此，Slice的使用者需要确保在Slice实例还在使用时，其指向的内存不会销毁。 Slice的默认比较方式比较主要通过memcmp实现： inline int Slice::compare(const Slice\u0026 b) const { const size_t min_len = (size_ \u003c b.size_) ? size_ : b.size_; int r = memcmp(data_, b.data_, min_len); if (r == 0) { if (size_ \u003c b.size_) r = -1; else if (size_ \u003e b.size_) r = +1; } return r; } ","date":"2021-03-04","objectID":"/posts/code-reading/leveldb-made-simple/2-basic-data-structure/:2:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x02 Bisic Data Format [施工中]","uri":"/posts/code-reading/leveldb-made-simple/2-basic-data-structure/"},{"categories":["深入浅出LevelDB"],"content":"2. 整型与Slice编码方式 相关文件：coding.h、coding.cc。 LevelDB中另一种常用的数据类型是整型。在LevelDB的源码中，其直接使用了\u003ccstdint\u003e的uint32_t与uint64_t作为整型类型，因此我们只需要关注其编码为字节数组的方式。 LevelDB中为整型提供了两类编码方式，一类是定长编码，一类是变长编码。 另外，LevelDB为了便于从字节数组中划分Slice，其还提供了一种LengthPrefixedSlice的编码方式，在编码中将长度确定的Slice的长度作为Slice的前缀。 ","date":"2021-03-04","objectID":"/posts/code-reading/leveldb-made-simple/2-basic-data-structure/:3:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x02 Bisic Data Format [施工中]","uri":"/posts/code-reading/leveldb-made-simple/2-basic-data-structure/"},{"categories":["深入浅出LevelDB"],"content":"2.1 整型定长编码 LevelDB中整型的定长编码（32bits或64bits）方式非常简单，只需要将整型按照小端的顺序编码即可： inline void EncodeFixed32(char* dst, uint32_t value) { uint8_t* const buffer = reinterpret_cast\u003cuint8_t*\u003e(dst); // Recent clang and gcc optimize this to a single mov / str instruction. buffer[0] = static_cast\u003cuint8_t\u003e(value); buffer[1] = static_cast\u003cuint8_t\u003e(value \u003e\u003e 8); buffer[2] = static_cast\u003cuint8_t\u003e(value \u003e\u003e 16); buffer[3] = static_cast\u003cuint8_t\u003e(value \u003e\u003e 24); } inline void EncodeFixed64(char* dst, uint64_t value) { uint8_t* const buffer = reinterpret_cast\u003cuint8_t*\u003e(dst); // Recent clang and gcc optimize this to a single mov / str instruction. buffer[0] = static_cast\u003cuint8_t\u003e(value); buffer[1] = static_cast\u003cuint8_t\u003e(value \u003e\u003e 8); buffer[2] = static_cast\u003cuint8_t\u003e(value \u003e\u003e 16); buffer[3] = static_cast\u003cuint8_t\u003e(value \u003e\u003e 24); buffer[4] = static_cast\u003cuint8_t\u003e(value \u003e\u003e 32); buffer[5] = static_cast\u003cuint8_t\u003e(value \u003e\u003e 40); buffer[6] = static_cast\u003cuint8_t\u003e(value \u003e\u003e 48); buffer[7] = static_cast\u003cuint8_t\u003e(value \u003e\u003e 56); } 定长整型的解码方式同理，这里不再赘述。 ","date":"2021-03-04","objectID":"/posts/code-reading/leveldb-made-simple/2-basic-data-structure/:3:1","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x02 Bisic Data Format [施工中]","uri":"/posts/code-reading/leveldb-made-simple/2-basic-data-structure/"},{"categories":["深入浅出LevelDB"],"content":"2.2 整型变长编码 当整型值较小时，LevelDB支持将其编码为变长整型，以减少其空间占用（对于值与类型最大值接近时，变长整型占用空间反而增加）。 对于变长整型编码，LevelDB需要知道该整型编码的终点在哪儿。因此，LevelDB将每个字节的最高位作为标识符，当字节最高位为1时表示编码未结束，当字节最高位为0时表示编码结束。因此，LevelDB的整型变长编码每8位用来表示整型值的7位。因此，当整型值接近其类型最大值时，变长编码需要额外一字节来容纳原整型值。 同样，变长整型编码也采用了小端顺序： // 笔者注：Encode char* EncodeVarint32(char* dst, uint32_t v) { // Operate on characters as unsigneds uint8_t* ptr = reinterpret_cast\u003cuint8_t*\u003e(dst); static const int B = 128; if (v \u003c (1 \u003c\u003c 7)) { *(ptr++) = v; } else if (v \u003c (1 \u003c\u003c 14)) { *(ptr++) = v | B; *(ptr++) = v \u003e\u003e 7; } else if (v \u003c (1 \u003c\u003c 21)) { *(ptr++) = v | B; *(ptr++) = (v \u003e\u003e 7) | B; *(ptr++) = v \u003e\u003e 14; } else if (v \u003c (1 \u003c\u003c 28)) { *(ptr++) = v | B; *(ptr++) = (v \u003e\u003e 7) | B; *(ptr++) = (v \u003e\u003e 14) | B; *(ptr++) = v \u003e\u003e 21; } else { *(ptr++) = v | B; *(ptr++) = (v \u003e\u003e 7) | B; *(ptr++) = (v \u003e\u003e 14) | B; *(ptr++) = (v \u003e\u003e 21) | B; *(ptr++) = v \u003e\u003e 28; } return reinterpret_cast\u003cchar*\u003e(ptr); } char* EncodeVarint64(char* dst, uint64_t v) { static const int B = 128; uint8_t* ptr = reinterpret_cast\u003cuint8_t*\u003e(dst); while (v \u003e= B) { *(ptr++) = v | B; v \u003e\u003e= 7; } *(ptr++) = static_cast\u003cuint8_t\u003e(v); return reinterpret_cast\u003cchar*\u003e(ptr); } 在解码时，LevelDB只需要根据字节的最高位判断变长编码是否结束即可，这里不再赘述。另外，LevelDB提供了解码同时返回一些信息的方法，以方便在不通场景下的使用。 ","date":"2021-03-04","objectID":"/posts/code-reading/leveldb-made-simple/2-basic-data-structure/:3:2","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x02 Bisic Data Format [施工中]","uri":"/posts/code-reading/leveldb-made-simple/2-basic-data-structure/"},{"categories":["深入浅出LevelDB"],"content":"2.3 长度确定的Slice编码 长度确定的Slice的编码方式非常简单，只需要在原Slice之前加上用变长整型表示的Slice长度即可： void PutLengthPrefixedSlice(std::string* dst, const Slice\u0026 value) { PutVarint32(dst, value.size()); dst-\u003eappend(value.data(), value.size()); } 施工中 … …","date":"2021-03-04","objectID":"/posts/code-reading/leveldb-made-simple/2-basic-data-structure/:3:3","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x02 Bisic Data Format [施工中]","uri":"/posts/code-reading/leveldb-made-simple/2-basic-data-structure/"},{"categories":["深入浅出LevelDB"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 施工中 … …","date":"2021-03-04","objectID":"/posts/code-reading/leveldb-made-simple/1-overview/:0:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x01 Overview [施工中]","uri":"/posts/code-reading/leveldb-made-simple/1-overview/"},{"categories":["深入浅出LevelDB"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2021-03-04","objectID":"/posts/code-reading/leveldb-made-simple/0-introduction/:0:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x00 Intro [施工中]","uri":"/posts/code-reading/leveldb-made-simple/0-introduction/"},{"categories":["深入浅出LevelDB"],"content":"施工中… …","date":"2021-03-04","objectID":"/posts/code-reading/leveldb-made-simple/0-introduction/:1:0","tags":["LevelDB","LSM-Tree"],"title":"深入浅出LevelDB —— 0x00 Intro [施工中]","uri":"/posts/code-reading/leveldb-made-simple/0-introduction/"},{"categories":["深入浅出boltdb"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2021-01-26","objectID":"/posts/code-reading/boltdb-made-simple/4-transaction/:0:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x04 事务","uri":"/posts/code-reading/boltdb-made-simple/4-transaction/"},{"categories":["深入浅出boltdb"],"content":"0. 引言 boltdb是一个支持完整ACID事务的kv数据。虽然boltdb将“事务”封装成了tx.go中的Tx结构体，但boltdb中处处实现都与事务息息相关，Tx结构体只提供了事务的抽象。 因此本文将从整体的视角介绍事务与boltdb中事务的实现，并介绍tx.go与db.go中的源码。 ","date":"2021-01-26","objectID":"/posts/code-reading/boltdb-made-simple/4-transaction/:1:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x04 事务","uri":"/posts/code-reading/boltdb-made-simple/4-transaction/"},{"categories":["深入浅出boltdb"],"content":"1. 事务 ","date":"2021-01-26","objectID":"/posts/code-reading/boltdb-made-simple/4-transaction/:2:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x04 事务","uri":"/posts/code-reading/boltdb-made-simple/4-transaction/"},{"categories":["深入浅出boltdb"],"content":"1.1 事务的ACID性质 ACID性质（Atomicity原子性、Consistency一致性、Isolation隔离性、Durability持久性）的解释方式有很多，笔者比较倾向于英文wiki[引文1]和IBM Knowledge Center - ACID properties of transactions[引文2]中的描述。 引文1 Atomicity : Transactions are often composed of multiple statements. Atomicity guarantees that each transaction is treated as a single “unit”, which either succeeds completely, or fails completely: if any of the statements constituting a transaction fails to complete, the entire transaction fails and the database is left unchanged. An atomic system must guarantee atomicity in each and every situation, including power failures, errors and crashes. A guarantee of atomicity prevents updates to the database occurring only partially, which can cause greater problems than rejecting the whole series outright. As a consequence, the transaction cannot be observed to be in progress by another database client. At one moment in time, it has not yet happened, and at the next it has already occurred in whole (or nothing happened if the transaction was cancelled in progress). Consistency : Consistency ensures that a transaction can only bring the database from one valid state to another, maintaining database invariants: any data written to the database must be valid according to all defined rules, including constraints, cascades, triggers, and any combination thereof. This prevents database corruption by an illegal transaction, but does not guarantee that a transaction is correct. Referential integrity guarantees the primary key – foreign key relationship. Isolation : Transactions are often executed concurrently (e.g., multiple transactions reading and writing to a table at the same time). Isolation ensures that concurrent execution of transactions leaves the database in the same state that would have been obtained if the transactions were executed sequentially. Isolation is the main goal of concurrency control; depending on the method used, the effects of an incomplete transaction might not even be visible to other transactions. Durability : Durability guarantees that once a transaction has been committed, it will remain committed even in the case of a system failure (e.g., power outage or crash). This usually means that completed transactions (or their effects) are recorded in non-volatile memory. 引文2 Atomicity : All changes to data are performed as if they are a single operation. That is, all the changes are performed, or none of them are. For example, in an application that transfers funds from one account to another, the atomicity property ensures that, if a debit is made successfully from one account, the corresponding credit is made to the other account. Consistency : Data is in a consistent state when a transaction starts and when it ends. For example, in an application that transfers funds from one account to another, the consistency property ensures that the total value of funds in both the accounts is the same at the start and end of each transaction. Isolation : The intermediate state of a transaction is invisible to other transactions. As a result, transactions that run concurrently appear to be serialized. For example, in an application that transfers funds from one account to another, the isolation property ensures that another transaction sees the transferred funds in one account or the other, but not in both, nor in neither. Durability : After a transaction successfully completes, changes to data persist and are not undone, even in the event of a system failure. For example, in an application that transfers funds from one account to another, the durability property ensures that the changes made to each account will not be reversed. 关于ACID中的Isolation隔离性，可以分为多个隔离级别（Isolation levels）。关于隔离级别，笔者建议阅读英文wiki提供的描述[引文3]。 引文3 Serializable This is the highest isolation level. With a lock-based concurrency control DBMS implementation, serializability requires read and write locks (acquired on selected data) to be released at the end of the transaction. Also range-locks","date":"2021-01-26","objectID":"/posts/code-reading/boltdb-made-simple/4-transaction/:2:1","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x04 事务","uri":"/posts/code-reading/boltdb-made-simple/4-transaction/"},{"categories":["深入浅出boltdb"],"content":"1.2 boltdb中ACID的实现 在笔者看来，ACID性质的实现并不是相互孤立的，而是通过各种技术整体实现的。但是为了理解的清晰，这里简要划分一下boltdb中各种技术与ACID间的关系。 Atomicity（原子性）： boltdb中事务的原子性是通过Shadow Paging实现的。当事务中的操作修改boltdb中的数据时，其不会直接修改数据库文件（mmap memory中的page），而是将更新内容写入到page buffer中。在事务提交时，会一并将这些page buffer中的dirty page写入到底层数据库文件，然后更新元数据将其指向新的页。因此，事务没有中间状态：要么全部写入，要么因回滚被丢弃。在元数据更新前，其指向的是事务执行前的旧page。因此，如果数据库在page buffer写入后且在元数据更新前故障，则数据仍保持在事务提交前的状态，而没有中间状态。 Consistency（一致性）： 数据库的“Consistency一致性”指事务前后的数据是否符合约束，有些资料也称其为“数据完整性”或“数据有效性”，论文《A relational model of data for large shared data banks》中有对其概念的形式化描述，感兴趣的读者可以看一下。 虽然boltdb不支持用户自定义约束，但笔者认为不保证正确性，boltdb中B+Tree结构也作为一种隐式约束。boltdb事务提交时，会通过rebalance与spill方法调整B+Tree结构，以使其满足B+Tree的性质。 有一种对ACID的解释为：Consistency是最终要实现的目标，而Atomicity、Isolation、Durability是实现Consistency的保证。笔者也比较认同这一观点，这也体现了ACID各个性质的实现不是孤立的，而是整个系统的结果。 Isolation（隔离性）： 上一节介绍了Isolation隔离性对应的4种隔离级别，boltdb实现的是最高的隔离界别：serializable序列化读。在serializable的基础上，boltdb支持“读读并发”与“读写并发”，boltdb中同时可以执行若干个只读事务，但同时只能执行一个读写事务，但只读事务与读写事务之间不影响。 Shadow Paging同样为实现事务隔离提供了支持。为了保证serializable的同时实现读写并发，当读写事务提交时，boltdb不会立即回收其不再使用的页（shadow page），这些页仍在freelist中该事务的pending列表中，因为此时这些页可能还在被未完成的只读事务读取。取而代之的是，boltdb会在事务开始时为其分配事务idtxid，只读事务的txid为当前数据库的txid，读写事务的txid为当前数据库的txid + 1。boltdb会记录正在执行的事务的事务id；当创建读写事务时，boltdb会从只读事务中找到进行中的最小的txid，显然，该txid之前的读写事务的shadow page不再需要被读取，此时可以安全地释放这些读写事务的shadow page，即可以freelist中该事务的pending列表中的页合并到freelist的ids中。 Shadow Paging保证了读读并发、读写并发的事务隔离性，boltdb还需要保证最多只有1个读写事务在进行。boltdb的读写事务开始前会申请互斥锁，以避免读写事务并行执行。这里需要注意两点：第一，因为boltdb支持读写并发，所以只读事务不需要申请S锁，否则只有读读事务才能并行执行；第二，在数据库领域，这种锁机制应叫做“latch”而非“lock”，只是其粒度较大。CMU 15-721中较为详细地介绍了Lock与Latch的区别，这里笔者搬运一下其总结表格。 Locks Latches Separate … User transactions Threads Proetect … Database contents In-memory data structures During … Entire transactions Critical sections Modes … Shared, exclusive, update, intention, escrow, schema, etc. Read, writes, (perhaps) update Deadlock … Detection \u0026 resolution Avoidance … by … Aanlysis of the waits-for graph, timeout, transaction abort, partial rollback, lock de-escalation Coding discipline, “lock leveling” Kept in … Lock manager’s hash table Protected data structure Durability（持久性）： boltdb的读写事务提交时，会通过pwrite系统调用写底层文件，并通过fdatasync系统调用确保数据被安全写入到磁盘中。因为boltdb的mmap模式为MAP_SHARED，因此绕过mmap直接写入底层文件不会影响mmap中数据对底层文件修改的可见性。 ","date":"2021-01-26","objectID":"/posts/code-reading/boltdb-made-simple/4-transaction/:2:2","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x04 事务","uri":"/posts/code-reading/boltdb-made-simple/4-transaction/"},{"categories":["深入浅出boltdb"],"content":"2. boltdb中事务的封装与实现 boltdb将事务封装成了tx.go中的Tx结构体。但只从Tx结构体分析boltdb中事务的封装与实现是不够的。因此，本节将先介绍Tx结构体的基本实现，然后按照事务的生命周期的顺序，介绍boltdb中tx.go与db.go中对事务的封装与实现。 ","date":"2021-01-26","objectID":"/posts/code-reading/boltdb-made-simple/4-transaction/:3:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x04 事务","uri":"/posts/code-reading/boltdb-made-simple/4-transaction/"},{"categories":["深入浅出boltdb"],"content":"2.1 Tx结构体 Tx结构体的源码如下： // txid represents the internal transaction identifier. type txid uint64 // Tx represents a read-only or read/write transaction on the database. // Read-only transactions can be used for retrieving values for keys and creating cursors. // Read/write transactions can create and remove buckets and create and remove keys. // // IMPORTANT: You must commit or rollback transactions when you are done with // them. Pages can not be reclaimed by the writer until no more transactions // are using them. A long running read transaction can cause the database to // quickly grow. type Tx struct { writable bool managed bool db *DB meta *meta root Bucket pages map[pgid]*page stats TxStats commitHandlers []func() // WriteFlag specifies the flag for write-related methods like WriteTo(). // Tx opens the database file with the specified flag to copy the data. // // By default, the flag is unset, which works well for mostly in-memory // workloads. For databases that are much larger than available RAM, // set the flag to syscall.O_DIRECT to avoid trashing the page cache. WriteFlag int } 字段 描述 writable bool true表示当前事务为读写事务，false表示当前事务为只读事务。 managed bool 标识当前事务是否为隐式事务，隐式事务由boltdb自动提交或回滚，用户不能主动提交或回滚。 db *DB 创建该事务的数据库对象。 meta *meta 当前事务创建时的meta拷贝。 root Bucket 当前事务所见的root bucket的Bucket实例。 page map[pgid]*page 索引当前事务所使用的dirty page（page buffer）。 stats TxStats 统计变量。 commitHandlers []func() 事务成功提交后需调用的回调函数列表。 WriteFlag int WriteTo方法reader打开文件时可配置的额外的flag。 Tx为boltdb的用户提供了一些方法来访问其中部分字段： 方法 描述 ID() int 返回当前事务id（tx.meta.txid）。 DB *DB 返回创建当前事务的数据库实例。 Size() int64 返回当前事务所见的数据库大小（非数据大小）。 Writable() bool 返回当前事务事务可写。 Stats() TxStats 返回当前事务的统计量。 此外，Tx还为boltdb的用户提供了一些访问root bucket的方法： 方法 描述 Cursor() *Cursor tx.root.Cursor()。从当前事务获取root bucket的Cursor。由于root bucket中只保存子bucket，因此其返回的所有value都是nil。 Bucket(name []byte) *Bucket tx.root.Bucket(name)。获取root bucket的子bucket。 CreateBucket(name []byte) (*Bucket, error) tx.root.CreateBucket(name) CreateBucketIfNotExists(name []byte) (*Bucket, error) tx.root.CreateBucketIfNotExists(name)。如果root bucket的子bucket未创建，则创建子bucket并返回实例；否则直接返回其实例。 DeleteBucket(name []byte) error tx.root.DeleteBucket(name)。删除root bucket的子bucket。 ForEach(fn func(name []byte, b *Bucket) error) error 遍历root bucket的所有子bucket并执行给定闭包。 ","date":"2021-01-26","objectID":"/posts/code-reading/boltdb-made-simple/4-transaction/:3:1","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x04 事务","uri":"/posts/code-reading/boltdb-made-simple/4-transaction/"},{"categories":["深入浅出boltdb"],"content":"2.2 事务的生命周期 本节将按照事务的生命周期，介绍并分析boltdb中事务的封装与实现。 在介绍事务的生命周期前，先简单介绍一下boltdb的DB中三把重要的锁： 字段 描述 rwlock sync.Mutex 用来隔离可写事务的互斥锁（注意，不是读写锁）。 metalock sync.Mutex 用来保护元数据访问的互斥锁。 mmaplock sync.RWMutex 用来保护mmap操作的读写锁。 boltdb支持“读读并发”与“读写并发”，用来隔离事务的锁rwlock是互斥锁，只有可写事务需要获取该锁，只读事务不受影响。由于事务开始时，需要复制当时的元数据，因此这里使用了互斥锁metalock来保护事务开始时的元数据访问，当事务初始化完成后就会释放metalock；另外，只读事务关闭时也需要获取metalock，但其目的是保护对DB对象的访问，而不时保护meta。而mmaplock是用来保护mmap操作的读写锁，只读事务会获取mmaplock的S锁，而mmap操作会获取mmaplock的X锁。这样，当可写事务需要更大的mmap空间时，其需要等待之前的只读事务都执行完毕，以避免只读事务引用的mmap地址失效；对于可写事务本身，其在mmap前会从根Bucket实例开始dereference操作，以避免可写事务本身引用了旧的mmap地址空间。 这三种锁的获取顺序是：（rwlock） $\\rightarrow$ metalock $\\rightarrow$ （mmaplock）。 此外，boltdb中还有两把锁。其一是读写锁statlock sync.RWMutex，其作用是保护统计量的访问，这里不作重点介绍；其二是互斥锁batchMu，该锁用来保护数据库实例的batch字段，作用较为单一，本文在2.3.2节介绍。 2.2.1 事务开始 boltdb的用户可以通过DB的Begin方法启动一个事务，通过Begin方法启动的事务需要用户自己控制其提交或回滚（用户还可以通过Update或View方法启动隐式事务，但二者都是对Begin的封装，因此放在最后介绍）。 Begin方法的实现如下： // Begin starts a new transaction. // Multiple read-only transactions can be used concurrently but only one // write transaction can be used at a time. Starting multiple write transactions // will cause the calls to block and be serialized until the current write // transaction finishes. // // Transactions should not be dependent on one another. Opening a read // transaction and a write transaction in the same goroutine can cause the // writer to deadlock because the database periodically needs to re-mmap itself // as it grows and it cannot do that while a read transaction is open. // // If a long running read transaction (for example, a snapshot transaction) is // needed, you might want to set DB.InitialMmapSize to a large enough value // to avoid potential blocking of write transaction. // // IMPORTANT: You must close read-only transactions after you are finished or // else the database will not reclaim old pages. func (db *DB) Begin(writable bool) (*Tx, error) { if writable { return db.beginRWTx() } return db.beginTx() } Begin方法会根据事务是否可写，调用beginRWTx方法或beginTx方法。 接下来首先分析启动只读事务beginTx方法的实现： func (db *DB) beginTx() (*Tx, error) { // Lock the meta pages while we initialize the transaction. We obtain // the meta lock before the mmap lock because that's the order that the // write transaction will obtain them. db.metalock.Lock() // Obtain a read-only lock on the mmap. When the mmap is remapped it will // obtain a write lock so all transactions must finish before it can be // remapped. db.mmaplock.RLock() // Exit if the database is not open yet. if !db.opened { db.mmaplock.RUnlock() db.metalock.Unlock() return nil, ErrDatabaseNotOpen } // Create a transaction associated with the database. t := \u0026Tx{} t.init(db) // Keep track of transaction until it closes. db.txs = append(db.txs, t) n := len(db.txs) // Unlock the meta pages. db.metalock.Unlock() // Update the transaction stats. db.statlock.Lock() db.stats.TxN++ db.stats.OpenTxN = n db.statlock.Unlock() return t, nil } beginTx方法执行了如下操作： 获取metalock锁与mmaplock的S锁。 检测数据库是否打开，如果没打开则释放锁并返回错误。 创建writable为false的Tx对象，调用init方法初始化Tx对象（Tx对象初始化时会复制当前的meta）。 将事务保存到DB的txs字段中。 释放metalock。 更新统计量，返回事务对象Tx。 beginRWTx方法实现与之相似： func (db *DB) beginRWTx() (*Tx, error) { // If the database was opened with Options.ReadOnly, return an error. if db.readOnly { return nil, ErrDatabaseReadOnly } // Obtain writer lock. This is released by the transaction when it closes. // This enforces only one writer transaction at a time. db.rwlock.Lock() // Once we have the writer lock then we can lock the meta pages so that // we can set up the transaction. db.metalock.Lock() defer db.metalock.Unlock() // Exit if the database is not open yet. if !db.opened { db.rwlock.Unlock() return nil, ErrDatabaseNotOpen } // Create a transaction associated with the database. t := \u0026Tx{writable: true} t.init(db) db.rwtx = t // Free any pages associated with closed read-only transactions. var minid txid = 0xFFFFFFFFFFFFFFFF for _, t := range db.txs { if t.meta.txid \u003c minid { minid = t.meta.txid } } if minid \u003e 0 { db.freelist.release(minid - 1) } ret","date":"2021-01-26","objectID":"/posts/code-reading/boltdb-made-simple/4-transaction/:3:2","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x04 事务","uri":"/posts/code-reading/boltdb-made-simple/4-transaction/"},{"categories":["深入浅出boltdb"],"content":"2.3 内置隐式事务 boltdb除了为用户提供了Begin方法来显式地启动读写事务或只读事务，其还提供一些内置的封装好的隐式事务方法，如Update、View与Batch。当用户只需要操作数据库而不需要关心何时提交或回滚时，可以使用这些方法。 2.3.1 隐式读写事务与隐式只读事务 Update与View分别是通过读写隐式事务与只读隐式事务操作数据库的方法。二者实现如下： // Update executes a function within the context of a read-write managed transaction. // If no error is returned from the function then the transaction is committed. // If an error is returned then the entire transaction is rolled back. // Any error that is returned from the function or returned from the commit is // returned from the Update() method. // // Attempting to manually commit or rollback within the function will cause a panic. func (db *DB) Update(fn func(*Tx) error) error { t, err := db.Begin(true) if err != nil { return err } // Make sure the transaction rolls back in the event of a panic. defer func() { if t.db != nil { t.rollback() } }() // Mark as a managed tx so that the inner function cannot manually commit. t.managed = true // If an error is returned from the function then rollback and return error. err = fn(t) t.managed = false if err != nil { _ = t.Rollback() return err } return t.Commit() } // View executes a function within the context of a managed read-only transaction. // Any error that is returned from the function is returned from the View() method. // // Attempting to manually rollback within the function will cause a panic. func (db *DB) View(fn func(*Tx) error) error { t, err := db.Begin(false) if err != nil { return err } // Make sure the transaction rolls back in the event of a panic. defer func() { if t.db != nil { t.rollback() } }() // Mark as a managed tx so that the inner function cannot manually rollback. t.managed = true // If an error is returned from the function then pass it through. err = fn(t) t.managed = false if err != nil { _ = t.Rollback() return err } if err := t.Rollback(); err != nil { return err } return nil } Update与View的参数是一个用来操作事务的方法闭包。这两个方法首先创建一个读写事务或只读事务，在执行方法闭包前先将managed字段置为true，以阻止用户在传入的方法闭包中手动提交或回滚事务，在执行后在将managed字段置为false，以便boltdb提交或回滚事务。 2.3.2 批处理隐式读写事务 每个Update操作都要等待磁盘I/O完成才能执行下一个Update操作，虽然这保证了事务特性，但是性能较差。boltdb还为用户提供了一个能够将并发的多个读写事务合并为一次事务的方法——Batch。虽然通过Batch能够减少并发读写事务等待磁盘I/O的开销，但是其对事务中的操作有一定要求：Batch中的事务可能被重试若干次（即使某个事务正常，也可能被重试，笔者会在后文分析其原因），因此这要求通过Batch执行的操作必须是幂等（idempotent）的，且只有调用者调用的Batch方法成功返回后，其变更才保证被永久写入到存储。boltdb中的Batch分批操作对用户使透明的，用户只需要像调用Update一样调用Batch，boltdb就会自动将其分批。 Batch方法使用到了batch结构体： type batch struct { db *DB timer *time.Timer start sync.Once calls []call } type call struct { fn func(*Tx) error err chan\u003c- error } batch结构体的calls字段记录了每批读写事务的方法闭包与错误返回信道。记录错误返回信道的作用是为了将每个事务的错误返回给相应地调用者。 数据库结构体db的实例的batch字段是指向当前正在等待积累的batch指针，当一批batch执行时，其会将该字段置为nil，下一次调用Batch时会创建新实例。 Batch方法的实现如下： // Batch calls fn as part of a batch. It behaves similar to Update, // except: // // 1. concurrent Batch calls can be combined into a single Bolt // transaction. // // 2. the function passed to Batch may be called multiple times, // regardless of whether it returns error or not. // // This means that Batch function side effects must be idempotent and // take permanent effect only after a successful return is seen in // caller. // // The maximum batch size and delay can be adjusted with DB.MaxBatchSize // and DB.MaxBatchDelay, respectively. // // Batch is only useful when there are multiple goroutines calling it. func (db *DB) Batch(fn func(*Tx) error) error { errCh := make(chan error, 1) db.batchMu.Lock() if (db.batch == nil) || (db.batch != nil \u0026\u0026 len(db.batch.calls) \u003e= db.MaxBatchSize) { // There is no existing batch, or the existing batch is full; start a new one. db.batch = \u0026batch{ db: db, } db.batch.timer = time.AfterFunc(db.MaxBatchDelay, db.batch.trigger) } db.batch.calls = append(db.batch.calls, call{fn: fn, err: errCh}) if len(db.batch.calls) \u003e= db.MaxBatchSize { // wake up batch, it's ready to run go db.batch.trigger() } db.batchMu.Unlock() err := \u003c-errCh if err == trySolo { err = db.Update(fn) } return err } 在Batch方法中，其通过互斥锁batchMu保护了对db实例的batch字段的访问。如果batch为空或者已满时，创建新的batch实例，并为其注册","date":"2021-01-26","objectID":"/posts/code-reading/boltdb-made-simple/4-transaction/:3:3","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x04 事务","uri":"/posts/code-reading/boltdb-made-simple/4-transaction/"},{"categories":["深入浅出boltdb"],"content":"3. 总结 本文介绍了事务的基本概念与boltdb中事务的相关实现。在boltdb的实现中，事务在各方各面都有体现，其ACID的实现也相辅相成。 关于boltdb的源码分析在这里也告一段落了，db.go中的重要代码已经在本系列各篇文章中分散地介绍过，这里也不再赘述。 ","date":"2021-01-26","objectID":"/posts/code-reading/boltdb-made-simple/4-transaction/:4:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x04 事务","uri":"/posts/code-reading/boltdb-made-simple/4-transaction/"},{"categories":["深入浅出boltdb"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2021-01-20","objectID":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/:0:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x03 bucket \u0026 cursor","uri":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/"},{"categories":["深入浅出boltdb"],"content":"0. 引言 在深入浅出boltdb —— 0x02 B+Tree中，笔者介绍了boltdb中B+Tree的实现。boltdb将B+Tree进一步封装成了bucket以便用户使用。 与大多数存储系统一样，bucket是一系列key/value的集合；同时，boltdb支持bucket无限嵌套。例如，一个银行的数据可以通过如下的多层嵌套的bucket以及其中的key/value表示： bucket嵌套bucket嵌套 \" bucket嵌套 在boltdb中，每个桶都是一棵B+Tree，为了便于用户访问桶中B+Tree的节点，boltdb实现了cursor游标。 本文，笔者将分析介绍boltdb中桶与游标的实现。 ","date":"2021-01-20","objectID":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/:1:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x03 bucket \u0026 cursor","uri":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/"},{"categories":["深入浅出boltdb"],"content":"1. bucket 与B+Tree的节点类似，bucket也是按需打开的。因此，分析boltdb中bucket也可以从存储与内存两方面入手。 本节笔者将先介绍boltdb中bucket的存储结构，然后再介绍当bucket被事务打开时，其内存结构与相关方法。 ","date":"2021-01-20","objectID":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/:2:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x03 bucket \u0026 cursor","uri":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/"},{"categories":["深入浅出boltdb"],"content":"1.1 bucket的存储结构 1.1.1 bucket与B+Tree 在boltdb中，每个bucket中的数据都是一棵独立的B+Tree。boltdb中B+Tree的节点是通过page存储的，分支节点branchNodePage保存了每个孩子的key和pgid，叶子节点leafNodePage保存了其中每个元素的key/value与元素类型（表示普通kv还是表示bucket）。因此，boltdb的一棵B+Tree的存储结构可表示为下图中的样子。 boltdb中B+Tree存储结构示意图boltdb中B+Tree存储结构示意图 \" boltdb中B+Tree存储结构示意图 boltdb除了需要根据bucket的名称（name）找到其相应的B+Tree的根节点的pgid（root）外，还需要为每个bucket保存一个64为整型值，以便实现生成单调递增的序列号的功能（便于并行程序存储中间结果或实现锁）。这样，bucket的元数据也可以表示为一个key/value对，即$name -\u003e (root,sequence)$，而boltdb也确实是这样实现的。 在boltdb中，bucekt是支持嵌套的，boltdb的数据库元数据meta中只保存了根bucket（即meta结构体中的root字段），所有嵌套的bucket组成了一个多叉树型结构，如下图所示。 bucket多叉树型结构示意图bucket多叉树型结构示意图 \" bucket多叉树型结构示意图 提示 boltdb的root bucket只保存子bucket，而不保存键值对。 既然bucket的元数据可以表示为key/value对，而bucket中存储的数据也是key/value对，且bucket是支持嵌套的，所以在boltdb中，bucket的元数据也以key/value的形式保存在其父bucket的B+Tree中叶子节点的元素中，只是该元素的flag字段有1位标识了该key/value表示的是bucket。只有根bucket不同，根bucket不需要通过name来索引，因此其只需要保存相当于其他bucket的元数据中value的数据，并直接通过boltdb的meta结构体的root字段索引。 例如，有3个bucketb0、b1、b2，其相应的B+Tree的根节点所在page分别为r0、r1、r2，b0是b1与b2的父bucket。其存储结构如下图所示： bucket与B+Tree示意图bucket与B+Tree示意图 \" bucket与B+Tree示意图 从上图中可以看出，b1和b2的元数据，分别保存在了b0的第0个（page l0的elem 0）和第11个（page l2的elem 3）键值对中。其中，键值对的key即为bucket的name，value为bucket结构体，该结构体由bucket的B+Tree树根节点的pgid root和64位整型序列号sequence组成： // bucket represents the on-file representation of a bucket. // This is stored as the \"value\" of a bucket key. If the bucket is small enough, // then its root page can be stored inline in the \"value\", after the bucket // header. In the case of inline buckets, the \"root\" will be 0. type bucket struct { root pgid // page id of the bucket's root-level page sequence uint64 // monotonically incrementing, used by NextSequence() } 1.1.2 inline bucket 由于每个bucket都是一棵B+Tree，而B+Tree至少需要有一个根节点，且boltdb中每个节点都是一个page，那么如果boltdb中有很多数据量达不到一页的bucket，则会浪费很多空间。为了避免这一问题，对于数据量较小的bucket，boltdb会将其数据放在其元数据后，直接保存在key/value结构的value中，此时该bucket的元数据的root值为0，其被称为inline bucket。为了简化实现，boltdb在保存inline bucket时采用了类似虚拟内存的做法：其数据仍按照page的方式组织，但是其实际大小并非真正的page大小。普通的bucket与inline bucket的存储结构示意图如下图所示。 普通bucket与inline bucket存储结构示意图普通bucket与inline bucket存储结构示意图 \" 普通bucket与inline bucket存储结构示意图 boltdb中判断bucket是否作为inline bucket存储的方法为inlineable。 // inlineable returns true if a bucket is small enough to be written inline // and if it contains no subbuckets. Otherwise returns false. func (b *Bucket) inlineable() bool { var n = b.rootNode // Bucket must only contain a single leaf node. if n == nil || !n.isLeaf { return false } // Bucket is not inlineable if it contains subbuckets or if it goes beyond // our threshold for inline bucket size. var size = pageHeaderSize for _, inode := range n.inodes { size += leafPageElementSize + len(inode.key) + len(inode.value) if inode.flags\u0026bucketLeafFlag != 0 { return false } else if size \u003e b.maxInlineBucketSize() { return false } } return true } // Returns the maximum total size of a bucket to make it a candidate for inlining. func (b *Bucket) maxInlineBucketSize() int { return b.tx.db.pageSize / 4 } 该方法判断bucket是否满足以下几点： bucket是否只有一个节点（即根节点为叶子节点）。 bucket中是否不包含子bucket。 bucket中数据大小是否小于阈值（默认为$\\frac{1}{4}$）。 如果以上3条都满足，那么该方法返回true，该bucket将作为inline bucket存储。 1.1.3 bucket元数据的序列化与反序列化 由于bucket需要存储的元数据较少，且bucket的元数据是作为B+Tree中的key/value保存的，因此bucket的序列化与反序列化方法较为简单。 在序列化普通bucket时，只需要序列化其元数据，因此直接深拷贝bucket结构体即可。相关代码在spill方法中（该方法序列化的是子bucket的元数据），如下所示： value = make([]byte, unsafe.Sizeof(bucket{})) var bucket = (*bucket)(unsafe.Pointer(\u0026value[0])) *bucket = *child.bucket 而write是序列化inline bucket的方法，其实现方式如下： // write allocates and writes a bucket to a byte slice. func (b *Bucket) write() []byte { // Allocate the appropriate size. var n = b.rootNode var value = make([]byte, bucketHeaderSize+n.size()) // Write a bucket header. var bucket = (*bucket)(unsafe.Pointer(\u0026value[0])) *bucket = *b.bucket // Convert byte slice to a fake page and write the root node. var p = (*page)(unsafe.Pointer(\u0026value[bucketHeaderSize])) n.write(p) return value } write方法的实现非常简单。其除了写入了bucket结构体中的数据，还将value的剩余空间作为虚拟页，将该bucket中唯一的B+Tree节点（也是根节点）的数据写入到该虚拟页中。这类似于操作系统中虚拟内存的实现。","date":"2021-01-20","objectID":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/:2:1","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x03 bucket \u0026 cursor","uri":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/"},{"categories":["深入浅出boltdb"],"content":"1.2 bucket的内存结构 当bucket被事务打开时，boltdb还需要记录bucket打开过的node、打开过的子bucket等信息，以避免同一事务重复打开bucket或node，同时在事务提交时便于找到所有相关数据一并写入到存储。 被事务打开的bucket在内存中表示为Bucket结构体，其包含字段如下： // Bucket represents a collection of key/value pairs inside the database. type Bucket struct { *bucket tx *Tx // the associated transaction buckets map[string]*Bucket // subbucket cache page *page // inline page reference rootNode *node // materialized node for the root page. nodes map[pgid]*node // node cache // Sets the threshold for filling nodes when they split. By default, // the bucket will fill to 50% but it can be useful to increase this // amount if you know that your write workloads are mostly append-only. // // This is non-persisted across transactions so it must be set in every Tx. FillPercent float64 } 字段 描述 *bucket bucket需要存储的元数据的value部分（详见上节），被只读事务打开的Bucket中该指针指向mmap memory，被读写事务打开的Bucket中该指针指向heap memory。 tx *Tx 保存打开该bucket的事务实例。 buckets map[string]*Bucket 记录打开的子bucket。 page *page 如果该bucket为inline bucket，那么该字段指向了其虚拟页的位置。 rootNode *node 用来记录该bucket的B+Tree根节点实例化后的node（根节点同样是按需实例化的，因此该字段可能为nil）。 nodes map[pgid]*node 用来记录该bucket的B+Tree中已实例化的node。 FillPercent float64 bucket中B+Tree的填充率阈值。 Bucket结构体中的字段基本都是对外不可见的，boltdb的用户需要通过Bucket提供的一些可见的方法来访问这些字段： 方法 描述 Tx() *Tx 返回打开该bucket的事务。 Root() pgid 返回该bucket的B+Tree的根节点的pgid。 Writable() bool 返回该bucket是否可写（打开该bucket的事务是否可写）。 ","date":"2021-01-20","objectID":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/:2:2","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x03 bucket \u0026 cursor","uri":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/"},{"categories":["深入浅出boltdb"],"content":"1.3 Bucket的操作与实现 boltdb为用户提供了创建、打开、删除bucket，与对bucket中数据进行增删改查的方法。 本节笔者将介绍这些方法与其实现。 1.3.1 Coursor Bucket中许多操作需要依赖游标Cursor，游标是Bucket用来遍历B+Tree寻找key/value的工具。Cursor的实现笔者放在本文后面的部分介绍，这里读者这需要知道Cursor的作用即可。游标的获取方法为Cursor： // Cursor creates a cursor associated with the bucket. // The cursor is only valid as long as the transaction is open. // Do not use a cursor after the transaction is closed. func (b *Bucket) Cursor() *Cursor { // Update transaction statistics. b.tx.stats.CursorCount++ // Allocate and return a cursor. return \u0026Cursor{ bucket: b, stack: make([]elemRef, 0), } } Cursor的生命周期与打开该Bucket的事务的声明周期相同，在使用boltdb时需要注意。 1.3.2 B+Tree节点的访问 由于Bucket操作涉及到B+Tree的更新，因此这里先介绍Bucket与Cursor访问或打开B+Tree节点的操作。这一部分与Cursor的实现关系更大，但由于bucket.go中包含了相关代码，因此笔者在这里先介绍B+Tree节点的访问方式。 当Cursor只需要读取B+Tree的内容时，其只需要根据节点的pgid，在mmap memory中找到相应位置读取即可；而在Cursor需要更新B+Tree时，由于boltdb只读取mmap memory中的内容，因此需要先读取page并实例化相应的node（但此时node的key/value还是直接指向mmap memory）。Bucket实例化node的方法是node： // node creates a node from a page and associates it with a given parent. func (b *Bucket) node(pgid pgid, parent *node) *node { _assert(b.nodes != nil, \"nodes map expected\") // Retrieve node if it's already been created. if n := b.nodes[pgid]; n != nil { return n } // Otherwise create a node and cache it. n := \u0026node{bucket: b, parent: parent} if parent == nil { b.rootNode = n } else { parent.children = append(parent.children, n) } // Use the inline page if this is an inline bucket. var p = b.page if p == nil { p = b.tx.page(pgid) } // Read the page into the node and cache it. n.read(p) b.nodes[pgid] = n // Update statistics. b.tx.stats.NodeCount++ return n } node方法执行了如下操作： 检查当前Bucket的nodes字段是否记录了该pgid，如果记录存在，说明当前事务已经实例化了该node，因此直接返回记录中缓存的node即可。 如果还没打开过，则实例化新node并缓存，同时设置node的部分字段。 选择需要读取的page，如果当前bucket不是inline bucket，则通过事务实例获取传入的pgid相应的page的指针（如果page被修改，该方法会返回page buffer，否则返回mmap中的page）；否则，直接使用bucket的虚拟页。 调用该node的read方法，读取page数据并构建node，同时将该node记录到当前bucket的nodes字段中。 更新统计变量，返回node实例。 node方法时是明确需要更新节点时才需要调用的。而如果只需要读取节点，Bucket提供了pageNode方法，该方法会返回给定pgid相应的page或node。即如果该节点已被实例化为node，则返回node，否则直接返回page： // pageNode returns the in-memory node, if it exists. // Otherwise returns the underlying page. func (b *Bucket) pageNode(id pgid) (*page, *node) { // Inline buckets have a fake page embedded in their value so treat them // differently. We'll return the rootNode (if available) or the fake page. if b.root == 0 { if id != 0 { panic(fmt.Sprintf(\"inline bucket non-zero page access(2): %d != 0\", id)) } if b.rootNode != nil { return nil, b.rootNode } return b.page, nil } // Check the node cache for non-inline buckets. if b.nodes != nil { if n := b.nodes[id]; n != nil { return nil, n } } // Finally lookup the page from the transaction if no node is materialized. return b.tx.page(id), nil } 该方法首先判断当前bucket是否为inline bucket，如果是那么直接返回其虚拟页；否则，检查nodes中是否缓存了相应的node，如果缓存中有则返回已实例化的node，否则通过事务返回相应的page。 1.3.3 bucket中B+Tree的rebalance与spill 当事务提交时，其需要从根bucket开始递归进行rebalance与spill操作，以调整所有修改过的bucket的B+Tree的结构。因此，bucket需要提供rebalance与spill方法，以封装其递归过程。 Bucket的rebalance方法实现如下： // rebalance attempts to balance all nodes. func (b *Bucket) rebalance() { for _, n := range b.nodes { n.rebalance() } for _, child := range b.buckets { child.rebalance() } } 该方法实现非常简单，首先其遍历了当前Bucket对象的nodes中缓存的node，调用rebalance方法，然后递归遍历buckets中缓存的子bucket的Bucket实例，调用rebalance方法。 相比rebalance方法，Bucket的spill方法实现稍微复杂一些： // spill writes all the nodes for this bucket to dirty pages. func (b *Bucket) spill() error { // Spill all child buckets first. for name, child := range b.buckets { // If the child bucket is small enough and it has no child buckets then // write it inline into the parent bucket's page. Otherwise spill it // like a normal bucket and make the parent value a pointer to the page. var value []byte if child.inlineable() { child.free() value = child.write() } else { if err := child.spill(); err != nil { return err } // Update the child bucket header in this bucket. value = make([]byte, unsafe.Sizeof(bucket{})) var bucket = (*bucket)(unsafe.P","date":"2021-01-20","objectID":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/:2:3","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x03 bucket \u0026 cursor","uri":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/"},{"categories":["深入浅出boltdb"],"content":"2. cursor 游标cursor是boltdb中用来遍历B+Tree访问其中key/value的工具。由于boltdb的B+Tree叶子节点没有实现链指针，因此其cursor实现中通过栈记录了根节点到当前节点的路径，以便于访问前驱或后继key/value。 boltdb的cursor实现主要在cursor.go中，由Cursor结构体实现，本节将介绍其实现方式。 ","date":"2021-01-20","objectID":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/:3:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x03 bucket \u0026 cursor","uri":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/"},{"categories":["深入浅出boltdb"],"content":"2.1 Cursor结构体 Cursor结构体中只有两个字段，其分别记录了Cursor所属的Bucket实例，与从根节点到当前key/value的路径。用于可以通过其Bucket方法获取其所属Bucket实例。 // Cursor represents an iterator that can traverse over all key/value pairs in a bucket in sorted order. // Cursors see nested buckets with value == nil. // Cursors can be obtained from a transaction and are valid as long as the transaction is open. // // Keys and values returned from the cursor are only valid for the life of the transaction. // // Changing data while traversing with a cursor may cause it to be invalidated // and return unexpected keys and/or values. You must reposition your cursor // after mutating data. type Cursor struct { bucket *Bucket stack []elemRef } // Bucket returns the bucket that this cursor was created from. func (c *Cursor) Bucket() *Bucket { return c.bucket } 由于在不需要更新时，Cursor直接通过B+Tree节点的pgid访问mmap memory中的页，只有在需要更新时才会将其实例化为node。因此，在Cursor遍历节点的过程中，不同路径可能既有通过page表示的节点，也可能有通过node表示的节点。因此，stack字段记录的节点结构elemRef中有page指针page、node指针node、还有表示位于节点的第几个元素（element或inode）的索引index，在访问时，如果node为空，则通过page访问。 // elemRef represents a reference to an element on a given page/node. type elemRef struct { page *page node *node index int } // isLeaf returns whether the ref is pointing at a leaf page/node. func (r *elemRef) isLeaf() bool { if r.node != nil { return r.node.isLeaf } return (r.page.flags \u0026 leafPageFlag) != 0 } // count returns the number of inodes or page elements. func (r *elemRef) count() int { if r.node != nil { return len(r.node.inodes) } return int(r.page.count) } Cursor的栈顶元素即为当前Cursor所在位置，keyValue方法是Cursor用来获取当前键值对的方法，其会根据当前节点是page还是node，通过不同方式获取键值对： // keyValue returns the key and value of the current leaf element. func (c *Cursor) keyValue() ([]byte, []byte, uint32) { ref := \u0026c.stack[len(c.stack)-1] if ref.count() == 0 || ref.index \u003e= ref.count() { return nil, nil, 0 } // Retrieve value from node. if ref.node != nil { inode := \u0026ref.node.inodes[ref.index] return inode.key, inode.value, inode.flags } // Or retrieve value from page. elem := ref.page.leafPageElement(uint16(ref.index)) return elem.key(), elem.value(), elem.flags } ","date":"2021-01-20","objectID":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/:3:1","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x03 bucket \u0026 cursor","uri":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/"},{"categories":["深入浅出boltdb"],"content":"2.2 Cursor的方法与实现 Cursor结构体提供了移动到第一个或最后一个键值对、移动到前一个或后一个键值对、通过二分查找的方式移动到给定键位置、及删除当前位置的键值对的方法（用户不能指定插入位置，避免破坏B+Tree结构），本节，笔者将依次介绍这些方法的实现。 注意 当事务删除了key但还未提交时，B+Tree中部分叶子节点可能没有内部元素。此时，应该跳过空节点。这一问题在pull#452中修复，其修复方式为在部分方法的实现最外成加入for循环，如果访问到空节点则进入下一次循环继续寻找，相关位置一般还有相应的注释“// If we land on an empty page then move to the next value. https://github.com/boltdb/bolt/issues/450”。后文分析源码中不再赘述。 2.2.1 First、Last、first、last First方法与Last方法是将Cursor移动到第一个或最后一个键值对处的方法，二者实现方式相似，这里一同介绍。 // First moves the cursor to the first item in the bucket and returns its key and value. // If the bucket is empty then a nil key and value are returned. // The returned key and value are only valid for the life of the transaction. func (c *Cursor) First() (key []byte, value []byte) { _assert(c.bucket.tx.db != nil, \"tx closed\") c.stack = c.stack[:0] p, n := c.bucket.pageNode(c.bucket.root) c.stack = append(c.stack, elemRef{page: p, node: n, index: 0}) c.first() // If we land on an empty page then move to the next value. // https://github.com/boltdb/bolt/issues/450 if c.stack[len(c.stack)-1].count() == 0 { c.next() } k, v, flags := c.keyValue() if (flags \u0026 uint32(bucketLeafFlag)) != 0 { return k, nil } return k, v } // Last moves the cursor to the last item in the bucket and returns its key and value. // If the bucket is empty then a nil key and value are returned. // The returned key and value are only valid for the life of the transaction. func (c *Cursor) Last() (key []byte, value []byte) { _assert(c.bucket.tx.db != nil, \"tx closed\") c.stack = c.stack[:0] p, n := c.bucket.pageNode(c.bucket.root) ref := elemRef{page: p, node: n} ref.index = ref.count() - 1 c.stack = append(c.stack, ref) c.last() k, v, flags := c.keyValue() if (flags \u0026 uint32(bucketLeafFlag)) != 0 { return k, nil } return k, v } First与Last方法本身并没有移动Cursor的实现，其只获取了bucket中B+Tree的根节点（page或node），并将其加入到stack中。真正实现移动Cursor的是first与last方法（分开实现以便其它方法复用），first与last方法会以stack中栈顶节点作为根节点，将Cursor移动到其下第一个键值对位置。First方法与Last方法除了设置查找起点外，还会检查first方法与last方法返回的键值对是否表示bucket，如果是bucket，则将value置为nil返回。first与last的实现如下： // first moves the cursor to the first leaf element under the last page in the stack. func (c *Cursor) first() { for { // Exit when we hit a leaf page. var ref = \u0026c.stack[len(c.stack)-1] if ref.isLeaf() { break } // Keep adding pages pointing to the first element to the stack. var pgid pgid if ref.node != nil { pgid = ref.node.inodes[ref.index].pgid } else { pgid = ref.page.branchPageElement(uint16(ref.index)).pgid } p, n := c.bucket.pageNode(pgid) c.stack = append(c.stack, elemRef{page: p, node: n, index: 0}) } } // last moves the cursor to the last leaf element under the last page in the stack. func (c *Cursor) last() { for { // Exit when we hit a leaf page. ref := \u0026c.stack[len(c.stack)-1] if ref.isLeaf() { break } // Keep adding pages pointing to the last element in the stack. var pgid pgid if ref.node != nil { pgid = ref.node.inodes[ref.index].pgid } else { pgid = ref.page.branchPageElement(uint16(ref.index)).pgid } p, n := c.bucket.pageNode(pgid) var nextRef = elemRef{page: p, node: n} nextRef.index = nextRef.count() - 1 c.stack = append(c.stack, nextRef) } } first与last循环查找栈顶节点的第一个或最后一个孩子并将其压入栈中，直到栈顶节点为叶子节点。其实现方式也较为简单这里不再赘述。 2.2.2 Next、Prev、next Next方法与Prev方法是将Cursor移动到当前键值对的前驱或后继键值对的方法。由于遍历Bucket的ForEach方法依赖Next，而原Next方法可能存在issue#450中的问题，因此其又封装了next函数，并通过循环跳过空叶子节点。而Prev则不存在这一问题，因此没有封装prev方法。 // Next moves the cursor to the next item in the bucket and returns its key and value. // If the cursor is at the end of the bucket then a nil key and value are returned. // The returned key and value are only valid for the life of the transaction. func (c *Cursor) Next() (key []byte, value []byte) { _assert(c.bucket.tx.db != nil, \"tx closed\") k, v, flags := c.next() if (flags \u0026 uint32(bucketLeafFlag)) != 0 { return k, nil } return k, v } // next moves to the next leaf element and returns the key and value. // If the cursor is at the last leaf element then it stays there and returns nil. func (c *Cursor) next() (key []byt","date":"2021-01-20","objectID":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/:3:2","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x03 bucket \u0026 cursor","uri":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/"},{"categories":["深入浅出boltdb"],"content":"3. 总结 本文介绍了boltdb中bucket与cursor的概念与实现。 bucket与cursor是基于B+Tree的封装，且部分面向用户，虽然其实现并不复杂，但对于刚接触的读者可能较难理解。 ","date":"2021-01-20","objectID":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/:4:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x03 bucket \u0026 cursor","uri":"/posts/code-reading/boltdb-made-simple/3-bucket-cursor/"},{"categories":["深入浅出boltdb"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2021-01-19","objectID":"/posts/code-reading/boltdb-made-simple/2-b+tree/:0:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x02 B+Tree","uri":"/posts/code-reading/boltdb-made-simple/2-b+tree/"},{"categories":["深入浅出boltdb"],"content":"0. 引言 boltdb是需要通过磁盘来持久化数据的kv数据库。为了平衡内存与磁盘的读写性能，boltdb使用了B+Tree来保存并索引数据。B+Tree（B+树）是B-Tree（B树）的一种变体，本文不会详细介绍有关B-Tree或B+Tree的具体操作，而仅给出B-Tree与B+Tree的概念，并通过boltdb对B+Tree的实现介绍。 一棵$m$阶B-Tree的性质如下： 每个节点最多有$m$个孩子。 除根节点外的每个非叶子节点至少有$\\lceil m/2 \\rceil$个孩子。 如果根节点不是叶子节点，那么根节点至少有2个孩子。 非叶子节点有$k$个孩子，$k-1$个key，key从小到大排列，且$k-1$个key划分了$k$个孩子中key的范围。 所有叶子节点都在同一层。 B-Tree有如下的优点： 树高度低，$\\lceil \\log_{m}{(n+1)} \\rceil -1 \\le h \\le \\lfloor \\log_{ \\lceil m/2 \\rceil }{\\frac{n+1}{2}} \\rfloor$。底数为$O(m)$。 与平衡二叉树相比，I/O次数少，适合基于磁盘的实现。 读取节点数据时能够充分利用缓存。 B+Tree在B-Tree的基础上，进一步做了一些优化。一棵$m$阶B+Tree的性质如下： 每个节点最多有$m$个孩子。 除根节点外的每个非叶子节点至少有$\\lceil m/2 \\rceil$个孩子。 中间节点的key只作为索引，key同时存在于其孩子中，且是相应孩子的最大（或最小）的key。 数据由且仅由叶子节点保存，叶子节点还有一个链指针，按从小到大的顺序指向下一个叶子节点。 相比B-Tree，B+Tree还有如下优点： 中间节点不保存数据，一个块中能保存更多元素，I/O次数更少。 所有查询都要落到叶子节点，性能更加稳定。 叶子节点形成有序链表，范围查询更方便。 无论B-Tree还是B+Tree都是为优化磁盘I/O设计的高级数据结构。其实现涉及到很多与I/O、缓存相关问题。因此，B+Tree并没有一种“标准”的实现，B+Tree的设计也与存储系统的使用场景、硬件环境密相关。由于boltdb是一个基于磁盘的kv数据库，其保存的key与value都是变长的，因此，boltdb中B+Tree的实现有如下特点： 由于boltdb的key是变长的，其没有使用$m$阶的概念，而是使用数据填充率来限制节点大小。数据填充率为节点大小与pageSize的比值。理论上，节点的数据填充率不能大于1，除根节点外的每个非叶子节点的数据填充率不小于设置的参数fillPercent。但实际上，由于一些key/value数据过大，节点填充率可能超过1但不可再分，这样的节点也是合法的节点。 叶子节点的value直接保存在叶子节点中，而不是通过指针指向其它页。这样可以减少一次I/O且在范围查询时可以充分利用缓存。 为了简化实现，boltdb的叶子节点没有实现链指针。因此在进行范围查询时，cursor需要通过栈来保存遍历路径。 为了保证事务的ACID性质并减少I/O次数，boltdb的B+Tree只有在需要写入到文件时才会进行调整以保持平衡。也就是说，在内存中，boltdb的B+Tree可能并不是平衡的。 ","date":"2021-01-19","objectID":"/posts/code-reading/boltdb-made-simple/2-b+tree/:1:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x02 B+Tree","uri":"/posts/code-reading/boltdb-made-simple/2-b+tree/"},{"categories":["深入浅出boltdb"],"content":"1. boltdb中B+Tree的实现 boltdb的B+Tree节点实现可分为存储中的实现（mmap memory）与内存中的实现（heap memory）两部分。 B+Tree节点存储部分的实现即branchNodePage与leafNodePage。branchNodePage中每个branchPageElement的pos与ksize字段标识了branch node中的每个key的位置、pgid字段标识了该key所对应的孩子节点的页id；leafNodePage中pos、ksize、vsize字段标识的leaf node中每个实际存储的key/value的位置、flags字段标识了该元素的类型（是普通的key/value还是bucket）。 虽然boltdb通过mmap的方式将数据库文件映射到了内存中，但boltdb不会直接修改mmap的内存空间，而是只读mmap内存空间。当需要更新B+Tree的节点时，boltdb会读取mmap内存中相应的page，并在heap memory中构建相应的数据结构来修改，最后再通过pwrite+fdatasync的方式写入底层文件。 B+Tree节点内存部分主要由node结构体实现。本节将详细介绍node结构体及其相关方法的实现。 boltdb中node是按需实例化的，对于不需要修改的node，boltdb直接从page中读取数据；而当boltdb需要修改B+Tree的某个节点时，则会将该节点从page实例化为node。在修改node时，boltdb会为其分配page buffer（dirty page），等到事务提交时，才会将这些page buffer中的数据统一落盘。 本文主要关注boltdb中B+Tree在内存中的实现（heap memory），B+Tree在存储中的实现（mmap memory - page）笔者已经在本系列的存储与缓存中介绍过，这里不再赘述。而node实例化的具体时机与boltdb中cursor的实现有关，因此笔者将其留到了本系列的后续文章中介绍。 ","date":"2021-01-19","objectID":"/posts/code-reading/boltdb-made-simple/2-b+tree/:2:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x02 B+Tree","uri":"/posts/code-reading/boltdb-made-simple/2-b+tree/"},{"categories":["深入浅出boltdb"],"content":"1.1 node结构体 boltdb中B+Tree的实现主要在node.go中。node结构体表示了boltdb中B+Tree的节点，其实现如下： // node represents an in-memory, deserialized page. type node struct { bucket *Bucket isLeaf bool unbalanced bool spilled bool key []byte pgid pgid parent *node children nodes inodes inodes } // inode represents an internal node inside of a node. // It can be used to point to elements in a page or point // to an element which hasn't been added to a page yet. type inode struct { flags uint32 pgid pgid key []byte value []byte } type inodes []inode 字段 描述 bucket *Bucket 该node所属的bucket指针。 isLeaf bool 当前node是否为叶子节点。 unbalanced bool 当前node是否可能不平衡。 spilled bool 当前node是否已被调整过。 key []byte 保存node初始化时的第一个key，用于在调整时索引。 pgid pgid 当前node在mmap内存中相应的页id。 parent *node 父节点指针。 children nodes 保存已实例化的孩子节点的node，用于spill时递归向下更新node。 inodes inodes 该node的内部节点，即该node所包含的元素。 ","date":"2021-01-19","objectID":"/posts/code-reading/boltdb-made-simple/2-b+tree/:2:1","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x02 B+Tree","uri":"/posts/code-reading/boltdb-made-simple/2-b+tree/"},{"categories":["深入浅出boltdb"],"content":"1.2 node在内存与存储中的关系 node的存储结构即为branchNodePage或leafNodePage，因此，boltdb仅持久化保存了node的isLeaf、pgid、inodes信息，其它信息都是在node创建或加载时指定的。node的write、read方法揭示了node是怎样被序列化或反序列化的： // read initializes the node from a page. func (n *node) read(p *page) { n.pgid = p.id n.isLeaf = ((p.flags \u0026 leafPageFlag) != 0) n.inodes = make(inodes, int(p.count)) for i := 0; i \u003c int(p.count); i++ { inode := \u0026n.inodes[i] if n.isLeaf { elem := p.leafPageElement(uint16(i)) inode.flags = elem.flags inode.key = elem.key() inode.value = elem.value() } else { elem := p.branchPageElement(uint16(i)) inode.pgid = elem.pgid inode.key = elem.key() } _assert(len(inode.key) \u003e 0, \"read: zero-length inode key\") } // Save first key so we can find the node in the parent when we spill. if len(n.inodes) \u003e 0 { n.key = n.inodes[0].key _assert(len(n.key) \u003e 0, \"read: zero-length node key\") } else { n.key = nil } } // write writes the items onto one or more pages. func (n *node) write(p *page) { // Initialize page. if n.isLeaf { p.flags |= leafPageFlag } else { p.flags |= branchPageFlag } if len(n.inodes) \u003e= 0xFFFF { panic(fmt.Sprintf(\"inode overflow: %d (pgid=%d)\", len(n.inodes), p.id)) } p.count = uint16(len(n.inodes)) // Stop here if there are no items to write. if p.count == 0 { return } // Loop over each item and write it to the page. b := (*[maxAllocSize]byte)(unsafe.Pointer(\u0026p.ptr))[n.pageElementSize()*len(n.inodes):] for i, item := range n.inodes { _assert(len(item.key) \u003e 0, \"write: zero-length inode key\") // Write the page element. if n.isLeaf { elem := p.leafPageElement(uint16(i)) elem.pos = uint32(uintptr(unsafe.Pointer(\u0026b[0])) - uintptr(unsafe.Pointer(elem))) elem.flags = item.flags elem.ksize = uint32(len(item.key)) elem.vsize = uint32(len(item.value)) } else { elem := p.branchPageElement(uint16(i)) elem.pos = uint32(uintptr(unsafe.Pointer(\u0026b[0])) - uintptr(unsafe.Pointer(elem))) elem.ksize = uint32(len(item.key)) elem.pgid = item.pgid _assert(elem.pgid != p.id, \"write: circular dependency occurred\") } // If the length of key+value is larger than the max allocation size // then we need to reallocate the byte array pointer. // // See: https://github.com/boltdb/bolt/pull/335 klen, vlen := len(item.key), len(item.value) if len(b) \u003c klen+vlen { b = (*[maxAllocSize]byte)(unsafe.Pointer(\u0026b[0]))[:] } // Write data for the element to the end of the page. copy(b[0:], item.key) b = b[klen:] copy(b[0:], item.value) b = b[vlen:] } // DEBUG ONLY: n.dump() } 需要注意的是，当读取page构建node时，inode的key与value是直接引用的page的地址，即node构建后并非完全不在依赖其page中的数据。但是随着数据库增大，当boltdb需要重新mmap以扩展存储空间时，boltdb需要执行dereference操作： // dereference causes the node to copy all its inode key/value references to heap memory. // This is required when the mmap is reallocated so inodes are not pointing to stale data. func (n *node) dereference() { if n.key != nil { key := make([]byte, len(n.key)) copy(key, n.key) n.key = key _assert(n.pgid == 0 || len(n.key) \u003e 0, \"dereference: zero-length node key on existing node\") } for i := range n.inodes { inode := \u0026n.inodes[i] key := make([]byte, len(inode.key)) copy(key, inode.key) inode.key = key _assert(len(inode.key) \u003e 0, \"dereference: zero-length inode key\") value := make([]byte, len(inode.value)) copy(value, inode.value) inode.value = value } // Recursively dereference children. for _, child := range n.children { child.dereference() } // Update statistics. n.bucket.tx.stats.NodeDeref++ } dereference会递归向下地将B+Tree中已实例化的node中的数据拷贝到heap memory中（非mmap映射的内存空间），以避免unmmap时node还在引用旧的mmap的内存地址。执行dereference前后，node在内存中的示意图如下： dereference执行前后node在内存中的示意图dereference执行前后node在内存中的示意图 \" dereference执行前后node在内存中的示意图 ","date":"2021-01-19","objectID":"/posts/code-reading/boltdb-made-simple/2-b+tree/:2:2","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x02 B+Tree","uri":"/posts/code-reading/boltdb-made-simple/2-b+tree/"},{"categories":["深入浅出boltdb"],"content":"1.3 node中用于查询的方法 除了node的字段外，node的一些方法也用来查询或索引数据（leafNode没有孩子，其child指其内部元素，即inode）： 方法 描述 root() *node 当前node所在B+Tree的根节点。 minKeys() int 节点至少应有的key的个数，leafNode返回1，branchNode返回2。 size() int 当前节点序列化后的字节数。 sizeLessThan(v int) bool 判断当前节点序列化后字节数是否小于给定值，当累加计算超出给定值时立即返回false，用于比较时性能比直接调用size方法好一些。 pageElementSize() int 当前类型节点的inode的header大小，用于计算序列化后字节数。 childAt(index int) *node 返回节点当前第index个孩子的指针（而不是初始时的第index个孩子）。 childIndex(child *node) int 返回给定的节点在当前节点的inode中的序号（而不是初始时的序号）。 numChildren() int 当前节点的孩子（inode）的个数。 nextSibling() *node 当前节点的下一个兄弟节点（通过查询父节点获取）。 prevSibling() *node 当前节点的上一个兄弟节点（通过查询父节点获取）。 这些方法的实现都非常简单，这里不再赘述。 ","date":"2021-01-19","objectID":"/posts/code-reading/boltdb-made-simple/2-b+tree/:2:3","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x02 B+Tree","uri":"/posts/code-reading/boltdb-made-simple/2-b+tree/"},{"categories":["深入浅出boltdb"],"content":"1.4 node中用于修改的方法 node还提供了在当前节点上插入（或修改）与删除inode的put、del方法： // put inserts a key/value. func (n *node) put(oldKey, newKey, value []byte, pgid pgid, flags uint32) { if pgid \u003e= n.bucket.tx.meta.pgid { panic(fmt.Sprintf(\"pgid (%d) above high water mark (%d)\", pgid, n.bucket.tx.meta.pgid)) } else if len(oldKey) \u003c= 0 { panic(\"put: zero-length old key\") } else if len(newKey) \u003c= 0 { panic(\"put: zero-length new key\") } // Find insertion index. index := sort.Search(len(n.inodes), func(i int) bool { return bytes.Compare(n.inodes[i].key, oldKey) != -1 }) // Add capacity and shift nodes if we don't have an exact match and need to insert. exact := (len(n.inodes) \u003e 0 \u0026\u0026 index \u003c len(n.inodes) \u0026\u0026 bytes.Equal(n.inodes[index].key, oldKey)) if !exact { n.inodes = append(n.inodes, inode{}) copy(n.inodes[index+1:], n.inodes[index:]) } inode := \u0026n.inodes[index] inode.flags = flags inode.key = newKey inode.value = value inode.pgid = pgid _assert(len(inode.key) \u003e 0, \"put: zero-length inode key\") } // del removes a key from the node. func (n *node) del(key []byte) { // Find index of key. index := sort.Search(len(n.inodes), func(i int) bool { return bytes.Compare(n.inodes[i].key, key) != -1 }) // Exit if the key isn't found. if index \u003e= len(n.inodes) || !bytes.Equal(n.inodes[index].key, key) { return } // Delete inode from the node. n.inodes = append(n.inodes[:index], n.inodes[index+1:]...) // Mark the node as needing rebalancing. n.unbalanced = true } put方法与del方法的实现思路相似，二者都是在node的inodes上进行二分搜索，找到第一个大于等于用于查询的key（put中为oldKey，del中为key）的位置，然后判断当前位置的key与用于查询的key是否相等。若二者相等，则修改或删除该key，否则插入新key或直接返回。此外，因为del操作可能导致node的数据填充率低于阈值，因此del会将node的unbalanced置为true，以便后续操作检查该节点是否需要进行rebalance操作。 ","date":"2021-01-19","objectID":"/posts/code-reading/boltdb-made-simple/2-b+tree/:2:4","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x02 B+Tree","uri":"/posts/code-reading/boltdb-made-simple/2-b+tree/"},{"categories":["深入浅出boltdb"],"content":"1.5 B+Tree的调整 在上文中笔者介绍过，boltdb只有在需要将B+Tree写入到文件时才需要调整B+Tree的结构，因此put和del不需要调整B+Tree的结构，实现非常简单。 boltdb的B+Tree实现中，用来调整B+Tree结构的方法有两个：rebalance和spill。rebalance用于检查node是否由于删除了inode而导致数据填充率低于阈值，并将数据填充率低于阈值的node与其兄弟节点合并，rebalance还会将只有一个孩子的根节点与该其唯一的孩子合并。spill则可以进一步分为两个步骤，spill首先会检查并将填充率过高的节点拆分为多个小节点（split），并维护B+Tree的结构，然后将更新后的节点写到新的page中。因此，在事务提交时，boltdb会先对B+Tree执行rebalance操作再执行spill操作。 1.5.1 rebalance 首先分析rebalance的实现： // rebalance attempts to combine the node with sibling nodes if the node fill // size is below a threshold or if there are not enough keys. func (n *node) rebalance() { if !n.unbalanced { return } n.unbalanced = false // Update statistics. n.bucket.tx.stats.Rebalance++ // Ignore if node is above threshold (25%) and has enough keys. var threshold = n.bucket.tx.db.pageSize / 4 if n.size() \u003e threshold \u0026\u0026 len(n.inodes) \u003e n.minKeys() { return } // Root node has special handling. if n.parent == nil { // If root node is a branch and only has one node then collapse it. if !n.isLeaf \u0026\u0026 len(n.inodes) == 1 { // Move root's child up. child := n.bucket.node(n.inodes[0].pgid, n) n.isLeaf = child.isLeaf n.inodes = child.inodes[:] n.children = child.children // Reparent all child nodes being moved. for _, inode := range n.inodes { if child, ok := n.bucket.nodes[inode.pgid]; ok { child.parent = n } } // Remove old child. child.parent = nil delete(n.bucket.nodes, child.pgid) child.free() } return } // If node has no keys then just remove it. if n.numChildren() == 0 { n.parent.del(n.key) n.parent.removeChild(n) delete(n.bucket.nodes, n.pgid) n.free() n.parent.rebalance() return } _assert(n.parent.numChildren() \u003e 1, \"parent must have at least 2 children\") // Destination node is right sibling if idx == 0, otherwise left sibling. var target *node var useNextSibling = (n.parent.childIndex(n) == 0) if useNextSibling { target = n.nextSibling() } else { target = n.prevSibling() } // If both this node and the target node are too small then merge them. if useNextSibling { // Reparent all child nodes being moved. for _, inode := range target.inodes { if child, ok := n.bucket.nodes[inode.pgid]; ok { child.parent.removeChild(child) child.parent = n child.parent.children = append(child.parent.children, child) } } // Copy over inodes from target and remove target. n.inodes = append(n.inodes, target.inodes...) n.parent.del(target.key) n.parent.removeChild(target) delete(n.bucket.nodes, target.pgid) target.free() } else { // Reparent all child nodes being moved. for _, inode := range n.inodes { if child, ok := n.bucket.nodes[inode.pgid]; ok { child.parent.removeChild(child) child.parent = target child.parent.children = append(child.parent.children, child) } } // Copy over inodes to target and remove node. target.inodes = append(target.inodes, n.inodes...) n.parent.del(n.key) n.parent.removeChild(n) delete(n.bucket.nodes, n.pgid) n.free() } // Either this node or the target node was deleted from the parent so rebalance it. n.parent.rebalance() } // removes a node from the list of in-memory children. // This does not affect the inodes. func (n *node) removeChild(target *node) { for i, child := range n.children { if child == target { n.children = append(n.children[:i], n.children[i+1:]...) return } } } // free adds the node's underlying page to the freelist. func (n *node) free() { if n.pgid != 0 { n.bucket.tx.db.freelist.free(n.bucket.tx.meta.txid, n.bucket.tx.page(n.pgid)) n.pgid = 0 } } rebalance方法依次做了如下操作： 如果当前节点没执行过del方法（unbalanced为false），跳过当前节点。 如果当前节点的填充率大于25%且inode数量比最少数量大，则不处理该节点，以免rebalance操作太频繁导致性能下降。 如果当前节点为根节点且只有一个孩子，那么将该根节点与唯一的孩子合并。需要注意的是，该孩子节点可能还有孩子，因此合并的时候需要修改相应的指针。合并后，释放孩子节点使用的page。 如果当前节点为空节点，删除当前节点并释放占用的page，并递归对父节点执行rebalance操作。 否则，与兄弟节点合并。此时，如果当前节点在父节点中不是首个孩子，则默认与后继兄弟节点合并，否则与前驱兄弟节点合并。合并后，释放占用的page，并递归对父节点执行rebalance操作。 注意，这里“释放page”，指将占用的page加到当前事务的pending列表中，而不是立即释放，因此此时可能仍有只读事务正在读取旧page。由于在合并时，一定在父节点中删除了当前节点的key，因此父节点会变为unbalanced状态，所以需要递归对父节点进行rebalance操作。 1.5.2 spill 接下来分析spill及相关方法的实现： // split breaks up a node into multiple smal","date":"2021-01-19","objectID":"/posts/code-reading/boltdb-made-simple/2-b+tree/:2:5","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x02 B+Tree","uri":"/posts/code-reading/boltdb-made-simple/2-b+tree/"},{"categories":["深入浅出boltdb"],"content":"2. 总结 本文介绍了B-Tree及其变体B+Tree的基本概念，并分析了boltdb中B+Tree的实现。 由于boltdb使用了mmap方式将数据库文件映射到了内存中，且mmap仅作为读取的方式而不作为写入的方式，所以当没有接触过类似实现的读者阅读分析其源码时经常会因混淆mmap memory和heap memory而困惑。对此比较困惑的读者可以再次阅读本系列《深入浅出boltdb —— 0x01 存储与缓存》中3. boltdb的读写与缓存策略一节。 ","date":"2021-01-19","objectID":"/posts/code-reading/boltdb-made-simple/2-b+tree/:3:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x02 B+Tree","uri":"/posts/code-reading/boltdb-made-simple/2-b+tree/"},{"categories":["深入浅出boltdb"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/1-storage-cache/:0:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x01 存储与缓存","uri":"/posts/code-reading/boltdb-made-simple/1-storage-cache/"},{"categories":["深入浅出boltdb"],"content":"0. 引言 boltdb使用单内存映射文件作为存储（single memory-mapped file on disk）。boltdb在启动时会通过mmap系统调用将数据库文件映射到内存，这样可以仅通过内存访问来对文件进行读写，而将磁盘I/O交给操作系统管理，只有在事务提交或更新元数据时，boltdb才会通过fdatasyc系统调用强制将脏页落盘，以保证事务的ACID语义。 在linux系统中，内存与磁盘间的换入换出是以页为单位的。为了充分利用这一特定，boltdb的数据库文件也是按页组织的，且页大小与操作系统的页大小相等。 由于mmap与unmmap系统调用的开销相对较大，因此boltdb在每次mmap时会预留一部分空间（小于1GB时倍增，超过1GB时每次额外申请1GB），这会产生一些空闲的页；同时，随着对数据库的操作，在更新值注1或删除值时，数据库也可能产生空闲页注2。为了高效地管理这些空闲页，boltdb学习操作系统引入了一个简化的空闲页列表。 boltdb的页与空闲页列表的实现分别在page.go与freelist.go中，本文主要围绕这两个文件，分析boltdb中页与空闲页列表的设计与实现。同时，本文后半部分介绍了boltdb的读写操作与缓存策略。由于boltdb的读写（特别是写入）与事务关系较为密切，因此本文后半部分的分析中可能涉及到一些与事务相关的代码，不了解boltdb事务实现的读者可以先阅读本系列后续的文章。 注1 为了在保证隔离性的同时支持“读读并发”、“读写并发”（boltdb不支持“写写并发”，即同一时刻只能有一个执行中的可写事务），boltdb在更新页时采用了Shadow Paging技术，其通过copy-on-write实现。在可写事务更新页时，boltdb首先会复制原页，然后在副本上更新，再将引用修改为新页上。这样，当可写事务更新页时，只读事务还可以读取原来的页；当创建读写事务时，boltdb会释放不再使用的页。这样，便实现了在支持“读读并发”、“读写并发”的同时保证事务的隔离性。 注2 boltdb不会将空闲的页归还给系统。其原因有二： 在不断增大的数据库中，被释放的页之后还会被重用。 boltdb为了保证读写并发的隔离性，使用copy-on-write来更新页，因此会在任意位置产生空闲页，而不只是在文件末尾产生空闲页（详见issue#308）。 ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/1-storage-cache/:1:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x01 存储与缓存","uri":"/posts/code-reading/boltdb-made-simple/1-storage-cache/"},{"categories":["深入浅出boltdb"],"content":"1. page ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/1-storage-cache/:2:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x01 存储与缓存","uri":"/posts/code-reading/boltdb-made-simple/1-storage-cache/"},{"categories":["深入浅出boltdb"],"content":"1.1 page的总体结构 boltdb中每个页的元数据保存在该页的开头处，我们可以将其看做页的头部Page Header，页的其余部分为页的数据Page Body，不同用途的页的Page Body中的数据存储格式不同。 页结构页结构 \" 页结构 页相关的代码主要在page.go中。Page Header是以page结构体表示的，其包含的字段如下： type pgid uint64 type page struct { id pgid flags uint16 count uint16 overflow uint32 ptr uintptr } 字段 描述 id 页id。页id从0开始，随地址空间单调递增。 flags 页标识，用来表示页的类型（用途）。 count 页中元素个数。 overflow 溢出页个数。当单页无法容纳数据时，可以用与该页相邻的页保存溢出的数据（详见后文中介绍）。 ptr 页的数据（Page Body）的起始位置。 boltdb中的页共有三种用途：保存数据库的元数据（meta page）1、保存空闲页列表(freelist page)、保存数据，因为boltdb中数据是按照B+树组织的，因此保存数据的页又可分为分支节点（branch page）和叶子节点（leaf page）两种。也就是说，boltdb中页的类型共有4种。 ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/1-storage-cache/:2:1","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x01 存储与缓存","uri":"/posts/code-reading/boltdb-made-simple/1-storage-cache/"},{"categories":["深入浅出boltdb"],"content":"1.2 page的数据结构 本节将分别介绍boltdb中meta page、leaf page与branch page的数组结构，free page的数据结构与其行为关系较为密切，将在本文之后的章节中介绍。 1.2.1 meta page meta page是boltdb记录数据库元数据的页。meta page的格式非常简单，其Page Body就是一个meta结构体。meta结构体的字段如下： type meta struct { magic uint32 version uint32 pageSize uint32 flags uint32 root bucket freelist pgid pgid pgid txid txid checksum uint64 } // bucket.go type bucket struct { root pgid // page id of the bucket's root-level page sequence uint64 // monotonically incrementing, used by NextSequence() } 字段 描述 magic 一个固定值，用来标识该文件为boltdb数据库文件。 version 用来标识该文件采用的数据库版本号。 pageSize 用来标识改文件采用的页大小。 flags 保留字段，未使用。 root boltdb记录根bucket的结构体，其包含了该bucket的根页id与bucket编号（单调递增）。 freelist 空闲页列表的首页id。 pgid 下一个分配的页id，即当前最大页id+1，用于mmap扩容时为新页编号。 txid 下一个事务的id，全局单调递增。 checksum meta页的校验和。 1.2.2 branch page \u0026 leaf page branch page与leaf page是boltdb中用来保存B+树节点的页。B+树的分支节点仅用来保存索引（key），而叶子节点既保存索引，又保存值（value）。boltdb支持任意长度的key和value，因此无法直接结构化保存key和value的列表。为了解决这一问题，branch page和leaf page的Page Body起始处是一个由定长的索引（branchPageElement或leafPageElement）组成的列表，第$i$个索引记录了第$i$个key或key/value的起始位置与key的长度或key/value各自的长度： // branchPageElement represents a node on a branch page. type branchPageElement struct { pos uint32 ksize uint32 pgid pgid } // key returns a byte slice of the node key. func (n *branchPageElement) key() []byte { buf := (*[maxAllocSize]byte)(unsafe.Pointer(n)) return (*[maxAllocSize]byte)(unsafe.Pointer(\u0026buf[n.pos]))[:n.ksize] } branch page结构示意图branch page结构示意图 \" branch page结构示意图 // leafPageElement represents a node on a leaf page. type leafPageElement struct { flags uint32 pos uint32 ksize uint32 vsize uint32 } // key returns a byte slice of the node key. func (n *leafPageElement) key() []byte { buf := (*[maxAllocSize]byte)(unsafe.Pointer(n)) return (*[maxAllocSize]byte)(unsafe.Pointer(\u0026buf[n.pos]))[:n.ksize:n.ksize] } // value returns a byte slice of the node value. func (n *leafPageElement) value() []byte { buf := (*[maxAllocSize]byte)(unsafe.Pointer(n)) return (*[maxAllocSize]byte)(unsafe.Pointer(\u0026buf[n.pos+n.ksize]))[:n.vsize:n.vsize] } leaf page结构示意图leaf page结构示意图 \" leaf page结构示意图 ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/1-storage-cache/:2:2","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x01 存储与缓存","uri":"/posts/code-reading/boltdb-made-simple/1-storage-cache/"},{"categories":["深入浅出boltdb"],"content":"1.3 page溢出结构 虽然B+树会拆分过大的节点，但当key或value过大时，或freelist过大时，不适合将其拆分为多个page。因此，boltdb允许过大的页的数据溢出到之后紧挨着的连续的页中，如下图所示： page溢出结构page溢出结构 \" page溢出结构 如上图所示，一个页和其溢出页共用该页的Page Header，即溢出页只有Page Body部分。这样做的好处是，因为溢出页与首页是连续的且溢出页只有Page Body，那么相当于数据的内存地址是连续的，访问数据时只需要正常计算偏移量即可，不需要特殊处理溢出页。溢出页的数量记录在首页的Page Header的overflow字段中。 ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/1-storage-cache/:2:3","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x01 存储与缓存","uri":"/posts/code-reading/boltdb-made-simple/1-storage-cache/"},{"categories":["深入浅出boltdb"],"content":"2. freelist boltdb通过freelist实现了空闲页列表。boltdb也将freelist按照一定格式持久化存储在了page中。当数据库初始化或者恢复时，如果能够找到保存在页中的freelist，则直接使用该freelist，否则扫描数据库，构建新的freelist。 ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/1-storage-cache/:3:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x01 存储与缓存","uri":"/posts/code-reading/boltdb-made-simple/1-storage-cache/"},{"categories":["深入浅出boltdb"],"content":"2.1 freelist的逻辑结构 freelist结构体中有3个字段： // freelist represents a list of all pages that are available for allocation. // It also tracks pages that have been freed but are still in use by open transactions. type freelist struct { ids []pgid // all free and available free page ids. pending map[txid][]pgid // mapping of soon-to-be free page ids by tx. cache map[pgid]bool // fast lookup of all free and pending page ids. } 字段 描述 ids []pgid 记录已释放的页id的有序列表。 pending map[txid][]pgid 事务id到事务待释放的页id。 cache map[pgid]bool 用来快速查找给定id的页是否被释放的缓存。出现在ids和pending中的页id均为true。 由于boltdb是通过copy-on-write的方式实现的读写事务并发的隔离性，因此当事务可写事务更新页时，其会复制已有的页，并将旧页加入到pending中该事务id下的待释放页的列表中。因为此时可能还有读事务在读取旧页，所以不能立刻释放该页，而是要等到所有事务都不再依赖该页时，才能将pending中的页加入到ids中。对于boltdb中事务的实现笔者会在本系列后面的文章中介绍，这里不再赘述，这里读者只需要了解pending的作用即可。 这样做的好处还有，当事务回滚时，可以重用pending中还未释放的页（由于该事务还未提交，因此其之前释放的所有页都可被重用）。而且，重用页时对freelist的操作十分简单，只需要将pending中该事务id对应的列表清空即可。 ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/1-storage-cache/:3:1","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x01 存储与缓存","uri":"/posts/code-reading/boltdb-made-simple/1-storage-cache/"},{"categories":["深入浅出boltdb"],"content":"2.2 freelist的存储结构 当数据库将freelist写入page时，会将ids与pending中的页id合并在一起写入ids。因为如果数据库crash了，那么所有事务都会终止，pending中的页都可以安全释放。 因此，保存freelist的page只需要写入有序的空闲页id列表即可，其结构如下： freelist的存储结构freelist的存储结构 \" freelist的存储结构 freelist页溢出的处理方式与其它page稍有不同。由于page结构体的count字段为uint16类型，其最大值为65535（0xFFFF），假设页大小为4KB，那么count字段能表示的最大的freelist只能记录256MB的页。也就是说，即使允许freelist的page溢出，但是由于受count字段的限制，其仍无法表示足够大的空间。因此，boltdb在写入freelist的page时，会判断空闲页列表的长度。当空闲页列表长度小于0xFF时，采用与其它的类型相同的方式处理；而当空闲页列表长度大于等于0xFF时，则用本应写入第一条pgid的位置（ptr指向的位置）记录空闲页列表的真实长度，而将真正的空闲页列表往后顺延一个条目的位置写入，同时将count置为0xFF。其示意图如下： 大型freelist的存储结构大型freelist的存储结构 \" 大型freelist的存储结构 从page读取freelist与将freelist写入page的相应方法如下： // read initializes the freelist from a freelist page. func (f *freelist) read(p *page) { // If the page.count is at the max uint16 value (64k) then it's considered // an overflow and the size of the freelist is stored as the first element. idx, count := 0, int(p.count) if count == 0xFFFF { idx = 1 count = int(((*[maxAllocSize]pgid)(unsafe.Pointer(\u0026p.ptr)))[0]) } // Copy the list of page ids from the freelist. if count == 0 { f.ids = nil } else { ids := ((*[maxAllocSize]pgid)(unsafe.Pointer(\u0026p.ptr)))[idx:count] f.ids = make([]pgid, len(ids)) copy(f.ids, ids) // Make sure they're sorted. sort.Sort(pgids(f.ids)) } // Rebuild the page cache. f.reindex() } // write writes the page ids onto a freelist page. All free and pending ids are // saved to disk since in the event of a program crash, all pending ids will // become free. func (f *freelist) write(p *page) error { // Combine the old free pgids and pgids waiting on an open transaction. // Update the header flag. p.flags |= freelistPageFlag // The page.count can only hold up to 64k elements so if we overflow that // number then we handle it by putting the size in the first element. lenids := f.count() if lenids == 0 { p.count = uint16(lenids) } else if lenids \u003c 0xFFFF { p.count = uint16(lenids) f.copyall(((*[maxAllocSize]pgid)(unsafe.Pointer(\u0026p.ptr)))[:]) } else { p.count = 0xFFFF ((*[maxAllocSize]pgid)(unsafe.Pointer(\u0026p.ptr)))[0] = pgid(lenids) f.copyall(((*[maxAllocSize]pgid)(unsafe.Pointer(\u0026p.ptr)))[1:]) } return nil } ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/1-storage-cache/:3:2","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x01 存储与缓存","uri":"/posts/code-reading/boltdb-made-simple/1-storage-cache/"},{"categories":["深入浅出boltdb"],"content":"2.3 freelist的方法 本节简单介绍freelist提供的方法，以便读者阅读源码时参考。 方法 描述 size() int 获取freelist序列化为page后的数据大小。 count() int 获取freelist中页的个数。 free_count() int ids中的空闲页数。 pending_count() int pending中待释放的空闲页数。 copyall(dst []pgid) 将ids与pending中的所有空闲页id合并、排序并写入目标位置。该方法在将freelist写入到page时使用。 allocate(n int) pgid 尝试从freelist中分配n个连续的页，返回首页的页id。 free(txid txid, p *page) 将页加入给定事务的pending列表中。 release(txid txid) 释放给定事务及其之前事务的pending列表中的所有待释放页，将其合并到ids中。 rollback(txid txid) 当事务回滚时调用该方法，删除该事务的pending列表记录的页id以复用。 freed(pgid pgid) bool 返回给定页是否在freelist中。 read(p *page) 从page中读取并构建freelist。 write(p *page) error 将freelist写入到page中。 reload(p *page) 从page中重新加载freelist，该方法先调用read方法，接下来从ids中过滤掉pengding中的页。 reindex() 重建freelist的缓存。 其中，release调用的时机为新读写事务启动时。在启动新的读写事务时，boltdb会根据事务id释放所有已完成的事务在pending中的页： func (db *DB) beginRWTx() (*Tx, error) { // ... ... // Free any pages associated with closed read-only transactions. var minid txid = 0xFFFFFFFFFFFFFFFF for _, t := range db.txs { if t.meta.txid \u003c minid { minid = t.meta.txid } } if minid \u003e 0 { db.freelist.release(minid - 1) } return t, nil } ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/1-storage-cache/:3:3","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x01 存储与缓存","uri":"/posts/code-reading/boltdb-made-simple/1-storage-cache/"},{"categories":["深入浅出boltdb"],"content":"3. boltdb的读写与缓存策略 boltdb的缓存策略主要有两种，一种是使用mmap的读缓存策略，一种是使用page buffer的写缓存策略。 ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/1-storage-cache/:4:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x01 存储与缓存","uri":"/posts/code-reading/boltdb-made-simple/1-storage-cache/"},{"categories":["深入浅出boltdb"],"content":"3.1 读操作与缓存策略 boltdb在读取数据库文件时，为了避免频繁进行设备I/O，使用了mmap技术作为缓存。当boltdb打开数据库时，其会将数据库文件通过mmap系统调用映射到内存。这样可以避免使用read系统调用读取I/O设备，而是直接以内存访问的方式读取数据。在通过mmap将数据库文件映射到内存后，boltdb会根据数据库文件构建内存数据结构，如meta、freelist、B+Tree结构。 根据使用方式的不同，meta、freelist、B+Tree使用mmap中数据的方式各不相同。 boltdb将其meta直接指向了mmap内存空间的meta page，但仅用来读取，不会直接修改meta page。当创建新事务时，boltdb会复制当前的meta page到一处内存中，作为该事务开始时的meta快照。 freelist和B+Tree都是根据mmap内存空间的page在内存别处构建的数据结构，但二者的构建策略不同。freelist是在打开数据库时完整地读取mmap内存空间中的freelist page构建的；而B+Tree则是在使用中按需构建的，即在读取B+Tree的node时，如果node已经在缓存中构建过，则读取已经构建好的缓存，如果node还没在缓存中构建过，则读取mmap内存空间中的数据，在内存别处构建node的缓存。 boltdb的读操作与缓存策略可通过下图表示： boltdb读操作与缓存策略示意图boltdb读操作与缓存策略示意图 \" boltdb读操作与缓存策略示意图 B+Tree的构建笔者会在本系列后续的文章中介绍，本节只关注与mmap相关的部分。 3.1.1 boltdb数据库文件结构 boltdb的数据库文件由两个meta页、一个freelist页、和若干个用来保存数据与索引的B+树的branchNode页和leafNode页组成（页可能包含若干个overflow页）。当数据库初始化时，其会将0、1号页初始化为meta页、将2号页初始化为freelist页、将3号页初始化为空的leafNodePage。 由于只有B+树的页是通过copy-on-write方式写入的，所以boltdb设置了两个meta页以进行本地容错。在更新元数据时，boltdb会交替写入两个meta页。这样，如果meta页写入中途数据库挂掉，数据库仍可以使用另一份完整的meta页。 3.1.2 mmap mmap是boltdb的主要缓存策略。与mmap相关的方法主要有mmap、munmap、mmapSize(位于db.go中)： 方法 描述 mmap(minsz int) error 以内存映射文件的方式打开数据库文件并初始化meta引用。参数minsz是最小的mmap大小，其实际mmap大小是通过mmapSize方法获取的。 munmap() error 取消文件的内存映射。 mmapSize(size int) (int, error) 计算mmap大小，参数size是最小大小。 // mmap opens the underlying memory-mapped file and initializes the meta references. // minsz is the minimum size that the new mmap can be. func (db *DB) mmap(minsz int) error { db.mmaplock.Lock() defer db.mmaplock.Unlock() info, err := db.file.Stat() if err != nil { return fmt.Errorf(\"mmap stat error: %s\", err) } else if int(info.Size()) \u003c db.pageSize*2 { return fmt.Errorf(\"file size too small\") } // Ensure the size is at least the minimum size. var size = int(info.Size()) if size \u003c minsz { size = minsz } size, err = db.mmapSize(size) if err != nil { return err } // Dereference all mmap references before unmapping. if db.rwtx != nil { db.rwtx.root.dereference() } // Unmap existing data before continuing. if err := db.munmap(); err != nil { return err } // Memory-map the data file as a byte slice. if err := mmap(db, size); err != nil { return err } // Save references to the meta pages. db.meta0 = db.page(0).meta() db.meta1 = db.page(1).meta() // Validate the meta pages. We only return an error if both meta pages fail // validation, since meta0 failing validation means that it wasn't saved // properly -- but we can recover using meta1. And vice-versa. err0 := db.meta0.validate() err1 := db.meta1.validate() if err0 != nil \u0026\u0026 err1 != nil { return err0 } return nil } // munmap unmaps the data file from memory. func (db *DB) munmap() error { if err := munmap(db); err != nil { return fmt.Errorf(\"unmap error: \" + err.Error()) } return nil } // mmapSize determines the appropriate size for the mmap given the current size // of the database. The minimum size is 32KB and doubles until it reaches 1GB. // Returns an error if the new mmap size is greater than the max allowed. func (db *DB) mmapSize(size int) (int, error) { // Double the size from 32KB until 1GB. for i := uint(15); i \u003c= 30; i++ { if size \u003c= 1\u003c\u003ci { return 1 \u003c\u003c i, nil } } // Verify the requested size is not above the maximum allowed. if size \u003e maxMapSize { return 0, fmt.Errorf(\"mmap too large\") } // If larger than 1GB then grow by 1GB at a time. sz := int64(size) if remainder := sz % int64(maxMmapStep); remainder \u003e 0 { sz += int64(maxMmapStep) - remainder } // Ensure that the mmap size is a multiple of the page size. // This should always be true since we're incrementing in MBs. pageSize := int64(db.pageSize) if (sz % pageSize) != 0 { sz = ((sz / pageSize) + 1) * pageSize } // If we've exceeded the max size then only grow up to the max size. if sz \u003e maxMapSize { sz = maxMapSize } return int(sz), nil } boltdb中mmap会调用Linux的系统调用，其prot参数为PROT_READ，flags为MAP_SHARED与数据库配置中MmapFlags按位或的结果。 boltdb的mmap大小增长策略如下：最小的mmap大小为32KB，在1GB之前mmap大小每次倍增，在1GB之后每次增长1GB。 注意 boltdb在进行映射时，mmap大小可能超过数据库文件大小。在访问超出文件大小的mmap部分时会引起SIGBUS异常。为了避免访问到超出文件的部分，同时尽可能减少对底层文","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/1-storage-cache/:4:1","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x01 存储与缓存","uri":"/posts/code-reading/boltdb-made-simple/1-storage-cache/"},{"categories":["深入浅出boltdb"],"content":"3.2 写操作与缓存策略 为了保证事务的ACID性质，数据库系统需要确保事务提交时文件的修改要安全地写入到磁盘或其它I/O设备。由于随机的设备I/O非常耗时，boltdb不会在数据更新时立刻修改底层文件。而是先将修改后的page写入到一块page buffer内存中作为缓存，等事务提交时，再将事务修改的所有page buffer顺序地写入I/O设备。同时，为了保证数据被安全地写入到了I/O设备，boltdb的刷盘采用pwrite+fdatasyc实现。 boltdb的写操作与缓存策略可通过下图表示： boltdb写操作与缓存策略示意图boltdb写操作与缓存策略示意图 \" boltdb写操作与缓存策略示意图 3.2.1 page buffer（memory-\u003ememory） 无论是修改meta、freelist，还是修改或写入新B+Tree的node时，boltdb都会先将数据按照page结构写入mmap内存空间外的page buffer中，等到事务提交时再将page buffer中数据写入到底层数据库文件相应的page处。 分配page buffer的逻辑在db.go的allocate方法中： // allocate returns a contiguous block of memory starting at a given page. func (db *DB) allocate(count int) (*page, error) { // Allocate a temporary buffer for the page. var buf []byte if count == 1 { buf = db.pagePool.Get().([]byte) } else { buf = make([]byte, count*db.pageSize) } p := (*page)(unsafe.Pointer(\u0026buf[0])) p.overflow = uint32(count - 1) // Use pages from the freelist if they are available. if p.id = db.freelist.allocate(count); p.id != 0 { return p, nil } // Resize mmap() if we're at the end. p.id = db.rwtx.meta.pgid var minsz = int((p.id+pgid(count))+1) * db.pageSize if minsz \u003e= db.datasz { if err := db.mmap(minsz); err != nil { return nil, fmt.Errorf(\"mmap allocate error: %s\", err) } } // Move the page id high water mark. db.rwtx.meta.pgid += pgid(count) return p, nil } 通过这段代码可以看出，allocate方法会先创建一段能够容纳给定数量page的连续内存空间，这段空间就是page buffer，然后将这段空间作为page返回给调用者。这样，调用者可以向读写正常page一样读写这段page buffer。 为了避免分配较小的page buffer造成过多的内存碎片，boltdb通过sync.Pool类型的字段pagePool分配、复用长度为1的page buffer。pagePool的定义与初始化代码如下： // in struct db pagePool sync.Pool // in `db.Open` // Initialize page pool. db.pagePool = sync.Pool{ New: func() interface{} { return make([]byte, db.pageSize) }, } 在分配page buffer时，需要分配的page buffer长度为1，boltdb会通过pagePool分配；在事务提交时，boltdb将page buffer中的数据写入到文件后，会将长度为1的page buffer放回pagePool。而如果所需的page buffer长度大于1，则boltdb会通过make分配page buffer的内存空间。 3.2.2 pwrite + fdatasync（memory-\u003edisk） 为了保证事务的ACID性质，当事务提交时，boltdb需要保证数据被完整地写入到了磁盘中。在介绍boltdb的数据同步策略前，笔者首先简单介绍Linux系统提供的文件数据同步方式。 在Linux中，为了性能考虑，write/pwrite等系统调用不会等待设备I/O完成后再返回。write/pwrite等系统调用只会更新page cache，而脏页的同步时间由操作系统控制。sync系统调用会在page cache中的脏页提交到设备I/O队列后返回，但是不会等待设备I/O完成。如果此时I/O设备故障，则数据还可能丢失。而fsync与fdatasync则会等待设备I/O完成后返回，以提供最高的同步保证。fsync与fdatasync的区别在于，fdatasync只会更新文件数据和必要的元数据（如文件大小等），而fsync会更新文件数据和所有相关的元数据（包括文件修改时间等），由于文件元数据与数据的保存位置可能不同，因此在磁盘上fsync往往比fdatasync多一次旋转时延。 对于内存映射文件，Linux提供了msync系统调用。该系统调用可以更精确地控制同步的内存范围。 虽然boltdb使用了内存映射文件，但是当事务提交时，其还是通过pwrite + fdatasync的方式同步刷盘。在Linux的文档中并没有详细说明混用普通文件的同步方式与内存映射文件的同步方式的影响。但是通过实践和mmap的MAP_SHARED模式的描述可知，使用SHARED的mmap，当其它进程通过fdatasync等系统调用修改底层文件后，修改能通过mmap的内存访问到。 // write writes any dirty pages to disk. func (tx *Tx) write() error { // Sort pages by id. pages := make(pages, 0, len(tx.pages)) for _, p := range tx.pages { pages = append(pages, p) } // Clear out page cache early. tx.pages = make(map[pgid]*page) sort.Sort(pages) // Write pages to disk in order. for _, p := range pages { size := (int(p.overflow) + 1) * tx.db.pageSize offset := int64(p.id) * int64(tx.db.pageSize) // Write out page in \"max allocation\" sized chunks. ptr := (*[maxAllocSize]byte)(unsafe.Pointer(p)) for { // Limit our write to our max allocation size. sz := size if sz \u003e maxAllocSize-1 { sz = maxAllocSize - 1 } // Write chunk to disk. buf := ptr[:sz] if _, err := tx.db.ops.writeAt(buf, offset); err != nil { return err } // Update statistics. tx.stats.Write++ // Exit inner for loop if we've written all the chunks. size -= sz if size == 0 { break } // Otherwise move offset forward and move pointer to next chunk. offset += int64(sz) ptr = (*[maxAllocSize]byte)(unsafe.Pointer(\u0026ptr[sz])) } } // Ignore file sync if flag is set on DB. if !tx.db.NoSync || IgnoreNoSync { if err := fdatasync(tx.db); err != nil { return err } } // Put small pages back to page pool. for _, p := range pages { // Ignore page sizes over 1 page. // These are allocated using make() instead of the page pool. if int(p.overflow) != 0 { continue } buf := (*[maxAllocSize]byte)(unsafe.Pointer(p))[:tx.db.pageSiz","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/1-storage-cache/:4:2","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x01 存储与缓存","uri":"/posts/code-reading/boltdb-made-simple/1-storage-cache/"},{"categories":["深入浅出boltdb"],"content":"4. 总结 总的来说，boltdb的读写操作与缓存策略可以概括如下： 读数据库文件时，通过mmap技术将其映射到内存中，避免频繁的设备I/O；不同的数结构在不同时刻读取mmap内存空间的数据完成构建。 写数据库文件时，通过page buffer先写入内存缓存，然后在事务提交时将page buffer中的数据通过pwrite + fdatasync系统调用写入底层文件并确保写入的安全性。 而连接mmap与pwrite + fdatasync的桥梁，是操作系统。操作系统保证了通过MAP_SHARED映射的内存空间在底层文件修改时会更新。 ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/1-storage-cache/:5:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x01 存储与缓存","uri":"/posts/code-reading/boltdb-made-simple/1-storage-cache/"},{"categories":["深入浅出boltdb"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/0-introduction/:0:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x00 引言","uri":"/posts/code-reading/boltdb-made-simple/0-introduction/"},{"categories":["深入浅出boltdb"],"content":"1. 引言 boltDB是一个完全由go语言编写的基于B+树的kv数据库，其完全支持事务的ACID特性，整个数据库只有一个文件，且有较高的读性能与加载时间。etcd的后端存储使用的便是基于boltdb优化的kv存储etcd-io/bbolt。 boltdb的源码非常适合用来学习B+Tree的实现。boltdb支持完整的事务ACID特性且实现方式较为简单，也适合存储与数据库初学者学习事务与简单的MVCC实现。另外，boltdb完全由go语言编写，因此其对于go语言或其它需要通过unsafe方式管理堆外内存的开发者，也是一个很好的示例。 本系列文章将自底向上地介绍并分析boltdb的实现，较为详细地分析了其源码功能。本文源码基于已归档的boltdb/bolt项目（commit#fd01fc）。 本系列文章主要着眼于boltdb的设计、源码实现与相关知识，在阅读签，读者需要： 学习go语言的基本语法，及unsafe的使用方式。 详细阅读boltdb的README，对boltdb有初步认识。 与前一个《深入浅出etcd/raft》系列相比，boltdb更多偏向工程实现而非算法，因此本系列不会逐行地分析每一行源码。 ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/0-introduction/:1:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x00 引言","uri":"/posts/code-reading/boltdb-made-simple/0-introduction/"},{"categories":["深入浅出boltdb"],"content":"2. 目录 深入浅出boltdb —— 0x00 引言 深入浅出boltdb —— 0x01 存储与缓存 深入浅出boltdb —— 0x02 B+Tree 深入浅出boltdb —— 0x03 bucket \u0026 cursor 深入浅出boltdb —— 0x04 事务 ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/0-introduction/:2:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x00 引言","uri":"/posts/code-reading/boltdb-made-simple/0-introduction/"},{"categories":["深入浅出boltdb"],"content":"3. 施工路线图 引言 存储 缓存 B+Tree bucket cursor 事务 db ","date":"2021-01-05","objectID":"/posts/code-reading/boltdb-made-simple/0-introduction/:3:0","tags":["boltdb","B+Tree"],"title":"深入浅出boltdb —— 0x00 引言","uri":"/posts/code-reading/boltdb-made-simple/0-introduction/"},{"categories":["深入浅出etcd/raft"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2021-01-01","objectID":"/posts/code-reading/etcdraft-made-simple/6-readonly/:0:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x06 只读请求优化","uri":"/posts/code-reading/etcdraft-made-simple/6-readonly/"},{"categories":["深入浅出etcd/raft"],"content":"0. 引言 本文介绍了etcd/raft中只读请求算法优化与实现。这里假定读者阅读过Diego Ongaro的《In Search of an Understandable Consensus Algorithm (Extended Version)》（这里有笔者的翻译，笔者英语水平一般，欢迎指正。），其中提到的部分，本文中不会做详细的解释。对etcd/raft的总体结构不熟悉的读者，可以先阅读《深入浅出etcd/raft —— 0x02 etcd/raft总体设计》。 ","date":"2021-01-01","objectID":"/posts/code-reading/etcdraft-made-simple/6-readonly/:1:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x06 只读请求优化","uri":"/posts/code-reading/etcdraft-made-simple/6-readonly/"},{"categories":["深入浅出etcd/raft"],"content":"1. 处理只读请求算法与优化 Raft算法的目标之一是实现**线性一致性（Linearizability）**的语义。一般在介绍“线性一致性”时，会称其为“强一致性”的一种，但笔者认为这种叫法可能会令读者产生误会。本文不会介绍线性一致性的概念，笔者之后可能会专门写一篇介绍各种一致性的文章，或翻译有关一致性的优质文章。有关线性一致性等各种一致性，笔者推荐阅读Consistency Models. JEPSEN与Strong consistency models. Aphyr，前者对各种一致性进行了全面且正式的介绍，后者通俗地介绍了常用的一致性与其产生的历史等。本文假设读者已经理解线性一致性的含义。 需要注意的，线性一致性的实现不仅与Raft算法本身有关，还与整个系统的实现（即状态机）有关。即使Raft算法本身保证了其日志的故障容错有序共识，但是在通过Raft算法实现系统时，仍会存在有关消息服务质量（Quality of Service，QoS；如至多一次、至少一次、恰好一次等语义问题）、系统整体线性一致性语义等问题。因此《CONSENSUS: BRIDGING THEORY AND PRACTICE》的“Chapter 6 Client interaction”，专门介绍了实现系统时客户端与系统交互的相关问题。需要实现基于Raft算法的读者应详细阅读该章节中介绍的问题与解决方案。 本文仅着眼于只读请求算法优化与实现，因为这一主题与Raft算法本身关系较大，而像“恰好一次”语义等问题的解决方式可能与Raft算法本身关系不大，而是系统实现的常见问题。 ","date":"2021-01-01","objectID":"/posts/code-reading/etcdraft-made-simple/6-readonly/:2:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x06 只读请求优化","uri":"/posts/code-reading/etcdraft-made-simple/6-readonly/"},{"categories":["深入浅出etcd/raft"],"content":"1.1 Log Read Raft算法通过Raft算法实现线性一致性读最简单的方法就是让读请求也通过Raft算法的日志机制实现。即将读请求也作为一条普通的Raft日志，在应用该日志时将读取的状态返回给客户端。这种方法被称为Log Read。 Log Read的实现非常简单，其仅依赖Raft算法已有的机制。但显然，Log Read算法的延迟、吞吐量都很低。因为其既有达成一轮共识所需的开销，又有将这条Raft日志落盘的开销。因此，为了优化只读请求的性能，就要想办法绕过Raft算法完整的日志机制。然而，直接绕过日志机制存在一致性问题，因为Raft算法是基于quorum确认的算法，因此即使日志被提交，也无法保证所有节点都能反映该应用了该日志后的结果。 在Raft算法中，所有的日志写入操作都需要通过leader节点进行。只有leader确认一条日志复制到了quorum数量的节点上，才能确认日志被提交。因此，只要仅通过leader读取数据，那么一定是能保证只读操作的线性一致性的。然而，在一些情况下，leader可能无法及时发现其已经不是合法的leader。这一问题在介绍Raft选举算法的Check Quorum优化是讨论过这一问题。当网络分区出现时，处于小分区的leader可能无法得知集群中已经选举出了新的leader。如果此时原leader还在为客户端提供只读请求的服务，可能会出现stale read的问题。为了解决这一问题，《CONSENSUS: BRIDGING THEORY AND PRACTICE》给出了两个方案：Read Index和Lease Read。 ","date":"2021-01-01","objectID":"/posts/code-reading/etcdraft-made-simple/6-readonly/:2:1","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x06 只读请求优化","uri":"/posts/code-reading/etcdraft-made-simple/6-readonly/"},{"categories":["深入浅出etcd/raft"],"content":"1.2 ReadIndex 显然，只读请求并没有需要写入的数据，因此并不需要将其写入Raft日志，而只需要关注收到请求时leader的commit index。只要在该commit index被应用到状态机后执行读操作，就能保证其线性一致性。因此使用了ReadIndex的leader在收到只读请求时，会按如下方式处理： 记录当前的commit index，作为read index。 向集群中的所有节点广播一次心跳，如果收到了数量达到quorum的心跳响应，leader可以得知当收到该只读请求时，其一定是集群的合法leader。 继续执行，直到leader本地的apply index大于等于之前记录的read index。此时可以保证只读操作的线性一致性。 让状态机执行只读操作，并将结果返回给客户端。 可以看出，ReadIndex的方法只需要一轮心跳广播，既不需要落盘，且其网络开销也很小。ReadIndex方法对吞吐量的提升十分显著，但由于其仍需要一轮心跳广播，其对延迟的优化并不明显。 需要注意的是，实现ReadIndex时需要注意一个特殊情况。当新leader刚刚当选时，其commit index可能并不是此时集群的commit index。因此，需要等到新leader至少提交了一条日志时，才能保证其commit index能反映集群此时的commit index。幸运的是，新leader当选时为了提交非本term的日志，会提交一条空日志。因此，leader只需要等待该日志提交就能开始提供ReadIndex服务，而无需再提交额外的空日志。 通过ReadIndex机制，还能实现follower read。当follower收到只读请求后，可以给leader发送一条获取read index的消息，当leader通过心跳广播确认自己是合法的leader后，将其记录的read index返回给follower，follower等到自己的apply index大于等于其收到的read index后，即可以安全地提供满足线性一致性的只读服务。 ","date":"2021-01-01","objectID":"/posts/code-reading/etcdraft-made-simple/6-readonly/:2:2","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x06 只读请求优化","uri":"/posts/code-reading/etcdraft-made-simple/6-readonly/"},{"categories":["深入浅出etcd/raft"],"content":"1.3 Lease Read ReadIndex虽然提升了只读请求的吞吐量，但是由于其还需要一轮心跳广播，因此只读请求延迟的优化并不明显。而Lease Read在损失了一定的安全性的前提下，进一步地优化了延迟。 Lease Read同样是为了确认当前的leader为合法的leader，但是其实通过心跳与时钟来检查自身合法性的。当leader的heartbeat timeout超时时，其需要向所有节点广播心跳消息。设心跳广播前的时间戳为$start$，当leader收到了至少quorum数量的节点的响应时，该leader可以认为其lease的有效期为$[start, start + election \\ timeout / clock\\ drift\\ bound)$。因为如果在$start$时发送的心跳获得了至少quorum数量节点的响应，那么至少要在election timeout后，集群才会选举出新的leader。但是，由于不同节点的cpu时钟可能有不同程度的漂移，这会导致在一个很小的时间窗口内，即使leader认为其持有lease，但集群已经选举出了新的leader。这与Raft选举优化Leader Lease存在同样的问题。因此，一些系统在实现Lease Read时缩小了leader持有lease的时间，选择了一个略小于election timeout的时间，以减小时钟漂移带来的影响。 当leader持有lease时，leader认为此时其为合法的leader，因此可以直接将其commit index作为read index。后续的处理流程与ReadIndex相同。 需要注意的是，与Leader Lease相同，Lease Read机制同样需要在选举时开启Check Quorum机制。其原因与Leader Lease相同，详见深入浅出etcd/raft —— 0x03 Raft选举，这里不再赘述。 提示 有些文章中常常将实现线性一致性只读请求优化Lease Read机制和选举优化Leader Lease混淆。 Leader Lease是保证follower在能收到合法的leader的消息时拒绝其它candidate，以避免不必要的选举的机制。 Lease Read时leader为确认自己是合法leader，以保证只通过leader为只读请求提供服务时，满足线性一致性的机制。 ","date":"2021-01-01","objectID":"/posts/code-reading/etcdraft-made-simple/6-readonly/:2:3","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x06 只读请求优化","uri":"/posts/code-reading/etcdraft-made-simple/6-readonly/"},{"categories":["深入浅出etcd/raft"],"content":"2. etcd/raft中只读请求优化的实现 ","date":"2021-01-01","objectID":"/posts/code-reading/etcdraft-made-simple/6-readonly/:3:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x06 只读请求优化","uri":"/posts/code-reading/etcdraft-made-simple/6-readonly/"},{"categories":["深入浅出etcd/raft"],"content":"2.1 etcd/raft中ReadIndex方法的使用 在etcd/raft中，使用ReadIndex还是Lease Read方法由通过raft的配置Config的ReadOnlyOption字段决定的： // ReadOnlyOption specifies how the read only request is processed. // // ReadOnlySafe guarantees the linearizability of the read only request by // communicating with the quorum. It is the default and suggested option. // // ReadOnlyLeaseBased ensures linearizability of the read only request by // relying on the leader lease. It can be affected by clock drift. // If the clock drift is unbounded, leader might keep the lease longer than it // should (clock can move backward/pause without any bound). ReadIndex is not safe // in that case. // CheckQuorum MUST be enabled if ReadOnlyOption is ReadOnlyLeaseBased. ReadOnlyOption ReadOnlyOption 该字段的取值有两种：ReadOnlySafe与ReadOnlyLeaseBased，分别对应ReadIndex方法与Lease Read方法： const ( // ReadOnlySafe guarantees the linearizability of the read only request by // communicating with the quorum. It is the default and suggested option. ReadOnlySafe ReadOnlyOption = iota // ReadOnlyLeaseBased ensures linearizability of the read only request by // relying on the leader lease. It can be affected by clock drift. // If the clock drift is unbounded, leader might keep the lease longer than it // should (clock can move backward/pause without any bound). ReadIndex is not safe // in that case. ReadOnlyLeaseBased ) 无论是ReadIndex方法还是Lease Read方法，都需要获取read index。Node的ReadIndex方法就是用来获取read index的方法： // node.go // type Node interface // ReadIndex request a read state. The read state will be set in the ready. // Read state has a read index. Once the application advances further than the read // index, any linearizable read requests issued before the read request can be // processed safely. The read state will have the same rctx attached. ReadIndex(ctx context.Context, rctx []byte) error 当etcd/raft模块的调用者需要获取read index时，需要调用ReadIndex方法。ReadIndex方法不会直接返回read index，而是会在后续的Ready结构体的ReadStates字段中返回多次ReadIndex调用对应的ReadState。 // node.go // type Ready struct // ReadStates can be used for node to serve linearizable read requests locally // when its applied index is greater than the index in ReadState. // Note that the readState will be returned when raft receives msgReadIndex. // The returned is only valid for the request that requested to read. ReadStates []ReadState // ReadState provides state for read only query. // It's caller's responsibility to call ReadIndex first before getting // this state from ready, it's also caller's duty to differentiate if this // state is what it requests through RequestCtx, eg. given a unique id as // RequestCtx type ReadState struct { Index uint64 RequestCtx []byte } 为了让调用者能够区分ReadState是哪次调用的结果，ReadIndex方法需要传入一个唯一的rctx字段进行标识，之后相应的的ReadState的RequestCtx字段会透传rctx的值，以便调用者区分多次调用的结果。 当调用者应用的日志的index大于等于ReadState的Index字段的值时，就可以安全地执行相应的只读请求并返回结果。 ","date":"2021-01-01","objectID":"/posts/code-reading/etcdraft-made-simple/6-readonly/:3:1","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x06 只读请求优化","uri":"/posts/code-reading/etcdraft-made-simple/6-readonly/"},{"categories":["深入浅出etcd/raft"],"content":"2.2 etcd/raft中获取read index的实现 2.2.1 readOnly结构体 在分析etcd/raft中获取read index的实现使用了raft结构体中的两个字段：readStates与readOnly。readStates字段是已经获取的read index，etcd/raft返回的下一个Ready结构体的ReadStates字段会获取readStates字段中的全量数据并清空该字段。而readOnly字段就是一个readOnly结构体的指针。readOnly结构体是leader仅使用ReadIndex时，用来记录等待心跳确认的read index的结构体，其声明如下： type readOnly struct { option ReadOnlyOption pendingReadIndex map[string]*readIndexStatus readIndexQueue []string } readOnly结构体的option字段记录了etcd/raft配置中实现read index的方法。readIndexQueue是多次调用ReadIndex方法时产生的rctx参数队列，其反映了ReadIndex的调用顺序。pendingReadIndex是rctx到其相应的状态readIndexStatus的映射。readIndexStatus结构体的req字段记录了该rctx对应的原消息（在发送该消息的响应时需要用到），index字段记录了待确认的read index的值，ack字段记录了已收到的确认该read index的心跳响应。 type readIndexStatus struct { req pb.Message index uint64 // NB: this never records 'false', but it's more convenient to use this // instead of a map[uint64]struct{} due to the API of quorum.VoteResult. If // this becomes performance sensitive enough (doubtful), quorum.VoteResult // can change to an API that is closer to that of CommittedIndex. acks map[uint64]bool } 如果readOnly的option字段的值为ReadOnlyLeaseBased，说明read index的实现使用了Lease Read，不需要在获取read index前广播心跳，因此不会用到pendingReadIndex与readIndexQueue字段。 readOnly还封装了如下方法： 方法 描述 addRequest(index uint64, m pb.Message) 在广播用来确认read index的心跳消息前，需要调用该方法将该read index加入待确认队列。 recvAck(id uint64, context []byte) map[uint64]bool 当收到确认read index的心跳响应时，需要调用该方法更新该read index的确认状态，该方法会返回收到的确认心跳响应的发送者的id集合。 advance(m pb.Message) []*readIndexStatus 当有read index得到了达到quorum数量节点的ack时，调用该方法返回相应的ReadState，并从待确认的队列中移除相应的read index及其状态。该方法支持批量与流水线操作，因为如果队列中靠后的read index被确认，则其之前的read index也可以确认，因此该方法会返回所有已确认的ReadState。 lastPendingRequestCtx() string 该方法用来获取待确认的最后一条read index对应的rctx。在heartbeat timeout超时构造心跳消息时，其携带的read index标识为最后一条待确认的read index的标识，因为如果队列中靠后的read index被确认，则其之前的read index也可以确认，该方法是为支持批量与流水线操作而设计的。 2.2.2 获取read index流程与实现 Node接口的ReadIndex方法会为Raft状态机应用一条MsgReadIndex消息。etcd/raft实现了Follower Read（1.2节介绍了Follower Read的简单实现），即follower需要将获取read index的请求转发给leader，leader确认自己是合法的leader后将read index返回给follower，然后follower根据其自己的apply index与read index确定什么时候可以执行只读请求。因此，如果应用MsgReadIndex消息的节点是follower，其会将该请求转发给leader： // stepFollower // ... ... case pb.MsgReadIndex: if r.lead == None { r.logger.Infof(\"%x no leader at term %d; dropping index reading msg\", r.id, r.Term) return nil } m.To = r.lead r.send(m) 当leader处理MsgReadIndex请求时（可能来自本地节点，也可能来自follower），其会执行如下逻辑： case pb.MsgReadIndex: // only one voting member (the leader) in the cluster if r.prs.IsSingleton() { if resp := r.responseToReadIndexReq(m, r.raftLog.committed); resp.To != None { r.send(resp) } return nil } // Reject read only request when this leader has not committed any log entry at its term. if !r.committedEntryInCurrentTerm() { return nil } // thinking: use an interally defined context instead of the user given context. // We can express this in terms of the term and index instead of a user-supplied value. // This would allow multiple reads to piggyback on the same message. switch r.readOnly.option { // If more than the local vote is needed, go through a full broadcast. case ReadOnlySafe: r.readOnly.addRequest(r.raftLog.committed, m) // The local node automatically acks the request. r.readOnly.recvAck(r.id, m.Entries[0].Data) r.bcastHeartbeatWithCtx(m.Entries[0].Data) case ReadOnlyLeaseBased: if resp := r.responseToReadIndexReq(m, r.raftLog.committed); resp.To != None { r.send(resp) } } return nil } 首先，leader检查当前是否是以单节点模式运行的（即voter集合是否只有一个节点，但可以有任意数量的learner），如果是，那么该leader一定是合法的leader，因此可以直接返回相应的ReadState。返回ReadState的方法为responseToReadIndexReq方法。该方法会判断获取read index的请求是来自leader本地还是来自follower，如果来自本地则直接将相应的ReadState追加到当前raft结构体的readStates字段中，并返回空消息；如果请求时来自follower，该方法会返回一条用来发送给相应follower的MsgReadIndexResp消息。因此，如果responseToReadIndexReq方法返回的请求的To字段为0，不需要做额外的处理；如果To字段非0，则需要将该消息放入信箱等待发送。 接着，leader需要判断当前的term是否提交过日志，这是为了解决1.2节中提到的新leader当选时commit index落后的问题。如果leader在当前term还没提交过消息，则其会忽略该MsgReadIndex消息。 然后，leader会根据配置的获取read index的方法执行不同的逻辑。当使用Lease Read时，leader可以直接返回相应的ReadState，因为etc","date":"2021-01-01","objectID":"/posts/code-reading/etcdraft-made-simple/6-readonly/:3:2","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x06 只读请求优化","uri":"/posts/code-reading/etcdraft-made-simple/6-readonly/"},{"categories":["深入浅出etcd/raft"],"content":"3. 总结 本文介绍了etcd/raft中只读请求算法优化与实现。etcd/raft中只读请求优化几乎完全是按照论文实现的。在其它的一些基于Raft算法的系统中，其实现的方式可能稍有不同，如不通过Check Quorum实现leader的lease，而是通过日志复制消息为lease续约，且lease的时间也小于election timeout，以减小时钟漂移对一致性的影响。 ","date":"2021-01-01","objectID":"/posts/code-reading/etcdraft-made-simple/6-readonly/:4:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x06 只读请求优化","uri":"/posts/code-reading/etcdraft-made-simple/6-readonly/"},{"categories":["深入浅出etcd/raft"],"content":"参考文献 [1] Ongaro D, Ousterhout J. In search of an understandable consensus algorithm[C]//2014 {USENIX} Annual Technical Conference ({USENIX}{ATC} 14). 2014: 305-319. [2] Ongaro D, Ousterhout J. In search of an understandable consensus algorithm (extended version)[J]. Retrieved July, 2016, 20: 2018. [3] Ongaro D. Consensus: Bridging theory and practice[D]. Stanford University, 2014. [4] Consistency Models. JEPSEN [5] Strong consistency models. Aphyr ","date":"2021-01-01","objectID":"/posts/code-reading/etcdraft-made-simple/6-readonly/:5:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x06 只读请求优化","uri":"/posts/code-reading/etcdraft-made-simple/6-readonly/"},{"categories":["深入浅出etcd/raft"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:0:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"0. 引言 本文会对etcd/raft中Raft成员变更算法的实现与优化进行分析。这里假定读者阅读过Diego Ongaro的《In Search of an Understandable Consensus Algorithm (Extended Version)》（这里有笔者的翻译，笔者英语水平一般，欢迎指正。），其中提到的部分，本文中不会做详细的解释。对etcd/raft的总体结构不熟悉的读者，可以先阅读《深入浅出etcd/raft —— 0x02 etcd/raft总体设计》。 提示 本文不严格区分成员变更（membership changes）与配置变更(configuration chagnes)。 ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:1:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"1. 成员变更算法 《CONSENSUS: BRIDGING THEORY AND PRACTICE》的Chapter 4 Cluster membership changes介绍了两种成员变更算法，一种是一次操作一个节点的简单算法，另一种是联合共识（joint consensus）算法。两种算法都是为了避免由于节点切换配置时间不同导致的同一term出现不只一个leader的问题，如下图所示。 由于不同节点切换配置时间不同而导致的多主问题由于不同节点切换配置时间不同而导致的多主问题 \" 由于不同节点切换配置时间不同而导致的多主问题 为了本文的连贯性，这里简单地介绍一下这两种算法，详细内容请读者自行阅读《CONSENSUS: BRIDGING THEORY AND PRACTICE》的Chapter 4 Cluster membership changes。 简单成员变更算法限制每次只能增加或移除一个节点。这样可以保证新配置与旧配置的quorum至少有一个相同的节点，因为一个节点在同一term仅能给一个节点投票，所以这能避免多主问题。 简单成员变更算法简单成员变更算法 \" 简单成员变更算法 联合共识算法可以一次变更多个成员，但是需要在进入新配置前先进入一个“联合配置（joint configuration）”，在联合配置的quorum分别需要新配置和旧配置的majority（大多数）节点，以避免多主问题。当联合配置成功提交后，集群可以开始进入新配置。 联合共识算法联合共识算法 \" 联合共识算法 etcd/raft的ConfChangeV2既支持简单的“one at a time”的成员变更算法，也支持完整的联合共识算法。需要注意的是，etcd/raft中的配置的应用时间与论文中的不同。在论文引文1中，节点会在追加配置变更日志时应用相应的配置，而在etcd/raft的实现中引文2，当节点应用（apply）配置变更日志条目时才会应用相应的配置。etcd/raft在“apply-time”应用新配置的方式，可以保证配置在应用前已被提交，因此不需要论文中提到的回滚旧配置的操作。但是这种方式需要引入新机制来解决影响集群liveness的边际情况下的问题。 注意 这种\"apply-time\"的方式仍存在一些“liveness”的问题，在编写本文时，etcd社区正在修复这一问题（详见issue#12359 raft: liveness problems during apply-time configuration change）。 本文中的源码基于master分支的commit#a3174d0版本，目前还没有修复这一issue。本文会在issue修复后，基于新版算法进行修改。 另外，需要注意的是，同一时间只能有一个正在进行的配置变更操作，在提议配置变更请求时，如果已经在进行配置变更，那么该提议会被丢弃（被改写成一条无任何意义的日志条目）。 引文1 The new configuration takes effect on each server as soon as it is added to that server’s log: the $C_{new}$ entry is replicated to the $C_{new}$ servers, and a majority of the new configuration is used to determine the $C_{new}$ entry’s commitment. This means that servers do not wait for configuration entries to be committed, and each server always uses the latest configuration found in its log. … … As with the single-server configuration change algorithm, each server starts using a new configuration as soon as it stores the configuration in its log. 引文2 Note that contrary to Joint Consensus as outlined in the Raft paper, configuration changes become active when they are applied to the state machine (not when they are appended to the log). ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:2:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"2. etcd/raft配置的实现 etcd/raft实现的配置是按照joint configuration组织的，本节笔者将以自底向上的方法介绍etcd/raft中配置的实现。 ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:3:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"2.1 MajorityConfig 在joint consensus中，中间状态$C_{old},C_{new}$的quorum同时需要$C_{old}$和$C_{new}$各自的majority。$C_{old}$或$C_{new}$配置中voter的集合（voter即有投票权的角色，包括candidate、follower、leader，而不包括learner），是通过MajorityConfig表示的，MajorityConfig还包括了一些统计majority信息的方法。 // MajorityConfig is a set of IDs that uses majority quorums to make decisions. type MajorityConfig map[uint64]struct{} MajorityConfig的实现非常简单，其只是voter节点id的集合，但MajorityConfig提供了一些很实用的与majority有关方法，如下表所示（仅给出主要方法）： 方法 描述 CommittedIndex(l AckedIndexer) Index 根据给定的AckedIndexer计算被大多数节点接受了的commit index 。 VoteResult(votes map[uint64]bool) VoteResult 根据给定的投票统计计算投票结果。 CommittedIndex是根据该MajorityConfig计算被大多数接受的commit index，其参数AckedIndexer是一个接口： // quorum.go // AckedIndexer allows looking up a commit index for a given ID of a voter // from a corresponding MajorityConfig. type AckedIndexer interface { AckedIndex(voterID uint64) (idx Index, found bool) } // tracker.go type matchAckIndexer map[uint64]*Progress // AckedIndex implements IndexLookuper. func (l matchAckIndexer) AckedIndex(id uint64) (quorum.Index, bool) { pr, ok := l[id] if !ok { return 0, false } return quorum.Index(pr.Match), true } AckedIndexer接口中只定义了一个方法AckedIndex，该方法用来返回给定id的voter的一种索引的值。通过实现该接口与方法，在调用CommittedIndex时，可以根据不同的index来计算被大多数接受的commit index。上面的源码中给出了tracker.go中的一种AckedIndexer实现——matchAckIndexer，其实现的AckedIndex方法返回了voter的match index。etcd/raft在计算commit index时，就是根据节点的match index来计算的。 CommittedIndex的实现也很简单，其通过排序来计算第$n/2+1$小的索引，即为被大多数节点接受的最小索引。该方法中还有针对小切片的分配优化，感兴趣的读者可以自行阅读源码，这里不再赘述。 VoteResult方法的实现也很简单，其根据参数中的投票情况与该MajorityConfig中的voter，计算投票结果。投票结果有三种：VoteWon表示赢得投票、VoteLost表示输掉投票、VotePending表示投票还未完成（既没被大多数接受，也没被大多数拒绝），需要继续等待投票。 ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:3:1","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"2.2 JointConfig JointConfig表示joint consensus下的配置，即$C_{old}$与$C_{new}$的组合： // JointConfig is a configuration of two groups of (possibly overlapping) // majority configurations. Decisions require the support of both majorities. type JointConfig [2]MajorityConfig 为了实现方便，JointConfig的元素0表示$C_{new}$或非*joint consensus*状态下的当前配置，元素0表示$C_{old}$，在非*joint consensus*状态下的元素1应是一个空配置。 JointConfig提供的方法与MajorityConfig几乎一样，其也提供了CommittedIndex方法与VoteResult方法。 JointConfig的CommittedIndex方法会获取两个MajorityConfig的CommittedIndex的返回值（若MajorityConfig为空配置，其返回类型最大值），并返回较小的结果，即被$C_{old}$和$C_{new}$都接受的*commit index*。 JointConfig的VoteResult方法也会获取两个MajorityConfig的VoteResult，如果二者结果相同可以直接返回，否则如果其中之一为VoteLost则直接返回VoteLost，否则还无法给出结果，返回VotePending。 // VoteResult takes a mapping of voters to yes/no (true/false) votes and returns // a result indicating whether the vote is pending, lost, or won. A joint quorum // requires both majority quorums to vote in favor. func (c JointConfig) VoteResult(votes map[uint64]bool) VoteResult { r1 := c[0].VoteResult(votes) r2 := c[1].VoteResult(votes) if r1 == r2 { // If they agree, return the agreed state. return r1 } if r1 == VoteLost || r2 == VoteLost { // If either config has lost, loss is the only possible outcome. return VoteLost } // One side won, the other one is pending, so the whole outcome is. return VotePending } ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:3:2","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"2.3 Config Config记录了全部的成员配置，既包括voter也包括learner。 // Config reflects the configuration tracked in a ProgressTracker. type Config struct { Voters quorum.JointConfig // AutoLeave is true if the configuration is joint and a transition to the // incoming configuration should be carried out automatically by Raft when // this is possible. If false, the configuration will be joint until the // application initiates the transition manually. AutoLeave bool // Learners is a set of IDs corresponding to the learners active in the // current configuration. // // Invariant: Learners and Voters does not intersect, i.e. if a peer is in // either half of the joint config, it can't be a learner; if it is a // learner it can't be in either half of the joint config. This invariant // simplifies the implementation since it allows peers to have clarity about // its current role without taking into account joint consensus. Learners map[uint64]struct{} // When we turn a voter into a learner during a joint consensus transition, // we cannot add the learner directly when entering the joint state. This is // because this would violate the invariant that the intersection of // voters and learners is empty. For example, assume a Voter is removed and // immediately re-added as a learner (or in other words, it is demoted): // // Initially, the configuration will be // // voters: {1 2 3} // learners: {} // // and we want to demote 3. Entering the joint configuration, we naively get // // voters: {1 2} \u0026 {1 2 3} // learners: {3} // // but this violates the invariant (3 is both voter and learner). Instead, // we get // // voters: {1 2} \u0026 {1 2 3} // learners: {} // next_learners: {3} // // Where 3 is now still purely a voter, but we are remembering the intention // to make it a learner upon transitioning into the final configuration: // // voters: {1 2} // learners: {3} // next_learners: {} // // Note that next_learners is not used while adding a learner that is not // also a voter in the joint config. In this case, the learner is added // right away when entering the joint configuration, so that it is caught up // as soon as possible. LearnersNext map[uint64]struct{} } Config的Voter字段类型即为上文介绍的JointConfig，AutoLeaver字段是标识joint consensus情况下离开joint configuration的方式，后文介绍配置变更的实现时会对其进行介绍。Config中与learner相关的字段有两个，分别是Learners和LearnsNext。Learners是当前的learner集合，LearnersNext是在joint consensus情况下，在离开joint configuration时需要新加入learner集合的节点。 etcd/raft为了简化配置变更的实现，其配置需要满足一条约束：配置的voter集合与learner集合不能有交集。在joint consensus方法下，可能出现如下情况： $C_{old}$：voters: {1 2 3}, learners: {}，$C_{new}$：voters: {1 2}, learners: {3}。此时，$C_{new,old}$：voters: {1 2} \u0026 {1 2 3}, learners: {3}，这违背了上文中的约束。 为此，Config中引入了LearnsNext字段，在joint configuration中，如果新的learn集合与voter集合有冲突，那么先将该learn加入到LearnerNext中，在退出joint consensus再将该LearnersNext中的节点加入到Learners中。例如： $C_{old}$：voters: {1 2 3}, learns: {}, next_learns: {} $C_{new,old}$：voters: {1 2} \u0026 {1 2 3}, learns: {}, next_learners: {3} $C_{new}$：voters: {1 2}, learns: {3}, next_learns: {} ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:3:3","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"2.4 ProgressTracker 本系列前文中在介绍etcd/raft的选举与日志复制的实现时，其实已经简单接触过了ProgressTracker。因为leader用来追踪follower复制进度的字段Progress和记录选票的Vote字段就在ProgressTracker中。 // ProgressTracker tracks the currently active configuration and the information // known about the nodes and learners in it. In particular, it tracks the match // index for each peer which in turn allows reasoning about the committed index. type ProgressTracker struct { Config Progress ProgressMap Votes map[uint64]bool MaxInflight int } ProgressTracker提供了如下方法： 方法 描述 func MakeProgressTracker(maxInflight int) ProgressTracker 新建空配置。 ConfState() pb.ConfState 返回当前激活的配置。 IsSingleton() bool 判断当前配置是否为单节点模式（voter数为0，且不处于joint configuration中）。 Committed() uint64 返回被quorum接受的commit index。 Visit(f func(id uint64, pr *Progress)) 有序遍历追踪的所有进度，并对其执行传入的函数闭包。 QuorumActive() bool 判断是否有达到quorum数量的节点处于活跃状态（用于Check Quorum）。 VoterNodes() []uint64 返回有序的voter节点id集合。 LearnerNodes() []uint64 返回有序的learner节点id集合。 ResetVotes() 重置记录的选票。 RecordVote(id uint64, v bool) 记录来自节点id的选票。 TallyVotes() (granted int, rejected int, _ quorum.VoteResult) 返回选票中赞同数、反对数、与当前投票结果。 ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:3:4","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"3. etcd/raft配置变更的实现 etcd/raft中新配置是在apply-time生效的，也就是说，如果etcd/raft模块的使用者在处理Ready结构体的CommittedEntries字段时遇到了实现了ConfChangeI接口的消息时（包括ConfChangeV2和兼容的旧版本ConfChange），需要主动调用Node的ApplyConfChange方法通知Raft状态机使用新配置。该方法最终会调用raft结构体的applyConfChange方法，切换到相应的配置。 本节笔者将分析这一流程的实现。 ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:4:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"3.1 ConfChange类型 在介绍配置变更的实现之前，本节先介绍一下etcd/raft支持的配置变更类型。由于旧版本的配置变更消息ConfChange仅支持“one node a time”的简单算法，这里不再赘述，仅介绍ConfChangeV2中支持的配置变更类型。 ConfChangeV2的Transition字段表示切换配置的行为，其支持的行为有3种： ConfChangeTransitionAuto（默认）：自动选择行为，当配置变更可以通过简单算法完成时，直接使用简单算法；否则使用ConfChangeTransitionJointImplicit行为。 ConfChangeTransitionJointImplicit：强制使用joint consensus，并在适当时间通过Changes为空的ConfChangeV2消息自动退出joint consensus。该方法适用于希望减少joint consensus时间且不需要在状态机中保存joint configuraiont的程序。 ConfChangeTransitionJointExplicit：强制使用joint consensus，但不会自动退出joint consensus，而是需要etcd/raft模块的使用者通过提交Changes为空的ConfChangeV2消息退出joint consensus。该方法适用于希望显式控制配置变更的程序，如自定义了Context字段内容的程序。 ConfChangeV2的Changes字段表示一系列的节点操作，其支持的操作有： ConfChangeAddNode：添加新节点。 ConfChangeRemoveNode：移除节点。 ConfChangeUpdateNode：用于状态机更新节点url等操作，etcd/raft模块本身不会对节点进行任何操作。 ConfChangeAddLearnerNode：添加learner节点。 ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:4:1","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"3.2 提议ConfChange // stepLeader // case pb.MsgProp: // ... ... for i := range m.Entries { e := \u0026m.Entries[i] var cc pb.ConfChangeI if e.Type == pb.EntryConfChange { var ccc pb.ConfChange if err := ccc.Unmarshal(e.Data); err != nil { panic(err) } cc = ccc } else if e.Type == pb.EntryConfChangeV2 { var ccc pb.ConfChangeV2 if err := ccc.Unmarshal(e.Data); err != nil { panic(err) } cc = ccc } if cc != nil { alreadyPending := r.pendingConfIndex \u003e r.raftLog.applied alreadyJoint := len(r.prs.Config.Voters[1]) \u003e 0 wantsLeaveJoint := len(cc.AsV2().Changes) == 0 var refused string if alreadyPending { refused = fmt.Sprintf(\"possible unapplied conf change at index %d (applied to %d)\", r.pendingConfIndex, r.raftLog.applied) } else if alreadyJoint \u0026\u0026 !wantsLeaveJoint { refused = \"must transition out of joint config first\" } else if !alreadyJoint \u0026\u0026 wantsLeaveJoint { refused = \"not in joint state; refusing empty conf change\" } if refused != \"\" { r.logger.Infof(\"%x ignoring conf change %v at config %s: %s\", r.id, cc, r.prs.Config, refused) m.Entries[i] = pb.Entry{Type: pb.EntryNormal} } else { r.pendingConfIndex = r.raftLog.lastIndex() + uint64(i) + 1 } } } // ... ... 在stepLeader方法处理MsgProp时，如果发现ConfChange消息或ConfChangeV2消息，会反序列化消息数据并对其进行一些预处理。 alreadyPending：上一次合法的ConfChange还没被应用时为真。 alreadyJoint：当前配置正处于joint configuration时为真。 wantsLeaveJoint：如果消息（旧格式的消息会转为V2处理）的Changes字段为空时，说明该消息为用于退出joint configuration而转到$C_{new}$的消息。 根据以上3个条件，拒绝该ConfChange的情况有3种： alreadyPending：Raft同一时间只能有一个未被提交的ConfChange，因此拒绝新提议。 alreadyJoint为真但wantsLeaveJoint为假：处于joint configuration的集群必须先退出joint configuration并转为$C_{new}，才能开始新的ConfChange，因此拒绝提议。 alreadyJoint为假单wantsLeaveJoint为真，未处于joint configuration，忽略不做任何变化空ConfChange消息。 对需要拒绝的提议的处理非常简单，只需要将该日志条目替换为没有任何意义的普通空日志条目pb.Entry{Type: pb.EntryNormal}即可。 而对于合法的ConfChange，除了将其追加到日志中外，还需要修改raft结构体的pendingConfIndex字段，将其置为$[上一条ConfChange.Index, 当前ConfChange.Index)$的值引文3（这里置为了处理该MsgProp之前的最后一条日志的index），以供之后遇到ConfChange消息时判断当前ConfChange是否已经被应用。 引文3 pendingConfIndex字段注释： Only one conf change may be pending (in the log, but not yet applied) at a time. This is enforced via pendingConfIndex, which is set to a value \u003e= the log index of the latest pending configuration change (if any). Config changes are only allowed to be proposed if the leader’s applied index is greater than this value. ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:4:2","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"3.3 应用ConfChange 上文中已经提到，etcd/raft需要使用者自行调用Node的ApplyConfChange方法来应用新配置，该方法最终会调用raft结构体的applyConfChange方法，其源码如下： func (r *raft) applyConfChange(cc pb.ConfChangeV2) pb.ConfState { cfg, prs, err := func() (tracker.Config, tracker.ProgressMap, error) { changer := confchange.Changer{ Tracker: r.prs, LastIndex: r.raftLog.lastIndex(), } if cc.LeaveJoint() { return changer.LeaveJoint() } else if autoLeave, ok := cc.EnterJoint(); ok { return changer.EnterJoint(autoLeave, cc.Changes...) } return changer.Simple(cc.Changes...) }() if err != nil { // TODO(tbg): return the error to the caller. panic(err) } return r.switchToConfig(cfg, prs) } 该方法首先通过检查ConfChangeV2的行为与变更内容，并通过confChange.Changer创建新的配置与用于进度跟踪的字典，最后调用switchToConfig方法切换到新配置并更新进度跟踪映射。其中confChange.Changer主要用于校验ConfChangeV2是否合法并生成新配置等，感兴趣的读者可以自行阅读其源码实现，本文不再赘述。下面来分析switchToConfig的实现： // switchToConfig reconfigures this node to use the provided configuration. It // updates the in-memory state and, when necessary, carries out additional // actions such as reacting to the removal of nodes or changed quorum // requirements. // // The inputs usually result from restoring a ConfState or applying a ConfChange. func (r *raft) switchToConfig(cfg tracker.Config, prs tracker.ProgressMap) pb.ConfState { r.prs.Config = cfg r.prs.Progress = prs r.logger.Infof(\"%x switched to configuration %s\", r.id, r.prs.Config) cs := r.prs.ConfState() pr, ok := r.prs.Progress[r.id] // Update whether the node itself is a learner, resetting to false when the // node is removed. r.isLearner = ok \u0026\u0026 pr.IsLearner if (!ok || r.isLearner) \u0026\u0026 r.state == StateLeader { // This node is leader and was removed or demoted. We prevent demotions // at the time writing but hypothetically we handle them the same way as // removing the leader: stepping down into the next Term. // // TODO(tbg): step down (for sanity) and ask follower with largest Match // to TimeoutNow (to avoid interruption). This might still drop some // proposals but it's better than nothing. // // TODO(tbg): test this branch. It is untested at the time of writing. return cs } // The remaining steps only make sense if this node is the leader and there // are other nodes. if r.state != StateLeader || len(cs.Voters) == 0 { return cs } if r.maybeCommit() { // If the configuration change means that more entries are committed now, // broadcast/append to everyone in the updated config. r.bcastAppend() } else { // Otherwise, still probe the newly added replicas; there's no reason to // let them wait out a heartbeat interval (or the next incoming // proposal). r.prs.Visit(func(id uint64, pr *tracker.Progress) { r.maybeSendAppend(id, false /* sendIfEmpty */) }) } // If the the leadTransferee was removed or demoted, abort the leadership transfer. if _, tOK := r.prs.Config.Voters.IDs()[r.leadTransferee]; !tOK \u0026\u0026 r.leadTransferee != 0 { r.abortLeaderTransfer() } return cs } 该方法会将新的Config与ProgressMap应用到ProgressTracker。应用后，获取激活的新状态，并检查该节点是否在新的ProgressMap。如果节点不造ProgressMap中，说明节点已被移除集群。根据不同的新状态，执行如下操作： 如果该节点原来为leader，但在新配置中成为了learner或被移除了集群，那么直接返回，该节点会在下个term退位且无法再发起投票请求。此时状态机可以停止Node。 如果节点原来不是leader节点，或新配置中不包含任何voter，那么该节点同样不需要进行任何操作，直接返回。 否则，该节点原来为leader且继续运行。需要执行一些操作。 如果该节点原来为leader且继续运行，那么按需广播日志复制请求与commit index。最后，该leader检查此时是否在进行leader transfer，如果正在进行leader transfer但目标节点已被从配置中移除，那么终止leader transfer。 ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:4:3","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"3.4 AutoLeave 当ConfChange的类型为ConfChangeTransitionAuto或ConfChangeTransitionJointImplicit时，退出joint configuration由etcd/raft自动实现。由于配置是在用户处理Ready结构体时主动调用ConfChange方法时生效的，因此实现“auto leave”的代码在Advance方法的实现advance方法中，即ConfChange已被用户应用后。在advance方法中，有如下代码： if r.prs.Config.AutoLeave \u0026\u0026 oldApplied \u003c= r.pendingConfIndex \u0026\u0026 newApplied \u003e= r.pendingConfIndex \u0026\u0026 r.state == StateLeader { // If the current (and most recent, at least for this leader's term) // configuration should be auto-left, initiate that now. We use a // nil Data which unmarshals into an empty ConfChangeV2 and has the // benefit that appendEntry can never refuse it based on its size // (which registers as zero). ent := pb.Entry{ Type: pb.EntryConfChangeV2, Data: nil, } // There's no way in which this proposal should be able to be rejected. if !r.appendEntry(ent) { panic(\"refused un-refusable auto-leaving ConfChangeV2\") } r.pendingConfIndex = r.raftLog.lastIndex() r.logger.Infof(\"initiating automatic transition out of joint configuration %s\", r.prs.Config) } 该方法判断调用Advance方法前处理的最后一个Ready中是否包含合法的ConfChange，如果有合法的ConfChange且当前配置开启了AutoLeave，同时该节点是leader的话，那么向其日志中追加一条空的ConfChangeV2消息，以用来触发退出joint configuration的操作。 ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:4:4","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"3.5 补充说明 在本系列之前的文章中，为了避免提前引入过多概念，在分析源码时省略了一些与配置相关的部分。在阅读完本文后，相信读者已经可以理解之前省略的代码中与成员变更相关的部分了，因此这里不再对之前省略的代码进行分析，读者可以找到自己之前不理解的地方自行阅读分析。 ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:4:5","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"4. 总结 本文介绍了etcd/raft中才用的Raft成员变更算法的实现，这部分etcd/raft的改动不是很多，因此更重要的是熟悉论文中的方法。同样，成员变更操作很多零碎的代码也分散在etcd/raft的实现中的各个位置，这里建议读者按照自己的节奏阅读源码，将本文作为参考。 ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:4:6","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"参考文献 [1] Ongaro D, Ousterhout J. In search of an understandable consensus algorithm[C]//2014 {USENIX} Annual Technical Conference ({USENIX}{ATC} 14). 2014: 305-319. [2] Ongaro D, Ousterhout J. In search of an understandable consensus algorithm (extended version)[J]. Retrieved July, 2016, 20: 2018. [3] Ongaro D. Consensus: Bridging theory and practice[D]. Stanford University, 2014. ","date":"2020-12-29","objectID":"/posts/code-reading/etcdraft-made-simple/5-confchange/:5:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x05 Raft成员变更","uri":"/posts/code-reading/etcdraft-made-simple/5-confchange/"},{"categories":["深入浅出etcd/raft"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:0:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"0. 引言 本文会对etcd/raft中Raft日志复制算法的实现与优化进行分析。这里假定读者阅读过Diego Ongaro的《In Search of an Understandable Consensus Algorithm (Extended Version)》（这里有笔者的翻译，笔者英语水平一般，欢迎指正。），其中提到的部分，本文中不会做详细的解释。对etcd/raft的总体结构不熟悉的读者，可以先阅读《深入浅出etcd/raft —— 0x02 etcd/raft总体设计》。 本文首先介绍了etcd/raft中日志复制部分的优化。由于etcd/raft中对日志复制的优化大部分属于实现上的优化，这些优化是在系统中很常见的优化，因此本文会一笔带过其理论部分，而着重于讲解etcd/raft中日志复制的实现。 因为日志复制的逻辑涉及到的方面多、逻辑复杂、经过数年的版本演进部分逻辑难以理解，因此本文后半部分详细地分析了etcd/raft中与日志复制相关的几乎所有逻辑，以供读者参考。这里不建议读者通读本文讲解实现的部分，而是按照自己的节奏阅读源码，在遇到难以理解的部分时可以将本文作为补充参考。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:1:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"1. etcd/raft日志复制优化 本节将介绍etcd/raft中日志复制部分采用的优化。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:2:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"1.1 快速回退 在《In Search of an Understandable Consensus Algorithm (Extended Version)》和《CONSENSUS: BRIDGING THEORY AND PRACTICE》介绍Raft算法基本概念时，提到了一种快速回退next index的方法。当follower拒绝leader的AppendEntries RPC（MsgApp）请求时，follower会通过响应消息（MsgAppResp）的一个字段（RejectHint）告知leader日志冲突的位置与当前term的第一条日志的index。这样，leader可以直接将该follower的next index回退到该位置，然后继续以“一次回退一条”的方式检查冲突。 etcd/raft中也实现了类似的优化，但是其将follower的最后一条日志作为该字段的值。正如Diego Ongaro所说，故障不会经常发生，因此出现很多不一致的日志条目的可能性不大（etcd/raft该部分的作者也是这样想的，详见pull#2021），所以回退到follower的最后一条日志后继续检查冲突即可。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:2:1","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"1.2 并行写入 《CONSENSUS: BRIDGING THEORY AND PRACTICE》的10.2.1 Writing to the leader’s disk in parallel介绍了一种减少Raft算法关键路径上的磁盘写入的优化。在朴素的实现方式中，leader需要先将新日志写入本地稳定存储之后再为follower复制这些日志，这会大大增加处理的延迟。 事实上，这次关键路径上的磁盘写入是可以优化的。leader可以在为follower复制日志的同时将这些日志写入其本地稳定存储。为了简化实现，leader自己的match index表示其写入到了稳定存储的最后一条日志的索引。当当前term的某日志被大多数的match index覆盖时，leader便可以使commit index前进到该节点处。这种优化是安全的，通过这种优化，leader甚至可以在日志写入本地稳定存储完成之前提交日志。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:2:2","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"1.3 批处理与流水线 《CONSENSUS: BRIDGING THEORY AND PRACTICE》的10.2.2 Batching and pipelining介绍了Raft算法实现时的批处理与流水线优化。批处理与流水线是各种系统提高吞吐量的常用方式，etcd/raft也不例外。 1.3.1 批处理 简而言之，批处理（batch）就是在消息到来时先推迟对消息的处理，等到消息积累到一定数量或者经过一段时间后一起处理这批消息，在损失可接受的延迟的情况下提高系统吞吐量。在etcd/raft的实现中主要有两处使用了批处理的设计，一处是网络，一处是存储： 网络：leader在为稳定的follower复制日志时，会用一条消息复制多条日志，且每次可能同时发送多条消息。后文会介绍相关实现。 存储：在前文中笔者介绍过数据的存储时etcd/raft的使用者的责任，使用者需要将Ready结构体中的HardStates、Entries、Snapshot保存到稳定存储，然后在处理完所有字段后调用Advance方法以接收下一批数据。Ready和Advance的设计即体现了微批处理的思想。 1.3.2 流水线 流水线（pipeline）同样是各种系统常用的提高吞吐量的方式。在etcd/raft的实现中，leader在向follower发送完日志复制请求后，不会等待follower响应，而是立即更新其nextIndex，并继续处理，以提高吞吐量。 在正常且稳定的情况下，消息应恰好一次且有序到达。但是在异常情况下，可能出现消息丢失、消息乱序、消息超时等等情况，在前文深入浅出etcd/raft —— 0x03 Raft选举介绍Step方法时，介绍了一些对过期消息的处理方式，重复的地方本文不再赘述。当follower收到过期的日志复制请求时，会拒绝该请求，随后follower会回退其nextIndex以重传之后的日志。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:2:3","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"2. etcd/raft中的日志结构 在分析etcd/raft的日志复制的实现时，首先要了解etcd/raft中Raft日志结构的实现方式。etcd/raft中Raft日志是通过结构体raftLog实现的。本节将介绍raftLog的设计与实现。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:3:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"2.1 raftLog的设计 etcd/raft中Raft日志是通过raftLog结构体记录的。raftLog结构体中，既有还未持久化的数据，也有已经持久化到稳定存储的数据；其中数据既有日志条目，也有快照。如果直观的给出raftLog中数据的逻辑结构，其大概如下图所示。 raftLog逻辑结构示意图raftLog逻辑结构示意图 \" raftLog逻辑结构示意图 raftLog中的数据，按照是否已持久化到稳定存储，可分为两部分：已持久化到稳定存储的部分（stable）和还未持久化到稳定存储的部分（unstable）。无论是stable的部分还是unstable的部分中，都可能包含快照或日志，且每部分的快照中包含的已压缩的日志比该部分相应的未压缩的日志更旧。需要注意的是，在etcd/raft的实现中，在同一时刻，raftLog中的4个段可能并不是同时存在的。 提示 关于日志unstable段对算法安全性的讨论详见5.1节。 在etcd/raft的日志操作中，有4个经常使用的索引： 索引名 描述 committed 在该节点所知数量达到quorum的节点保存到了稳定存储中的日志里，index最高的日志的index。 applied 在该节点的应用程序已应用到其状态机的日志里，index最高的日志的index。 其中， $ applied \\le committed $ 总是成立的。 firstIndex 在该节点的日志中，最新的快照之后的第一条日志的index。 lastIndex 在该节点的日志中，最后一条日志的index。 这里需要注意的是，所有的这些索引都是相对于当前节点而不是整个集群的，例如，当index为$i_1$的日志已被集群中数量达到quorum的节点保存到稳定存储时，一些节点可能还不知道$i_1$已被commit。 raftLog中unstable的部分保存在unstable结构体中，而stable的部分稍有些复杂。为了让使用etcd/raft模块的开发者能够根据自己的需求自定义Raft日志存储，stable的部分不是直接通过内部的结构体实现的。go.etcd.io/etcd/raft/storage.go文件中定义了Storage接口，只要实现了该接口，都可以用来存储stable日志。在深入浅出etcd/raft —— 0x02 etcd/raft总体设计的引言中，笔者提到Storage接口只定义了读取稳定存储中的日志、快照、状态的方法（如下图所示），etcd/raft并不关心也不知道开发者写入稳定存储的方式。那么，etcd/raft是怎样将unstable中的数据写入到稳定存储中的呢？ etcd/raft职责示意图etcd/raft职责示意图 \" etcd/raft职责示意图 在深入浅出etcd/raft —— 0x01 raftexample中，笔者通过了官方提供的raftexample示例，介绍了使用etcd/raft的开发者与Node接口打交道并处理Ready结构体的方式（在深入浅出etcd/raft —— 0x02 etcd/raft总体设计中也有提到）。其中，开发者需要将Ready结构体Entities和Snapshot字段中的数据保存到稳定存储中，这就是将数据从unstable转移到stable中的过程，这种设计也让etcd/raft不需要依赖稳定存储的具体写入方法。下图直观地表示了follower节点从收到leader发来的日志到将其保存至稳定存储中的大致流程（快照的处理方式也同理）。 日志复制流程示意图日志复制流程示意图 \" 日志复制流程示意图 接下来，笔者对Storage和unstable进行分析。其中，对Storage的分析包括etcd采用的Storage实现——MemoryStorage。 说明 Storage接口定义的是稳定存储的读取方法。之所以etcd使用了基于内存的MemoryStorage，是因为etcd在写入MemoryStorage前，需要先写入预写日志（Write Ahead Log，WAL）或快照。而预写日志和快照是保存在稳定存储中的。这样，在每次重启时，etcd可以基于保存在稳定存储中的快照和预写日志恢复MemoryStorage的状态。也就是说，etcd的稳定存储是通过快照、预写日志、MemoryStorage三者共同实现的。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:3:1","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"2.2 Storage的设计与实现 Storage接口定义了etcd/raft中需要的读取稳定存储中日志、快照、状态等方法。 // Storage is an interface that may be implemented by the application // to retrieve log entries from storage. // // If any Storage method returns an error, the raft instance will // become inoperable and refuse to participate in elections; the // application is responsible for cleanup and recovery in this case. type Storage interface { // TODO(tbg): split this into two interfaces, LogStorage and StateStorage. // InitialState returns the saved HardState and ConfState information. InitialState() (pb.HardState, pb.ConfState, error) // Entries returns a slice of log entries in the range [lo,hi). // MaxSize limits the total size of the log entries returned, but // Entries returns at least one entry if any. Entries(lo, hi, maxSize uint64) ([]pb.Entry, error) // Term returns the term of entry i, which must be in the range // [FirstIndex()-1, LastIndex()]. The term of the entry before // FirstIndex is retained for matching purposes even though the // rest of that entry may not be available. Term(i uint64) (uint64, error) // LastIndex returns the index of the last entry in the log. LastIndex() (uint64, error) // FirstIndex returns the index of the first log entry that is // possibly available via Entries (older entries have been incorporated // into the latest Snapshot; if storage only contains the dummy entry the // first log entry is not available). FirstIndex() (uint64, error) // Snapshot returns the most recent snapshot. // If snapshot is temporarily unavailable, it should return ErrSnapshotTemporarilyUnavailable, // so raft state machine could know that Storage needs some time to prepare // snapshot and call Snapshot later. Snapshot() (pb.Snapshot, error) } Storage中定义了需要从稳定存储读取的6种方法（具体定义见上面给出的代码与注释，这里不再赘述）。其中HardState指Raft状态机需要在本地稳定存储中持久化保存的状态，对应论文中的Persistent State（etcd/raft为了优化Raft日志，其保存的字段与原文稍有不同），相应的，SoftState只不需要持久化保存的状态，对应论文中的Volatile State；另外，由于陈旧的日志会被压缩成快照，因此有些方法并不总能获取到所需的值。 接下来对MemoryStorage的实现进行分析。MemoryStorage是etcd中使用的Storage实现，其实现了Storage中定义的读取稳定存储的方法，还实现了相应的写入稳定存储的方法。MemoryStorage结构体中的字段如下： // MemoryStorage implements the Storage interface backed by an // in-memory array. type MemoryStorage struct { // Protects access to all fields. Most methods of MemoryStorage are // run on the raft goroutine, but Append() is run on an application // goroutine. sync.Mutex hardState pb.HardState snapshot pb.Snapshot // ents[i] has raft log position i+snapshot.Metadata.Index ents []pb.Entry } 其中，ents字段就是用来保存日志条目的切片。该切片的首元素被设计为用来保存元数据的条目，而不是真正的日志条目，其保存了快照的最后一条日志对应的term和index，该条目也被成为dummy entry。MemoryStorage中ents字段可以直观表示为下图的结构。 MemoryStorage中ents结构示意图MemoryStorage中ents结构示意图 \" MemoryStorage中ents结构示意图 注意，图中的first index和last index是Storage接口的FirstIndex和LastIndex方法定义的索引，而不是整个raftLog的first index或last index。 MemoryStorage的实现不是很复杂，其中很多逻辑是在处理越界和dummy entry，这里不再占用篇幅详细解释。此外，MemoryStorage通过互斥锁保证其操作是线程安全的。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:3:2","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"2.3 unstable的设计与实现 unstable结构体中保存了还未被保存到稳定存储中的快照或日志条目。 // unstable.entries[i] has raft log position i+unstable.offset. // Note that unstable.offset may be less than the highest log // position in storage; this means that the next write to storage // might need to truncate the log before persisting unstable.entries. type unstable struct { // the incoming unstable snapshot, if any. snapshot *pb.Snapshot // all entries that have not yet been written to storage. entries []pb.Entry offset uint64 logger Logger } unstable结构体中的offset字段记录了unstable的日志起点，该起点可能比Storage中index最高的日志条目旧，也就是说Storage和unstable中的日志可能有部分重叠，因此在处理二者之间的日志时，有一些裁剪日志的操作。 unstable中较为重要的方法如下表所示。 方法 描述 maybeFirstIndex() (uint64, bool) 获取相对整个raftLog的first index，当unstable无法得知该值时，第二个返回值返回false。 maybeLastIndex() (uint64, bool) 获取相对整个raftLog的last index，当unstable无法得知该值时，第二个返回值返回false。 maybeTerm(i uint64) (uint64, bool) 获取给定index的日志条目的term，当unstable无法得知该值时，第二个返回值返回false stableTo(i, t uint64) 通知unstable当前index为i、term为t及其之前的日志已经被保存到了稳定存储中，可以裁剪掉unstable中的这段日志了。裁剪后会根据空间利用率适当地对空间进行优化。 stableSnapTo(i uint64) 通知unstable当前index在i及其之前的快照已经保存到了稳定存储中，如果unstable中保存了该快照，那么可以释放该快照了。 restore(s pb.Snapshot) 根据快照恢复unstable的状态（设置unstbale中的offset、snapshot，并将entries置空）。 truncateAndAppend(ents []pb.Entry) 对给定的日志切片进行裁剪，并将其加入到unstable保存的日志中。 slice(lo uint64, hi uint64) 返回给定范围内的日志切片。首先会通过mustCheckOutOfBounds(lo, hi uint64)方法检查是否越界，如果越界会因此panic。 unstable中的first index和last index的实现与Storage稍有不同。unstable的maybeFirstIndex方法与maybeLastIndex方法获取的是相对整个raftLog的first index或last index，当unstable无法得知整个raftLog的first index或last index时，这两个方法的第二个返回值会被置为false。这种设计与raftLog的实现有关，在raftLog的firstIndex和lastIndex方法中，首先会调用unstable的maybeFirstIndex方法或maybeLastIndex方法，如果查询的索引不在unstable中时，其会继续询问Storage。unstable中maybeFirstIndex方法与maybeLastIndex方法的实现如下。 // maybeFirstIndex returns the index of the first possible entry in entries // if it has a snapshot. func (u *unstable) maybeFirstIndex() (uint64, bool) { if u.snapshot != nil { return u.snapshot.Metadata.Index + 1, true } return 0, false } // maybeLastIndex returns the last index if it has at least one // unstable entry or snapshot. func (u *unstable) maybeLastIndex() (uint64, bool) { if l := len(u.entries); l != 0 { return u.offset + uint64(l) - 1, true } if u.snapshot != nil { return u.snapshot.Metadata.Index, true } return 0, false } 简单来说，只有unstable中包含快照时，unstable才可能得知整个raftLog的first index的位置（快照前的日志不会影响快照后的状态）；而只有当unstable中既没有日志也没有快照时，unstable才无法得知last index的位置。 unstable中另一处比较重要的方法是truncateAndAppend。当raftLog需要将新日志保存到unstable中时会调用该方法。该方法会根据给定的日志切片的范围和当前unstable中日志切片的范围对二者进行适当地裁剪，其逻辑如下。 func (u *unstable) truncateAndAppend(ents []pb.Entry) { after := ents[0].Index switch { case after == u.offset+uint64(len(u.entries)): // after is the next index in the u.entries // directly append u.entries = append(u.entries, ents...) case after \u003c= u.offset: u.logger.Infof(\"replace the unstable entries from index %d\", after) // The log is being truncated to before our current offset // portion, so set the offset and replace the entries u.offset = after u.entries = ents default: // truncate to after and copy to u.entries // then append u.logger.Infof(\"truncate the unstable entries before index %d\", after) u.entries = append([]pb.Entry{}, u.slice(u.offset, after)...) u.entries = append(u.entries, ents...) } } 该方法首先会获取给定日志切片中第一个日志条目的index（after），将其与unstable中已有日志的index进行比较，以确定处理方法： 当after恰好是unstable的下一条日志时，直接将其追加到unstable当前保存的日志之后。 当after比unstable的第一条日志还早时，此时给定的日志切片与unstable中的日志可能有冲突的部分（如Raft算法中leader强制覆盖follower中的日志时），为了更简单地处理冲突，直接将unstable中保存的日志替换为给定日志。 当after在offset之后但与unstable中部分日志重叠时，重叠部分和之后部分可能会有冲突，因此裁剪掉unstable的日志中在after及其之后的部分，并将给定日志追加到其后。 另外，在unstable的stableTo方法裁剪完日志后，会调用shrinkEntriesArray方法优化空间利用率。即如果剩余日志条目小于用来保存日志的切片容量的一半时，将剩余的日志拷贝到容量恰好为剩余日志长度的新切片中，并释放对原切片的引用。需要注意的是，这里不能直接释放原切片的空间或在原切片上进行修改，因为程序的其它部分可能还持有对原切片的引用。 // shrinkEntriesArray discards the underlying array used by the entries slice // if most of it isn't being used. This avoids holding references to a bunch of // potentially large entri","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:3:3","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"2.4 raftLog的实现 在介绍了Storage接口和unstable结构体后，接下来继续看raftLog的具体实现。raftLog结构体源码如下： type raftLog struct { // storage contains all stable entries since the last snapshot. storage Storage // unstable contains all unstable entries and snapshot. // they will be saved into storage. unstable unstable // committed is the highest log position that is known to be in // stable storage on a quorum of nodes. committed uint64 // applied is the highest log position that the application has // been instructed to apply to its state machine. // Invariant: applied \u003c= committed applied uint64 logger Logger // maxNextEntsSize is the maximum number aggregate byte size of the messages // returned from calls to nextEnts. maxNextEntsSize uint64 } 可以看到，raftLog由Storage接口实例storage和unstable结构体实例unstable组成。在2.1节提到的4个常用索引中，committed和applied索引是通过raftLog的字段实现的，而firstIndex和lastIndex是通过raftLog的方法实现的： func (l *raftLog) firstIndex() uint64 { if i, ok := l.unstable.maybeFirstIndex(); ok { return i } index, err := l.storage.FirstIndex() if err != nil { panic(err) // TODO(bdarnell) } return index } func (l *raftLog) lastIndex() uint64 { if i, ok := l.unstable.maybeLastIndex(); ok { return i } i, err := l.storage.LastIndex() if err != nil { panic(err) // TODO(bdarnell) } return i } firstIndex和lastIndex的实现方式在2.3节中已经介绍过，这里不再赘述。raftLog在创建时，会将unstable的offset置为storage的last index + 1，并将committed和applied置为storage的forst index - 1。 // newLog returns log using the given storage and default options. It // recovers the log to the state that it just commits and applies the // latest snapshot. func newLog(storage Storage, logger Logger) *raftLog { return newLogWithSize(storage, logger, noLimit) } // newLogWithSize returns a log using the given storage and max // message size. func newLogWithSize(storage Storage, logger Logger, maxNextEntsSize uint64) *raftLog { if storage == nil { log.Panic(\"storage must not be nil\") } log := \u0026raftLog{ storage: storage, logger: logger, maxNextEntsSize: maxNextEntsSize, } firstIndex, err := storage.FirstIndex() if err != nil { panic(err) // TODO(bdarnell) } lastIndex, err := storage.LastIndex() if err != nil { panic(err) // TODO(bdarnell) } log.unstable.offset = lastIndex + 1 log.unstable.logger = logger // Initialize our committed and applied pointers to the time of the last compaction. log.committed = firstIndex - 1 log.applied = firstIndex - 1 return log } raftLog提供了如下的较为重要的方法： 方法 描述 maybeAppend(index, logTerm, committed uint64, ents ...pb.Entry) 尝试追加新日志，并更新committed索引（详见下文分析）。 unstableEntries() []pb.Entry 返回全部unstable中的日志条目。 nextEnts() (ents []pb.Entry) 返回可被应用到状态机的日志条目（已提交但还未应用），返回的长度受创建raftLog时指定的maxNextEntsSize限制。 hasNextEnts() bool 返回是否存在可被应用到状态机的日志。该方法不会调用slice方法，性能更高。 hasPendingSnapshot() bool 返回是否有未应用到状态机的快照（即unstable中保存的快照）。 commitTo(tocommit uint64) 更新committed索引，该方法会检查参数合法性。 appliedTo(i uint64) 更新applied索引，该方法会检查参数合法性。 stableTo(i, t uint64) 通知unstable当前已保存到稳定存储中最后的日志的index与term，让其适当裁剪日志。 stableSnapTo(i uint64) 通知unstable当前已保存到稳定存储中最后的快照的index，让其释放快照。 lastTerm() uint64 获取最后一条日志的term。 term(i uint64) (uint64, error) 获取给定index的日志条目的term。 entries(i, maxsize uint64) ([]pb.Entry, error) 获取index从i开始的最多maxsize条日志。 allEntries() []pb.Entry 获取全部日志条目。 isUpToDate(lasti, term uint64) bool 判断给定的term和index对应的日志条目是否至少与当前最后一个日志条目一样新。 matchTerm(i, term uint64) bool 判断给定的index与term是否日志中相应index的条目的term匹配。 maybeCommit(maxIndex, term uint64) bool 如果给定的index和term对应的日志条目还未被提交，将日志提交到给日志条目处并返回true，否则返回false。 restore(s pb.Snapshot) 将给定快照应用到（unstable）日志中。 slice(lo, hi, maxSize uint64) ([]pb.Entry, error) 返回index从lo到hi的最多maxSize条日志，该方法会检查参数是否合法。 append与maybeAppend是向raftLog写入日志的方法。二者的区别在于append不会检查给定的日志切片是否与已有日志有冲突，因此leader向raftLog中追加日志时会调用该函数；而maybeAppend会检查是否有冲突并找到冲突位置，并试图通过覆盖本地日志的方式解决冲突。但是，二者都会检查给定的日志起点是否在committed索引位置之前，如果在其之前，这违背了Raft算法的Log Matching性质，因此会引起panic（其实follower不会将committed之前的日志传给该函数，因此永远不会进入该分支）。源码如下： // maybeAppend returns (0, false) if the entries cannot be appended. Otherwise, // it returns (last index of new entries, true). func (l *raftLog) maybeAppen","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:3:4","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"3. 复制进度跟踪 在《In Search of an Understandable Consensus Algorithm (Extended Version)》中，leader只通过 nextInext[] 和 matchIndex[] 来跟踪follower的日志进度。而etcd/raft为了解耦不同情况下的日志复制逻辑并实现一些日志复制相关的优化，还需要记录一些其它信息。因此，etcd/raft中leader使用Progress结构体来跟踪每个follower（和learner）的日志复制进度。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:4:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"3.1 Progress结构体 Progess结构体是leader用来跟踪follower日志复制进度的结构，即“表示从leader视角看到的follower的进度”。leader会为每个follower（和learner）维护各自的Progress结构。官方提供了Progress的设计文档，该文档简单介绍了其设计与功能。 Progress的结构如下： // Progress represents a follower’s progress in the view of the leader. Leader // maintains progresses of all followers, and sends entries to the follower // based on its progress. // // NB(tbg): Progress is basically a state machine whose transitions are mostly // strewn around `*raft.raft`. Additionally, some fields are only used when in a // certain State. All of this isn't ideal. type Progress struct { Match, Next uint64 // State defines how the leader should interact with the follower. // // When in StateProbe, leader sends at most one replication message // per heartbeat interval. It also probes actual progress of the follower. // // When in StateReplicate, leader optimistically increases next // to the latest entry sent after sending replication message. This is // an optimized state for fast replicating log entries to the follower. // // When in StateSnapshot, leader should have sent out snapshot // before and stops sending any replication message. State StateType // PendingSnapshot is used in StateSnapshot. // If there is a pending snapshot, the pendingSnapshot will be set to the // index of the snapshot. If pendingSnapshot is set, the replication process of // this Progress will be paused. raft will not resend snapshot until the pending one // is reported to be failed. PendingSnapshot uint64 // RecentActive is true if the progress is recently active. Receiving any messages // from the corresponding follower indicates the progress is active. // RecentActive can be reset to false after an election timeout. // // TODO(tbg): the leader should always have this set to true. RecentActive bool // ProbeSent is used while this follower is in StateProbe. When ProbeSent is // true, raft should pause sending replication message to this peer until // ProbeSent is reset. See ProbeAcked() and IsPaused(). ProbeSent bool // Inflights is a sliding window for the inflight messages. // Each inflight message contains one or more log entries. // The max number of entries per message is defined in raft config as MaxSizePerMsg. // Thus inflight effectively limits both the number of inflight messages // and the bandwidth each Progress can use. // When inflights is Full, no more message should be sent. // When a leader sends out a message, the index of the last // entry should be added to inflights. The index MUST be added // into inflights in order. // When a leader receives a reply, the previous inflights should // be freed by calling inflights.FreeLE with the index of the last // received entry. Inflights *Inflights // IsLearner is true if this progress is tracked for a learner. IsLearner bool } Progress中有两个重要的索引：match与next。match表示leader所知的该follower的日志中匹配的日志条目的最高index，如果leader不知道该follower的日志状态时，match为0；next表示leader接下来要给该follower发送的日志的第一个条目的index。根据Raft算法论文，next是可能因异常回退的，而match是单调递增的。next小于match的节点会被认为是落后的节点。 Progress的一些常用的方法如下表所示： 方法 描述 ResetState(state StateType) 重置状态为目标状态，该方法会清空所有状态记录的数据。该方法由BecomeXXX方法调用。 BecomeProbe() 将follower转为Probe状态。 BecomeReplicate() 将follower转为Replicate状态。 BecomeSnapshot(snapshoti uint64) 将follower转为Snapshot状态，并指定需要为其发送的快照的index。 MaybeUpdate(n uint64) bool 用于更新follower的进度（match index），如果传入的进度比当前进度旧，则不会更新进度并返回false，该方法还会根据传入的进度更新next index。leader会在收到来自follower的MsgAppResp消息时调用该方法。 OptimisticUpdate(n uint64) 不做检查直接更新next index，用于StateReplicate状态下日志复制流水线优化。 MaybeDecrTo(rejected, last uint64) bool 用于回退next index，该方法会根据参数判断是否需要回退，如果参数是来自过期的消息，那么不会回退。如果回退，则会返回true。 IsPaused() bool 判断为该follower发送消息的发送窗口是否阻塞，发送窗口大小与该follower的状态和Raft的配置有关。 以上的很多方法都与follower的状态有关，因此这里先介绍Progress中规定的3中follower状态。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:4:1","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"3.2 follower的3种状态 为了更加清晰地处理leader为follower复制日志的各种情况，etcd/raft将leader向follower复制日志的行为分成三种，记录在Progress的State字段中： StateProbe：当leader刚刚当选时，或当follower拒绝了leader复制的日志时，该follower的进度状态会变为StateProbe类型。在该状态下，leader每次心跳期间仅为follower发送一条MsgApp消息，且leader会根据follower发送的相应的MsgAppResp消息调整该follower的进度。 StateReplicate：该状态下的follower处于稳定状态，leader会优化为其复制日志的速度，每次可能发送多条MsgApp消息（受Progress的流控限制，后文会详细介绍）。 StateSnapshot：当follower所需的日志已被压缩无法访问时，leader会将该follower的进度置为StateSnapshot状态，并向该follower发送快照。leader不会为处于StateSnapshot状态的follower发送任何的MsgApp消息，直到其成功收到快照。 提示 每条MsgApp消息可以包含多个日志条目。 Progress中的PendingSnapshot、ProbeSent字段是StateProebe和StateSnapshot状态下需要记录的字段，后文会详细讲解。 Progress中的RecentActive字段用来标识该follower最近是否是“活跃”的。该字段除了用于Check Quorum外（详见深入浅出etcd/raft —— 0x03 Raft选举），在日志复制时，leader不会将不活跃的follower转为StateSnapshot状态或发送快照。（这是为了修复issue#3378中提到的问题，感兴趣的读者可以查看该issue和issue#3976）。 Progress的Inflights字段是对日志复制操作进行流控的字段。虽然Config的MaxSizePerMsg字段限制了每条MsgApp消息的字节数，但是在StateReplicate状态下优化日志复制时，每次可能会发送多条MsgApp消息。因此，Config中又加入了MaxInflightMsgs字段来限制每次发送的MsgApp消息数。Inflights实现了MaxInflightMsgs字段配置的流控。 Inflights结构体实现了一个动态扩容的FIFO队列，其中记录了每条MsgApp的Index字段的值，以在收到MsgAppResp的ack时释放队列。Inflights的实现也比较简单，感兴趣的读者可以自行阅读源码学习其实现，这里不再赘述。 Progress的三种状态看做是不同大小的Inflights下的行为（其实并不是这样实现的）: StateProbe =\u003e Inflight.size = 1 StateReplicate =\u003e Inflight.size = MaxInflightMsgs StateSnapshot =\u003e Inflight.size = 0 从IsPaused方法中看到类似的逻辑： // IsPaused returns whether sending log entries to this node has been throttled. // This is done when a node has rejected recent MsgApps, is currently waiting // for a snapshot, or has reached the MaxInflightMsgs limit. In normal // operation, this is false. A throttled node will be contacted less frequently // until it has reached a state in which it's able to accept a steady stream of // log entries again. func (pr *Progress) IsPaused() bool { switch pr.State { case StateProbe: return pr.ProbeSent case StateReplicate: return pr.Inflights.Full() case StateSnapshot: return true default: panic(\"unexpected state\") } } ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:4:2","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"3.3 状态转换与更新回退 在进一步分析etcd/raft的日志复制实现时，需要先简单了解BecomeXXX在进行状态转换时的做的操作及更新进度、回退进度的操作。 // BecomeProbe transitions into StateProbe. Next is reset to Match+1 or, // optionally and if larger, the index of the pending snapshot. func (pr *Progress) BecomeProbe() { // If the original state is StateSnapshot, progress knows that // the pending snapshot has been sent to this peer successfully, then // probes from pendingSnapshot + 1. if pr.State == StateSnapshot { pendingSnapshot := pr.PendingSnapshot pr.ResetState(StateProbe) pr.Next = max(pr.Match+1, pendingSnapshot+1) } else { pr.ResetState(StateProbe) pr.Next = pr.Match + 1 } } BecomeProbe分为两种情况，一种是从StateSnapshot进入StateProbe状态，当leader得知follower成功应用了快照后，需要调用Node的ReportSnapshot方法，该方法会调用BecomeProbe将该follower的进度状态转为StateProbe。此时，可以将next index置为该快照的index的下一条。在一般情况下，则从match index处开始检测冲突（Next是下一条应为该follower发送的日志的index，因此应为当前认为的最后一条匹配日志的index+1）。 // BecomeReplicate transitions into StateReplicate, resetting Next to Match+1. func (pr *Progress) BecomeReplicate() { pr.ResetState(StateReplicate) pr.Next = pr.Match + 1 } // BecomeSnapshot moves the Progress to StateSnapshot with the specified pending // snapshot index. func (pr *Progress) BecomeSnapshot(snapshoti uint64) { pr.ResetState(StateSnapshot) pr.PendingSnapshot = snapshoti } BecomeReplicate和BecomeSnapshot逻辑都很简单，在重置状态后，二者分别设置了相应的next index和正在发送的快照的index。 接下来分析更新match index和next index与回退next index的相关逻辑： // MaybeUpdate is called when an MsgAppResp arrives from the follower, with the // index acked by it. The method returns false if the given n index comes from // an outdated message. Otherwise it updates the progress and returns true. func (pr *Progress) MaybeUpdate(n uint64) bool { var updated bool if pr.Match \u003c n { pr.Match = n updated = true pr.ProbeAcked() } pr.Next = max(pr.Next, n+1) return updated } // OptimisticUpdate signals that appends all the way up to and including index n // are in-flight. As a result, Next is increased to n+1. func (pr *Progress) OptimisticUpdate(n uint64) { pr.Next = n + 1 } MaybeUpdate会根据传入的index更新Match和Next到更高的值，如果Match更新，则会返回true，同时立刻对StateProbe状态的follower进行确认，否则返回false。其调用者会根据返回值判断该follower是否跟上了复制进度。 // MaybeDecrTo adjusts the Progress to the receipt of a MsgApp rejection. The // arguments are the index the follower rejected to append to its log, and its // last index. // // Rejections can happen spuriously as messages are sent out of order or // duplicated. In such cases, the rejection pertains to an index that the // Progress already knows were previously acknowledged, and false is returned // without changing the Progress. // // If the rejection is genuine, Next is lowered sensibly, and the Progress is // cleared for sending log entries. func (pr *Progress) MaybeDecrTo(rejected, last uint64) bool { if pr.State == StateReplicate { // The rejection must be stale if the progress has matched and \"rejected\" // is smaller than \"match\". if rejected \u003c= pr.Match { return false } // Directly decrease next to match + 1. // // TODO(tbg): why not use last if it's larger? pr.Next = pr.Match + 1 return true } // The rejection must be stale if \"rejected\" does not match next - 1. This // is because non-replicating followers are probed one entry at a time. if pr.Next-1 != rejected { return false } pr.Next = max(min(rejected, last+1), 1) pr.ProbeSent = false return true } MaybeDecrTo的参数有follower拒绝的MsgApp请求的index（rejected，即MsgApp或MsgAppResp的Index）和该follower最后一条日志的索引（last，即MsgAppResp的RejectHint）。其中，rejected参数是用来判断该消息是否是过期的消息的，其判断逻辑如下： 如果follower的状态为StateReplicate，Next应该是跟上Match的进度的，那么如果rejected不大于Match，那么该消息过期。 在其它状态下，Next可能没有跟上Match的进度，因此不能通过Match判断。由于其它状态下至多只会为其发送一条日志复制请求，因此只要rejected不等于Next-1，该消息就是过期的。 MaybeDecrTo不会对过期的消息进行处理。否则，将回退Next。Next的回退有两种方案： 回退一条日志。即新的Next为上一条Next-1，这里的Next-1即为发送MsgApp时用于日志匹配的Index字段的值，也是rejected的值。 快速回退，回退到该follower的最后一条日志。即新的Next为该follower最后一条日志的后一条日志的index，即last+1。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:4:3","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"4. etcd/raft中日志复制实现 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:5:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"4.1 节点启动时日志处理 在节点启动时，日志的last index就是稳定存储Storage的last index。follower和candidate不需要对日志进行额外的处理，而leader需要获取每个follower（和learner）的进度，并以当前term提交一条空日志条目，以提交之前term的日志（详见《In Search of an Understandable Consensus Algorithm (Extended Version)》中5.4.2 Committing entries from previous terms）。同样，当新leader当选时，也需要做同样的操作。如下是becomeLeader与reset中与日志复制有关的源码： func (r *raft) becomeLeader() { // ... ... r.reset(r.Term) // ... ... // Followers enter replicate mode when they've been successfully probed // (perhaps after having received a snapshot as a result). The leader is // trivially in this state. Note that r.reset() has initialized this // progress with the last index already. r.prs.Progress[r.id].BecomeReplicate() // ... ... emptyEnt := pb.Entry{Data: nil} if !r.appendEntry(emptyEnt) { // This won't happen because we just called reset() above. r.logger.Panic(\"empty entry was dropped\") } // As a special case, don't count the initial empty entry towards the // uncommitted log quote. This is because we want to preserve the // behavior of allowing one entry larger than quote if the current // usage is zero. r.reduceUncommittedSize([]pb.Entry{emptyEnt}) r.logger.Infof(\"%x became leader at term %d\", r.id, r.Term) } func (r *raft) reset(term uint64) { // ... ... r.prs.ResetVotes() r.prs.Visit(func(id uint64, pr *tracker.Progress) { *pr = tracker.Progress{ Match: 0, Next: r.raftLog.lastIndex() + 1, Inflights: tracker.NewInflights(r.prs.MaxInflight), IsLearner: pr.IsLearner, } if id == r.id { pr.Match = r.raftLog.lastIndex() } }) // ... ... } 在becomeLeader调用reset方法时，会初始化所有节点的next index为leader日志的last index + 1。因为leader刚当选时不知道除了自己之外的节点的复制进度，将除自己外的所有节点的match index置为0，而将自己的match index置为自己的last index。 随后，leader会在当前term为自己的日志追加一条空日志条目，以提交之前term的日志（详见《In Search of an Understandable Consensus Algorithm (Extended Version)》中5.4.2 Committing entries from previous terms）。 在将控日志条目加入到日志后，有一行r.reduceUncommittedSize([]pb.Entry{emptyEnt})代码。想了解这样代码的作用，需要先了解etcd/raft中避免新日志过多无法处理速度跟不上的机制。 在Config中，可以看到如下的一条配置： // MaxUncommittedEntriesSize limits the aggregate byte size of the // uncommitted entries that may be appended to a leader's log. Once this // limit is exceeded, proposals will begin to return ErrProposalDropped // errors. Note: 0 for no limit. MaxUncommittedEntriesSize uint64 该配置用于限制leader日志中未提交日志的最大字节数，如果超过该值则丢弃新提议，以避免新日志过多处理速度跟不上。当该值为0时，表示不设限制。etcd/raft是以如下方式实现该约束的： 在leader调用appendEntry方法向日志追加新条目时，appendEntry方法会调用increaseUncommittedSize(ents []pb.Entry) bool方法，该方法会根据配置与raft结构体中的uncommittedSize字段判断追加后会不会超过MaxUncommittedEntriesSize的限制，如果超过了该限制，会返回false，appendEntry方法会拒绝这些提议，如果没有超过限制，则仅增大uncommittedSize字段字段并返回true。需要注意的是，当uncommittedSize字段为0时不会拒绝提议，以保证leader不会因单条较大的MsgProp消息阻塞；同样该方法也不会拒绝空日志条目，因为其常用于新当选的leader提交之前的term的日志或离开joint configuration。 在etcd/raft的使用者调用Node的Advance方法时，会调用reduceUncommittedSize(ents []pb.Entry)方法，以释放流控容量。 increaseUncommittedSize和reduceUncommittedSize的源码如下： // increaseUncommittedSize computes the size of the proposed entries and // determines whether they would push leader over its maxUncommittedSize limit. // If the new entries would exceed the limit, the method returns false. If not, // the increase in uncommitted entry size is recorded and the method returns // true. // // Empty payloads are never refused. This is used both for appending an empty // entry at a new leader's term, as well as leaving a joint configuration. func (r *raft) increaseUncommittedSize(ents []pb.Entry) bool { var s uint64 for _, e := range ents { s += uint64(PayloadSize(e)) } if r.uncommittedSize \u003e 0 \u0026\u0026 s \u003e 0 \u0026\u0026 r.uncommittedSize+s \u003e r.maxUncommittedSize { // If the uncommitted tail of the Raft log is empty, allow any size // proposal. Otherwise, limit the size of the uncommitted tail of the // log and drop any proposal that would push the size over the limit. // Note the added requirement s\u003e0 which is used to make sure that // appending single empty entries to the log always succeeds, used both // for replicating a new leader's initial empty entry, and for // auto-leaving joi","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:5:1","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"4.2 leader中的日志提议 “提议”是新的日志条目的起点，因此这里从日志的提议开始分析。 日志的提议是通过MsgProp消息实现的。candidate或pre candidate直接丢弃MsgProp消息，follower会将MsgProp消息转发给leader，只有leader会对MsgProp消息做真正的处理： // stepCandidate // ... ... case pb.MsgProp: r.logger.Infof(\"%x no leader at term %d; dropping proposal\", r.id, r.Term) return ErrProposalDropped // stepFollower : // ... ... case pb.MsgProp: if r.lead == None { r.logger.Infof(\"%x no leader at term %d; dropping proposal\", r.id, r.Term) return ErrProposalDropped } else if r.disableProposalForwarding { r.logger.Infof(\"%x not forwarding to leader %x at term %d; dropping proposal\", r.id, r.lead, r.Term) return ErrProposalDropped } m.To = r.lead r.send(m) // stepLeader : // ... ... case pb.MsgProp: if len(m.Entries) == 0 { r.logger.Panicf(\"%x stepped empty MsgProp\", r.id) } if r.prs.Progress[r.id] == nil { // If we are not currently a member of the range (i.e. this node // was removed from the configuration while serving as leader), // drop any new proposals. return ErrProposalDropped } if r.leadTransferee != None { r.logger.Debugf(\"%x [term %d] transfer leadership to %x is in progress; dropping proposal\", r.id, r.Term, r.leadTransferee) return ErrProposalDropped } // Process ConfChange Msg //... ... if !r.appendEntry(m.Entries...) { return ErrProposalDropped } r.bcastAppend() return nil 在leader将MsgProp中的提议追加到本地日志之前，还需要做一些判断与处理： 首先leader会检查自己的Progress结构是否还存在，以判断自己是否已经被ConfChange操作移出了集群，如果该leader被移出了集群，则不会处理该提议。 接着，leader还会判断当前是否在进行leader transfer，如果该leader正在将领导权转移给其它节点，那么同样不会处理该提议。 如果提议中包含ConfChange消息，会做特殊处理，在后文介绍ConfChange时会分析这部分逻辑，这里暂时不做介绍。 如果在追加提议中的日志后会超过MaxUncommittedSize的限制，则不会追加该提议。这部分逻辑在4.1 节点启动时日志处理已经做过介绍，这里不再赘述。 如果leader成功地将这些日志追加到了本地日志中，leade会调用bcastAppend方法，为所有follower（和learner）广播日志追加消息。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:5:2","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"4.3 leader为follower复制日志 leader通过bcastAppend方法为follower（和learner）复制日志，该方法及其相关方法源码如下： // bcastAppend sends RPC, with entries to all peers that are not up-to-date // according to the progress recorded in r.prs. func (r *raft) bcastAppend() { r.prs.Visit(func(id uint64, _ *tracker.Progress) { if id == r.id { return } r.sendAppend(id) }) } // sendAppend sends an append RPC with new entries (if any) and the // current commit index to the given peer. func (r *raft) sendAppend(to uint64) { r.maybeSendAppend(to, true) } // maybeSendAppend sends an append RPC with new entries to the given peer, // if necessary. Returns true if a message was sent. The sendIfEmpty // argument controls whether messages with no entries will be sent // (\"empty\" messages are useful to convey updated Commit indexes, but // are undesirable when we're sending multiple messages in a batch). func (r *raft) maybeSendAppend(to uint64, sendIfEmpty bool) bool { pr := r.prs.Progress[to] if pr.IsPaused() { return false } m := pb.Message{} m.To = to term, errt := r.raftLog.term(pr.Next - 1) ents, erre := r.raftLog.entries(pr.Next, r.maxMsgSize) if len(ents) == 0 \u0026\u0026 !sendIfEmpty { return false } if errt != nil || erre != nil { // send snapshot if we failed to get term or entries // ... ... #1 } else { // ... ... #2 } r.send(m) return true } leader在调用bcastAppend方法时，会向所有其它节点广播MsgApp或MsgSnap消息，且即使是空消息也会广播。这里需要关注的是maybeSendAppend的实现，该函数是向一个节点发送MsgApp或MsgSnap消息的方法。 该方法的大致流程如下： 首先通过Prgoress的IsPaused方法检查该节点进度，如果该节点进度阻塞了，那么不会向其发送消息。 获取用于日志匹配的日志条目（index为next index - 1的日志）的term。 获取该节点的next index之后的日志。 如果日志长度为0且不需要发送空日志，那么直接返回。 如果步骤2、3中任一步骤产生了错误，说明用于日志匹配的条目已被压缩，该节点落后了太多，因此需要为其发送MsgSnap消息；否则，发送MsgApp消息。 调用send方法，填充需要发送的消息中缺失的字段（如Term、From，具体逻辑见send方法的源码，这里不再赘述），并将消息放入该节点的信箱。由于etcd/raft不负责通信模块，因此这里不会真正发送，而是通过Ready结构体将需要发送的消息交给etcd/raft的使用者处理。 接下来先来分析第5步中MsgApp消息的生成方式： // ... ... #2 m.Type = pb.MsgApp m.Index = pr.Next - 1 m.LogTerm = term m.Entries = ents m.Commit = r.raftLog.committed if n := len(m.Entries); n != 0 { switch pr.State { // optimistically increase the next when in StateReplicate case tracker.StateReplicate: last := m.Entries[n-1].Index pr.OptimisticUpdate(last) pr.Inflights.Add(last) case tracker.StateProbe: pr.ProbeSent = true default: r.logger.Panicf(\"%x is sending append in unhandled state %s\", r.id, pr.State) } } 这段逻辑配置了MsgApp消息的相关字段。Index和LogTerm字段是用于日志匹配的日志（即发送的日志的上一条日志）的index与term（用于日志匹配的term字段为LogTerm，消息的Term字段为该节点当前的term，部分消息需要自己指定，部分消息由send方法填充）。Entries字段保存了需要复制的日志条目。Commit字段为leader提交的最后一条日志的索引。 如果该消息携带的日志非空，该方法还会更新该follower的进度状态： 如果节点处于StateReplicate状态，此时通过流水线的方式优化日志复制速度，直接更新其Next索引（详见1.3节），并通过Inflights进行流控（详见3.2节）。 如果节点处于StateProbe状态，此时将ProbeSent置为true，阻塞后续的消息，直到收到确认。 在分析了MsgApp消息的生成方式后，接下来分析MsgSnap消息的生成： // ... ... #1 if !pr.RecentActive { r.logger.Debugf(\"ignore sending snapshot to %x since it is not recently active\", to) return false } m.Type = pb.MsgSnap snapshot, err := r.raftLog.snapshot() if err != nil { if err == ErrSnapshotTemporarilyUnavailable { r.logger.Debugf(\"%x failed to send snapshot to %x because snapshot is temporarily unavailable\", r.id, to) return false } panic(err) // TODO(bdarnell) } if IsEmptySnap(snapshot) { panic(\"need non-empty snapshot\") } m.Snapshot = snapshot sindex, sterm := snapshot.Metadata.Index, snapshot.Metadata.Term r.logger.Debugf(\"%x [firstindex: %d, commit: %d] sent snapshot[index: %d, term: %d] to %x [%s]\", r.id, r.raftLog.firstIndex(), r.raftLog.committed, sindex, sterm, to, pr) pr.BecomeSnapshot(sindex) r.logger.Debugf(\"%x paused sending replication messages to %x [%s]\", r.id, to, pr) 在准备快照之前，这段逻辑线判断了该follower节点最近是否是活跃的，如果不活跃则不会为其发送快照（详见3.2节）。 在生成快照并检测快照无误后，需要通过BecomeSnapshot方法将该follower的状态转为StateSnapshot，以阻塞该节点后续的MsgApp消息。 在follower转为StateSnapshot后，只有两种跳出StateSnapshot的方法： follower节点应用快照后会发送MsgAppResp消息，该消息会报告当前follower的last index。如果follower应用了快照后last index就追赶上了其match index，那么leader会直接将follower的状态转移到StateRelicate状态，为其继续复制日志。 leader节点的使用者还需要主动调用Node的ReportSnapshot方法告知leader节点快照的应用状态，leader会将该follower的状态转移到StateProbe状态","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:5:3","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"4.4 follower处理来自leader的日志 follower处理来自leader的日志复制消息时，同样分为对MsgApp和对MsgSnap的处理，handleAppendEntries方法用来处理MsgApp消息、handleSnapshot用来处理MsgSnap消息。在处理这两种消息时，都会使用MsgAppResp方法对其进行相应。 func (r *raft) handleAppendEntries(m pb.Message) { if m.Index \u003c r.raftLog.committed { r.send(pb.Message{To: m.From, Type: pb.MsgAppResp, Index: r.raftLog.committed}) return } if mlastIndex, ok := r.raftLog.maybeAppend(m.Index, m.LogTerm, m.Commit, m.Entries...); ok { r.send(pb.Message{To: m.From, Type: pb.MsgAppResp, Index: mlastIndex}) } else { r.logger.Debugf(\"%x [logterm: %d, index: %d] rejected MsgApp [logterm: %d, index: %d] from %x\", r.id, r.raftLog.zeroTermOnErrCompacted(r.raftLog.term(m.Index)), m.Index, m.LogTerm, m.Index, m.From) r.send(pb.Message{To: m.From, Type: pb.MsgAppResp, Index: m.Index, Reject: true, RejectHint: r.raftLog.lastIndex()}) } } follower对MsgApp消息的处理可分为如下情况： 如果用于日志匹配的条目在committed之前，说明这是一条过期的消息，因此直接返回MsgAppResp消息，并将消息的Index字段置为committed的值，以让leader快速更新该follower的next index。 接下来，验证用于日志匹配的字段Term与Index是否与本地的日志匹配。如果匹配并保存了日志，则返回MsgAppResp消息，并将消息的Index字段置为本地最后一条日志的index，以让leader发送后续的日志。 如果日志不匹配，返回的MsgAppResp的Reject字段会被置为true，且RejectHint字段值为本地最后一条日志的索引，以便leader快速回退其next index。同时，MsgApp的Index字段会透传给MsgAppResp，以便leader校验该消息是否为过期的消息。 关于RejectHint的使用在1.1节和3.3节中已经介绍过，这里不再赘述。需要注意的是，这里的“返回”，指的同样是将消息存入相应节点的信箱中，等待etcd/raft模块的使用者处理Ready结构体时发送给相应的节点。 而handleSnapshot的处理方式如下： func (r *raft) handleSnapshot(m pb.Message) { sindex, sterm := m.Snapshot.Metadata.Index, m.Snapshot.Metadata.Term if r.restore(m.Snapshot) { r.logger.Infof(\"%x [commit: %d] restored snapshot [index: %d, term: %d]\", r.id, r.raftLog.committed, sindex, sterm) r.send(pb.Message{To: m.From, Type: pb.MsgAppResp, Index: r.raftLog.lastIndex()}) } else { r.logger.Infof(\"%x [commit: %d] ignored snapshot [index: %d, term: %d]\", r.id, r.raftLog.committed, sindex, sterm) r.send(pb.Message{To: m.From, Type: pb.MsgAppResp, Index: r.raftLog.committed}) } } 处理MsgSnap消息时，handleSnapshot方法会调用restore方法尝试应用快照。如果快照应用功能成功，则返回一条MsgAppResp消息，该消息的Index字段为本地最后一条日志的index；而如果快照没有被应用，那么返回的MsgAppResp消息的Index字段会被置为本地的committed索引。 可以看出，对MsgSnap消息的处理，重点在restore方法的实现。 // restore recovers the state machine from a snapshot. It restores the log and the // configuration of state machine. If this method returns false, the snapshot was // ignored, either because it was obsolete or because of an error. func (r *raft) restore(s pb.Snapshot) bool { if s.Metadata.Index \u003c= r.raftLog.committed { return false } // ... ... // Now go ahead and actually restore. if r.raftLog.matchTerm(s.Metadata.Index, s.Metadata.Term) { r.logger.Infof(\"%x [commit: %d, lastindex: %d, lastterm: %d] fast-forwarded commit to snapshot [index: %d, term: %d]\", r.id, r.raftLog.committed, r.raftLog.lastIndex(), r.raftLog.lastTerm(), s.Metadata.Index, s.Metadata.Term) r.raftLog.commitTo(s.Metadata.Index) return false } r.raftLog.restore(s) // ... ... r.logger.Infof(\"%x [commit: %d, lastindex: %d, lastterm: %d] restored snapshot [index: %d, term: %d]\", r.id, r.raftLog.committed, r.raftLog.lastIndex(), r.raftLog.lastTerm(), s.Metadata.Index, s.Metadata.Term) return true } restore方法中有一些对ConfChange的处理，这部分会在本系列后续的文章中介绍，这里暂时略过。除此之外，restore中还有一些防止不应发生的情况的“Defense in depth”代码，这里也不做介绍，感兴趣的读者可以自行结合注释了解。 restore对快照做了如下处理： 如果快照的index没超过本地的committed索引，这说明快照过旧，因此不做处理直接返回false。 将快照的index和term与本地日志匹配，如果成功匹配，说明本地日志已经包含了快照覆盖的日志，因此不要应用该快照。同时，因为快照覆盖的日志都应是已被提交的日志，这也说明了本地的committed索引落后了，因此调用raftLog的commitTo方法，让本地committed索引快速前进到该快照的index，然后直接返回false。 如果到这里方法仍没返回，则可以将快照应用到本地。调用raftLog的restore方法，并返回true。 无论是处理MsgApp消息还是处理MsgSnap消息，返回的消息都是MsgAppResp。下一节中将分析leader对MsgAppResp消息的处理方式。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:5:4","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"4.5 leader处理来自follower的日志复制响应 stepLeader方法在处理MsgAppResp消息时，会根据该消息和发送该消息的follower的状态来进行不同的处理： // stepLeader // ... ... case pb.MsgAppResp: pr.RecentActive = true if m.Reject { r.logger.Debugf(\"%x received MsgAppResp(MsgApp was rejected, lastindex: %d) from %x for index %d\", r.id, m.RejectHint, m.From, m.Index) if pr.MaybeDecrTo(m.Index, m.RejectHint) { r.logger.Debugf(\"%x decreased progress of %x to [%s]\", r.id, m.From, pr) if pr.State == tracker.StateReplicate { pr.BecomeProbe() } r.sendAppend(m.From) } } else { oldPaused := pr.IsPaused() if pr.MaybeUpdate(m.Index) { switch { case pr.State == tracker.StateProbe: pr.BecomeReplicate() case pr.State == tracker.StateSnapshot \u0026\u0026 pr.Match \u003e= pr.PendingSnapshot: // TODO(tbg): we should also enter this branch if a snapshot is // received that is below pr.PendingSnapshot but which makes it // possible to use the log again. r.logger.Debugf(\"%x recovered from needing snapshot, resumed sending replication messages to %x [%s]\", r.id, m.From, pr) // Transition back to replicating state via probing state // (which takes the snapshot into account). If we didn't // move to replicating state, that would only happen with // the next round of appends (but there may not be a next // round for a while, exposing an inconsistent RaftStatus). pr.BecomeProbe() pr.BecomeReplicate() case pr.State == tracker.StateReplicate: pr.Inflights.FreeLE(m.Index) } if r.maybeCommit() { r.bcastAppend() } else if oldPaused { // If we were paused before, this node may be missing the // latest commit index, so send it. r.sendAppend(m.From) } // We've updated flow control information above, which may // allow us to send multiple (size-limited) in-flight messages // at once (such as when transitioning from probe to // replicate, or when freeTo() covers multiple messages). If // we have more entries to send, send as many messages as we // can (without sending empty messages for the commit index) for r.maybeSendAppend(m.From, false) { } // ... ... } } 这段逻辑首先处理了MsgAppResp的Reject字段为true的情况，这只会在follower处理MsgApp消息时发现日志条目不匹配时发生。因此，处理这种消息时，调用了MaybeDecrTo方法回退其Next索引。如果回退失败，说明这是一条过期的消息，不做处理；如果回退成功，且该节点为StateReplicate状态，则调用BecomeProbe使其转为StateProbe状态来查找最后一条匹配日志的位置。回退成功时，还会再次为该节点调用sendAppend方法，以为其发送MsgApp消息。 在处理MsgAppResp的Reject为false的消息时，其会调用MaybeUpdate方法来判断该消息的Index字段是否跟上了该follower的match index，并在需要时更新其next index。如果该消息没有跟上match index，那么不会对该消息做其它处理。其原因有三： 这条消息是过期的消息，不需要处理。 这条消息可能是follower应用快照发来的响应，且此时该follower仍未跟上其match index（可能是follower重启恢复后导致的）。此处后续处理逻辑即为在4.3节中提到的跳出StateSnapshot的第1中情况；如果这里因没跟上match index而没有跳出StateSnapshot状态，也会在etcd/raft模块使用者主动调用ReportSnapshot方法时跳出该状态。因此不会阻塞。 这条消息可能是StateProbe状态的follower发来的确认相应，但此时该follower仍未跟上其match index（可能是follower重启恢复后导致的）。因在一次心跳周期内，leader仅应向处于StateProbe状态的follower发送1条MsgApp消息，因此其释放应在心跳相关的逻辑中，该逻辑会在后文分析。因此也不会阻塞。 在分析完为什么这里仅处理跟上match index的MsgAppResp消息后，接下来其处理方式。 首先，该方法会根据发送该消息的follower的状态进行处理： 如果该follower处于StateProbe状态且现在跟上了进度，则将其转为StateReplica状态。 如果该follower处于StateSnapshot状态且现在跟上了进度，且从该follower发送该消息后到leader处理这条消息时，leader没有为其发送新快照（通过比较Match与PendingSnapshot判断），则将其转为StateReplica状态。 如果该follower处于StateReplicate状态，那么释放Inflights中该消息的Index字段值之前的所有消息。因为收到的MsgAppResp可能是乱序的，因此需要释放之前的所有消息（过期消息不会被处理）。 接下来，该方法调用了maybeCommit方法，该方法会根据所有节点的进度更新leader的commit index，在commit index有更新时返回true，否则返回false（该方法中有与成员变更相关的逻辑，这里暂时不对其进行分析，而是将其留给后续的文章，这里只需要知道其功能即可）。如果commit index有更新，那么调用bcastAppend方法广播新的committed索引。如果commit index没有更新，还需要进一步判断该follower之前是否是阻塞的，如果是那么为该follower发送一条日志复制消息以更新其committed索引，因为在该节点阻塞时可能错过了committed索引的更新消息。 接着，通过for循环继续为该节点发送新的日志复制消息。因为日志复制部分有流控逻辑，因此这里的循环不会成为死循环。这样做可以尽可能多地为节点复制日志，以提高日志复制效率。 最后这里还有一处leader transfer的逻辑，此处在本系列介绍Raft选举时有提到过，这里不再赘述。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:5:5","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"4.6 心跳消息中日志复制相关操作 除了MsgApp、MsgSnap、MsgAppResp消息外，心跳消息MsgHeartbeat即其相应的相应消息MsgHeartbeatResp中也有一些与日志复制相关的逻辑（如StateProbe状态下释放阻塞状态的逻辑）。虽然这部分逻辑不多但同样重要，本节分析这部分逻辑。 首先是leader为follower发送心跳消息时的相关逻辑： // stepLeader // ... ... case pb.MsgBeat: r.bcastHeartbeat() // ... ... // sendHeartbeat sends a heartbeat RPC to the given peer. func (r *raft) sendHeartbeat(to uint64, ctx []byte) { // Attach the commit as min(to.matched, r.committed). // When the leader sends out heartbeat message, // the receiver(follower) might not be matched with the leader // or it might not have all the committed entries. // The leader MUST NOT forward the follower's commit to // an unmatched index. commit := min(r.prs.Progress[to].Match, r.raftLog.committed) m := pb.Message{ To: to, Type: pb.MsgHeartbeat, Commit: commit, Context: ctx, } r.send(m) } 在tickHeartbeat方法中，每次心跳会将一条MsgBeat应用到状态机。该消息会触发bcastHeartbeat方法，为其它节点广播心跳消息。bcastHeartbeat方法中有一些与实现线性一致性读相关的逻辑，这里将其留给本系列的后续文章。这里只需要看该方法最后调用的sendHeartbeat方法，方法生成的MsgHeartbeat消息中的Index字段为leader的committed索引。而在follower处理MsgHeartbeat消息时，会根据该字段更新自己的committed索引，以避免空闲集群没有新提议无法更新follower的committed状态的问题。 func (r *raft) handleHeartbeat(m pb.Message) { r.raftLog.commitTo(m.Commit) r.send(pb.Message{To: m.From, Type: pb.MsgHeartbeatResp, Context: m.Context}) } 随后，follower会向leader发送MsgHeartbeatResp消息作为响应。leader在处理该消息时，主要做的也是线性一致性读相关的处理，但也有部分与日志复制相关的逻辑： case pb.MsgHeartbeatResp: pr.RecentActive = true pr.ProbeSent = false // free one slot for the full inflights window to allow progress. if pr.State == tracker.StateReplicate \u0026\u0026 pr.Inflights.Full() { pr.Inflights.FreeFirstOne() } if pr.Match \u003c r.raftLog.lastIndex() { r.sendAppend(m.From) } // ... ... 在leader收到新条例响应时，会重置ProbeSent为false，以在下一个心跳周期继续为处于StateProbe的follower复制日志。 同时，如果该follower处于StateReplicate状态且其用于流控的Inflights已满，leader会为其释放一个Inflights的槽位，以保证在每个心跳周期处于StateReplicate状态的follower都至少能收到一条MsgApp消息。 最后，如果该节点的match index小于leader当前最后一条日志，则为其调用sendAppend方法来复制新日志。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:5:6","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"5. Q \u0026 A ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:6:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"5.1 为什么raftLog使用了unstable也能保证安全性？ etcd/raft为了能够批处理网络与磁盘I/O，在raftLog中设计了一段还未保存到稳定存储的unstable段。在阅读日志复制部分代码时，有些读者可能会有这一疑惑： follower回复MsgAppResp请求时Index字段为整个raftLog的last index，其中包括了unstable段。而leader会根据MsgAppResp的Index字段更新follower的match index，且leader会根据quorum的match index计算committed index。那么会不会出现被commit的日志其实还没有被quorum的节点保存到稳定存储从而无法保证安全性的情况？ 显然，如果日志在commit之前没有被quorum的节点保存到稳定存储，那么的确存在日志丢失的情况。在《Consensus: Bridging theory and practice》的11.7.3 Avoiding persistent storage writes中确实提到了这种设计引用1。但是etcd/raft中，其实并不会出现没有被quorum节点保存到稳定存储就commit的情况。这与Ready要求的字段处理顺序有关。 首先，正如上文提到，因为etcd/raft中的网络操作也是批处理设计，因此send方法只是将消息放入信箱，而不是立刻将其发出（etcd/raft也没有通信模块）。因此，当follower收到MsgApp请求时，执行的操作实际上是（不考虑特殊情况）： 将新日志追加到unstable中。 将包含unstable的last index的MsgAppResp消息放入信箱，等待发送。 当用户收到下一个Ready结构体时，其收到的其实是如下内容： // node.go func newReady(r *raft, prevSoftSt *SoftState, prevHardSt pb.HardState) Ready { rd := Ready{ Entries: r.raftLog.unstableEntries(), CommittedEntries: r.raftLog.nextEnts(), Messages: r.msgs, } if softSt := r.softState(); !softSt.equal(prevSoftSt) { rd.SoftState = softSt } if hardSt := r.hardState(); !isHardStateEqual(hardSt, prevHardSt) { rd.HardState = hardSt } if r.raftLog.unstable.snapshot != nil { rd.Snapshot = *r.raftLog.unstable.snapshot } if len(r.readStates) != 0 { rd.ReadStates = r.readStates } rd.MustSync = MustSync(r.hardState(), prevHardSt, len(rd.Entries)) return rd } // log.go func (l *raftLog) unstableEntries() []pb.Entry { if len(l.unstable.entries) == 0 { return nil } return l.unstable.entries } 可以看到，Ready结构体的Entries字段是全量的unstable段的日志，Messages字段是全量的信箱中的消息。而Ready结构体的处理顺序必须满足如下顺序： 先将Ready的Entries、HardState、Snapshot保存到稳定存储（如果值非空）。 再发送Ready的Messages字段中的消息。 因此，在etcd/raft模块的使用者将含有unstable的last index的MsgAppResp消息发出之前，unstable中的所有日志已经被保存到了稳定存储中。所以，当leader收到该MsgAppResp并根据其Index字段更新该follower的match index时，match index之前的消息确实被保存到了该follower的稳定存储中。 关于稳定存储与安全性，《Consensus: Bridging theory and practice》给出了更详细的描述与形式化的证明，这里引用2再摘录部分与本问题相关的段落，便于读者参考。 引用1 11.7.3 Avoiding persistent storage writes Many papers suggest using replication rather than stable storage for durability. For example, in Viewstamped Replication Revisited, servers do not write log entries to stable storage. When a server restarts, its log is not used for voting until it learns the current information (its disk is only used as an optimization to avoid network transfers). The trade-off is that data loss is possible in catastrophic events. For example, if a majority of the cluster were to restart simultaneously, the cluster would have potentially lost entries and would not be able to form a new view. Raft could be extended in similar ways to support disk-less operation, but we think the risk of availability or data loss usually outweighs the benefits. 引用2 3.8 Persisted state and server restarts … … Each server also persists new log entries before they are counted towards the entries’ commitment; this prevents committed entries from being lost or “uncommitted” when servers restart. … … The state machine can either be volatile or persistent. A volatile state machine must be recovered after restarts by reapplying log entries (after applying the latest snapshot; see Chapter 5). A persistent state machine, however, has already applied most entries after a restart; to avoid reapplying them, its last applied index must also be persistent. … … If a server loses any of its persistent state, it cannot safely rejoin the cluster with its prior identity. Such a server can usually be added back into the cluster with a new identity by invoking a cluster membership change (see Chapter 4). If a majority of the cluster loses its persistent state, however, log entries may be lost and progress on cluster membership changes will not be possible; to proceed, a system administrator would need to admit the possibility of data loss. ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:6:1","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"5.2 Entries、HardState、Snapshot持久化顺序有要求吗？ 在处理Ready结构体时，除了要保证先持久化再发送消息的顺序，需要持久化的字段的保存顺序也值得关注。官方的建议是按照Entries、HardState、Snapshot的顺序持久化。因为在raft初始化加载HardState时，会检查commit index是否在[snapshot last index, log last index)范围内， // raft.go func (r *raft) loadState(state pb.HardState) { if state.Commit \u003c r.raftLog.committed || state.Commit \u003e r.raftLog.lastIndex() { r.logger.Panicf(\"%x state.commit %d is out of range [%d, %d]\", r.id, state.Commit, r.raftLog.committed, r.raftLog.lastIndex()) } r.raftLog.committed = state.Commit r.Term = state.Term r.Vote = state.Vote } 在etcd的预写日志wal的实现中，Entries和HardState时同步落盘的，以避免重启时不一致的问题。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:6:2","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"6. 总结 本文会对etcd/raft中Raft日志复制算法的实现与优化进行分析。由于etcd/raft中对日志复制的优化大部分属于实现上的优化，因此本文讲解优化理论的部分较少，而讲解etcd/raft中日志复制实现的部分较多。 因为日志复制的逻辑涉及到的方面多、逻辑复杂、经过数年的版本演进部分逻辑难以理解，因此本文详细地分析了etcd/raft中与日志复制相关的几乎所有逻辑，以供读者参考。这里不建议读者通读本文讲解实现的部分，而是按照自己的节奏阅读源码，在遇到难以理解的部分时可以将本文作为补充参考。 ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:7:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"参考文献 [1] Ongaro D, Ousterhout J. In search of an understandable consensus algorithm[C]//2014 {USENIX} Annual Technical Conference ({USENIX}{ATC} 14). 2014: 305-319. [2] Ongaro D, Ousterhout J. In search of an understandable consensus algorithm (extended version)[J]. Retrieved July, 2016, 20: 2018. [3] Ongaro D. Consensus: Bridging theory and practice[D]. Stanford University, 2014. [4] Raft 笔记(五) – Log replication. 我叫尤加利（技术博客） [5] Raft协议实现学习之—初始化和Leader Election过程. BUCKET \u0026 HAMMER（技术博客）（其中对unstable的讨论存在问题，但该文仍提出了一些很好的问题） ","date":"2020-12-23","objectID":"/posts/code-reading/etcdraft-made-simple/4-log/:8:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x04 Raft日志","uri":"/posts/code-reading/etcdraft-made-simple/4-log/"},{"categories":["深入浅出etcd/raft"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/3-election/:0:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x03 Raft选举","uri":"/posts/code-reading/etcdraft-made-simple/3-election/"},{"categories":["深入浅出etcd/raft"],"content":"0. 引言 本文会对etcd/raft中Raft选举算法的实现与优化进行分析。这里假定读者阅读过Diego Ongaro的《In Search of an Understandable Consensus Algorithm (Extended Version)》（这里有笔者的翻译，笔者英语水平一般，欢迎指正。），其中提到的部分，本文中不会做详细的解释。对etcd/raft的总体结构不熟悉的读者，可以先阅读《深入浅出etcd/raft —— 0x02 etcd/raft总体设计》。 本文首先会简单介绍etcd/raft对Raft选举部分的算法优化，然后通过源码分析etcd/raft的选举实现。 提示 由于选举是Raft算法中重要且复杂的部分，因此其代码分布比较零散。只想了解etcd/raft中对Raft算法做出的优化的读者可以只看本文的第一章。对于想要深入etcd/raft源码实现的读者，建议自己先按照自己的节奏阅读源码，对于不太理解的地方可以参考本文的分析，如果直接从本文的分析入手，可能会感觉有些绕。 ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/3-election/:1:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x03 Raft选举","uri":"/posts/code-reading/etcdraft-made-simple/3-election/"},{"categories":["深入浅出etcd/raft"],"content":"1. Raft选举算法优化 在leader选举方面，etcd/raft对《In Search of an Understandable Consensus Algorithm (Extended Version)》中介绍的基本Raft算法做了三种优化。这三种优化都在Diego Ongaro的博士论文《CONSENSUS: BRIDGING THEORY AND PRACTICE》的6.4 Processing read-only queries more efficiently和9.6 Preventing disruptions when a server rejoins the cluster中有提到。 etcd/raft实现的与选举有关的优化有Pre-Vote、Check Quorum、和Leader Lease。在这三种优化中，只有Pre-Vote和Leader Lease最初是对选举过程的优化，Check Quorum期初是为了更高效地实现线性一致性读（Linearizable Read）而做出的优化，但是由于Leader Lease需要依赖Check Quorum，因此笔者也将其放在这里一起讲解。本系列将etcd/raft对实现线性一致性读的优化留在了后续的文章中，本文仅介绍为了实现更高效的线性一致性读需要在选举部分做出的优化。 除此之外，etcd/raft还实现了Leader Transfer，即主动地进行leader的交接。其实现方式比较简单，只需要让希望成为新leader节点主动发起投票请求即可，这里不再过多讲解。需要注意的是，Leader Transfer不保证交接一定成功，只有目标节点能够得到数量达到quorum的选票时才能当选leader，Leader Transfer类型的投票不受Pre-Vote、Check Quorum、Leader Lease机制约束。 ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/3-election/:2:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x03 Raft选举","uri":"/posts/code-reading/etcdraft-made-simple/3-election/"},{"categories":["深入浅出etcd/raft"],"content":"1.1 Pre-Vote 如下图所示，当Raft集群的网络发生分区时，会出现节点数达不到quorum（达成共识至少需要的节点数）的分区，如图中的Partition 1。 网络分区示意图网络分区示意图 \" 网络分区示意图 在节点数能够达到quorum的分区中，选举流程会正常进行，该分区中的所有节点的term最终会稳定为新选举出的leader节点的term。不幸的是，在节点数无法达到quorum的分区中，如果该分区中没有leader节点，因为节点总是无法收到数量达到quorum的投票而不会选举出新的leader，所以该分区中的节点在election timeout超时后，会增大term并发起下一轮选举，这导致该分区中的节点的term会不断增大。 如果网络一直没有恢复，这是没有问题的。但是，如果网络分区恢复，此时，达不到quorum的分区中的节点的term值会远大于能够达到quorum的分区中的节点的term，这会导致能够达到quorum的分区的leader退位（step down）并增大自己的term到更大的term，使集群产生一轮不必要的选举。 Pre-Vote机制就是为了解决这一问题而设计的，其解决的思路在于不允许达不到quorum的分区正常进入投票流程，也就避免了其term号的增大。为此，Pre-Vote引入了“预投票”，也就是说，当节点election timeout超时时，它们不会立即增大自身的term并请求投票，而是先发起一轮预投票。收到预投票请求的节点不会退位。只有当节点收到了达到quorum的预投票响应时，节点才能增大自身term号并发起投票请求。这样，达不到quorum的分区中的节点永远无法增大term，也就不会在分区恢复后引起不必要的一轮投票。 ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/3-election/:2:1","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x03 Raft选举","uri":"/posts/code-reading/etcdraft-made-simple/3-election/"},{"categories":["深入浅出etcd/raft"],"content":"1.2 Check Quorum 在Raft算法中，保证线性一致性读取的最简单的方式，就是讲读请求同样当做一条Raft提议，通过与其它日志相同的方式执行，因此这种方式也叫作Log Read。显然，Log Read的性能很差。而在很多系统中，读多写少的负载是很常见的场景。因此，为了提高读取的性能，就要试图绕过日志机制。 但是，直接绕过日志机制从leader读取，可能会读到陈旧的数据，也就是说存在stale read的问题。在下图的场景中，假设网络分区前，Node 5是整个集群的leader。在网络发生分区后，Partition 0分区中选举出了新leader，也就是图中的Node 1。 stale read示意图stale read示意图 \" stale read示意图 但是，由于网络分区，Node 5无法收到Partition 0中节点的消息，Node 5不会意识到集群中出现了新的leader。此时，虽然它不能成功地完成日志提交，但是如果读取时绕过了日志，它还是能够提供读取服务的。这会导致连接到Node 5的client读取到陈旧的数据。 Check Quorum可以减轻这一问题带来的影响，其机制也非常简单：让leader每隔一段时间主动地检查follower是否活跃。如果活跃的follower数量达不到quorum，那么说明该leader可能是分区前的旧leader，所以此时该leader会主动退位转为follower。 需要注意的是，Check Quorum并不能完全避免stale read的发生，只能减小其发生时间，降低影响。如果需要严格的线性一致性，需要通过其它机制实现。 ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/3-election/:2:2","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x03 Raft选举","uri":"/posts/code-reading/etcdraft-made-simple/3-election/"},{"categories":["深入浅出etcd/raft"],"content":"1.3 Leader Lease 分布式系统中的网络环境十分复杂，有时可能出现网络不完全分区的情况，即整个整个网络拓补图是一个连通图，但是可能并非任意的两个节点都能互相访问。 不完全分区示意图不完全分区示意图 \" 不完全分区示意图 这种现象不止会出现在网络故障中，还会出现在成员变更中。在通过ConfChange移除节点时，不同节点应用该ConfChange的时间可能不同，这也可能导致这一现象发生。 在上图的场景下，Node 1与Node 2之间无法通信。如果它们之间的通信中断前，Node 1是集群的leader，在通信中断后，Node 2无法再收到来自Node 1的心跳。因此，Node 2会开始选举。如果在Node 2发起选举前，Node 1和Node 3中都没有新的日志，那么Node 2仍可以收到能达到quorum的投票（来自Node 2本身的投票和来自Node 3的投票），并成为leader。 Leader Lease机制对投票引入了一条新的约束以解决这一问题：当节点在election timeout超时前，如果收到了leader的消息，那么它不会为其它发起投票或预投票请求的节点投票。也就是说，Leader Lease机制会阻止了正常工作的集群中的节点给其它节点投票。 Leader Lease需要依赖Check Quorum机制才能正常工作。接下来笔者通过一个例子说明其原因。 假如在一个5个节点组成的Raft集群中，出现了下图中的分区情况：Node 1与Node 2互通，Node 3、Node 4、Node 5之间两两互通、Node 5与任一节点不通。在网络分区前，Node 1是集群的leader。 一种可能的网络分区示意图一种可能的网络分区示意图 \" 一种可能的网络分区示意图 在既没有Leader Lease也没有Check Quorum的情况下，Node 3、Node 4会因收不到leader的心跳而发起投票，因为Node 2、Node 3、Node 4互通，该分区节点数能达到quorum，因此它们可以选举出新的leader。 而在使用了Leader Lease而不使用Check Quorum的情况下，由于Node 2仍能够收到原leader Node 1的心跳，受Leader Lease机制的约束，它不会为其它节点投票。这会导致即使整个集群中存在可用节点数达到quorum的分区，但是集群仍无法正常工作。 而如果同时使用了Leader Lease和Check Quorum，那么在上图的情况下，Node 1会在election timeout超时后因检测不到数量达到quorum的活跃节点而退位为follower。这样，Node 2、Node 3、Node 4之间的选举可以正常进行。 ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/3-election/:2:3","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x03 Raft选举","uri":"/posts/code-reading/etcdraft-made-simple/3-election/"},{"categories":["深入浅出etcd/raft"],"content":"1.4 引入的新问题与解决方案 引入Pre-Vote和Check Quorum（etcd/raft的实现中，开启Check Quorum会自动开启Leader Lease）会为Raft算法引入一些新的问题。 当一个节点收到了term比自己低的消息时，原本的逻辑是直接忽略该消息，因为term比自己低的消息仅可能是因网络延迟的迟到的旧消息。然而，开启了这些机制后，在如下的场景中会出现问题： 场景1示意图场景1示意图 \" 场景1示意图 场景1： 如上图所示，在开启了Check Quorum / Leader Lease后（假设没有开启Pre-Vote，Pre-Vote的问题在下一场景中讨论），数量达不到quorum的分区中的leader会退位，且该分区中的节点永远都无法选举出leader，因此该分区的节点的term会不断增大。当该分区与整个集群的网络恢复后，由于开启了Check Quorum / Leader Lease，即使该分区中的节点有更大的term，由于原分区的节点工作正常，它们的选举请求会被丢弃。同时，由于该节点的term比原分区的leader节点的term大，因此它会丢弃原分区的leader的请求。这样，该节点永远都无法重新加入集群，也无法当选新leader。（详见issue #5451、issue #5468）。 场景2示意图场景2示意图 \" 场景2示意图 场景2： Pre-Vote机制也有类似的问题。如上图所示，假如发起预投票的节点，在预投票通过后正要发起正式投票的请求时出现网络分区。此时，该节点的term会高于原集群的term。而原集群因没有收到真正的投票请求，不会更新term，继续正常运行。在网络分区恢复后，原集群的term低于分区节点的term，但是日志比分区节点更新。此时，该节点发起的预投票请求因没有日志落后会被丢弃，而原集群leader发给该节点的请求会因term比该节点小而被丢弃。同样，该节点永远都无法重新加入集群，也无法当选新leader。（详见issue #8501、issue #8525）。 场景3： 在更复杂的情况中，比如，在变更配置时，开启了原本没有开启的Pre-Vote机制。此时可能会出现与上一条类似的情况，即可能因term更高但是log更旧的节点的存在导致整个集群的死锁，所有节点都无法预投票成功。这种情况比上一种情况更危险，上一种情况只有之前分区的节点无法加入集群，在这种情况下，整个集群都会不可用。（详见issue #8501、issue #8525）。 为了解决以上问题，节点在收到term比自己低的请求时，需要做特殊的处理。处理逻辑也很简单： 如果收到了term比当前节点term低的leader的消息，且集群开启了Check Quorum / Leader Lease或Pre-Vote，那么发送一条term为当前term的消息，令term低的节点成为follower。（针对场景1、场景2） 对于term比当前节点term低的预投票请求，无论是否开启了Check Quorum / Leader Lease或Pre-Vote，都要通过一条term为当前term的消息，迫使其转为follower并更新term。（针对场景3） ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/3-election/:2:4","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x03 Raft选举","uri":"/posts/code-reading/etcdraft-made-simple/3-election/"},{"categories":["深入浅出etcd/raft"],"content":"2. etcd/raft中Raft选举的实现 本节中，笔者将分析etcd/raft中选举部分的实现。 ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/3-election/:3:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x03 Raft选举","uri":"/posts/code-reading/etcdraft-made-simple/3-election/"},{"categories":["深入浅出etcd/raft"],"content":"2.1 MsgHup与hup 在etcd/raft的实现中，选举的触发是通过MsgHup消息实现的，无论是主动触发选举还是因election timeout超时都是如此： // *** node.go *** func (n *node) Campaign(ctx context.Context) error { return n.step(ctx, pb.Message{Type: pb.MsgHup}) } // *** rawnode.go *** func (rn *RawNode) Campaign() error { return rn.raft.Step(pb.Message{ Type: pb.MsgHup, }) } // *** raft.go *** // tickElection is run by followers and candidates after r.electionTimeout. func (r *raft) tickElection() { r.electionElapsed++ if r.promotable() \u0026\u0026 r.pastElectionTimeout() { r.electionElapsed = 0 r.Step(pb.Message{From: r.id, Type: pb.MsgHup}) } } 因此可以跟着MsgHup的处理流程，分析etcd/raft中选举的实现。正如笔者在《深入浅出etcd/raft —— 0x02 etcd/raft总体设计》中所说，etcd/raft通过raft结构体的Step方法实现Raft状态机的状态转移。 func (r *raft) Step(m pb.Message) error { // ... ... switch m.Type { case pb.MsgHup: if r.preVote { r.hup(campaignPreElection) } else { r.hup(campaignElection) } // ... ... } // ... ... } Step方法在处理MsgHup消息时，会根据当前配置中是否开启了Pre-Vote机制，以不同的CampaignType调用hup方法。CampaignType是一种枚举类型（go语言的枚举实现方式），其可能值如下表所示。 值 描述 campaignPreElection 表示Pre-Vote的预选举阶段。 campaignElection 表示正常的选举阶段（仅超时选举，不包括Leader Transfer）。 campaignTransfer 表示Leader Transfer阶段。 接下来对hup的实现进行分析。 func (r *raft) hup(t CampaignType) { if r.state == StateLeader { r.logger.Debugf(\"%x ignoring MsgHup because already leader\", r.id) return } if !r.promotable() { r.logger.Warningf(\"%x is unpromotable and can not campaign\", r.id) return } ents, err := r.raftLog.slice(r.raftLog.applied+1, r.raftLog.committed+1, noLimit) if err != nil { r.logger.Panicf(\"unexpected error getting unapplied entries (%v)\", err) } if n := numOfPendingConf(ents); n != 0 \u0026\u0026 r.raftLog.committed \u003e r.raftLog.applied { r.logger.Warningf(\"%x cannot campaign at term %d since there are still %d pending configuration changes to apply\", r.id, r.Term, n) return } r.logger.Infof(\"%x is starting a new election at term %d\", r.id, r.Term) r.campaign(t) } hup方法会对节点当前状态进行一些检查，如果检查通过才会试图让当前节点发起投票或预投票。首先，hup会检查当前节点是否已经是leader，如果已经是leader那么直接返回。接下来，hup通过promotable()方法判断当前节点能否提升为leader。 // promotable indicates whether state machine can be promoted to leader, // which is true when its own id is in progress list. func (r *raft) promotable() bool { pr := r.prs.Progress[r.id] return pr != nil \u0026\u0026 !pr.IsLearner \u0026\u0026 !r.raftLog.hasPendingSnapshot() } promotable()的判定规则有三条： 当前节点是否已被集群移除。（通过ProgressTracker.ProgressMap映射中是否有当前节点的id的映射判断。当节点被从集群中移除后，被移除的节点id会被从该映射中移除。笔者会在后续讲解集群配置变更的文章中详细分析其实现。） 当前节点是否为learner节点。 当前节点是否还有未被保存到稳定存储中的快照。 这三条规则中，只要有一条为真，那么当前节点就无法成为leader。在hup方法中，除了需要promotable()为真，还需要判断一条规则： 当前的节点已提交的日志中，是否有还未被应用的集群配置变更ConfChange消息。 如果当前节点已提交的日志中还有未应用的ConfChange消息，那么该节点也无法提升为leader。 只有当以上条件都满足后，hup方法才会调用campaign方法，根据配置，开始投票或预投票。 ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/3-election/:3:1","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x03 Raft选举","uri":"/posts/code-reading/etcdraft-made-simple/3-election/"},{"categories":["深入浅出etcd/raft"],"content":"2.2 campaign campaign是用来发起投票或预投票的重要方法。 // campaign transitions the raft instance to candidate state. This must only be // called after verifying that this is a legitimate transition. func (r *raft) campaign(t CampaignType) { if !r.promotable() { // This path should not be hit (callers are supposed to check), but // better safe than sorry. r.logger.Warningf(\"%x is unpromotable; campaign() should have been called\", r.id) } var term uint64 var voteMsg pb.MessageType if t == campaignPreElection { r.becomePreCandidate() voteMsg = pb.MsgPreVote // PreVote RPCs are sent for the next term before we've incremented r.Term. term = r.Term + 1 } else { r.becomeCandidate() voteMsg = pb.MsgVote term = r.Term } if _, _, res := r.poll(r.id, voteRespMsgType(voteMsg), true); res == quorum.VoteWon { // We won the election after voting for ourselves (which must mean that // this is a single-node cluster). Advance to the next state. if t == campaignPreElection { r.campaign(campaignElection) } else { r.becomeLeader() } return } var ids []uint64 { idMap := r.prs.Voters.IDs() ids = make([]uint64, 0, len(idMap)) for id := range idMap { ids = append(ids, id) } sort.Slice(ids, func(i, j int) bool { return ids[i] \u003c ids[j] }) } for _, id := range ids { if id == r.id { continue } r.logger.Infof(\"%x [logterm: %d, index: %d] sent %s request to %x at term %d\", r.id, r.raftLog.lastTerm(), r.raftLog.lastIndex(), voteMsg, id, r.Term) var ctx []byte if t == campaignTransfer { ctx = []byte(t) } r.send(pb.Message{Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx}) } } 因为调用campaign的方法不止有hup，campaign方法首先还是会检查promotable()是否为真。 if t == campaignPreElection { r.becomePreCandidate() voteMsg = pb.MsgPreVote // PreVote RPCs are sent for the next term before we've incremented r.Term. term = r.Term + 1 } else { r.becomeCandidate() voteMsg = pb.MsgVote term = r.Term } 在开启Pre-Vote后，首次调用campaign时，参数为campaignPreElection。此时会调用becomePreCandidate方法，该方法不会修改当前节点的Term值，因此发送的MsgPreVote消息的Term应为当前的Term + 1 。而如果没有开启Pre-Vote或已经完成预投票进入正式投票的流程或是Leader Transfer时（即使开启了Pre-Vote，Leader Transfer也不会进行预投票），会调用becomeCandidate方法。该方法会增大当前节点的Term，因此发送MsgVote消息的Term就是此时的Term。becomeXXX用来将当前状态机的状态与相关行为切换相应的角色，笔者会在后文详细分析其实现与修改后的行为。 接下来，campaign方法开始发送投票请求。在向其它节点发送请求之前，该节点会先投票给自己： if _, _, res := r.poll(r.id, voteRespMsgType(voteMsg), true); res == quorum.VoteWon { // We won the election after voting for ourselves (which must mean that // this is a single-node cluster). Advance to the next state. if t == campaignPreElection { r.campaign(campaignElection) } else { r.becomeLeader() } return } poll方法会在更新本地的投票状态并获取当前投票结果。如果节点投票给自己后就赢得了选举，这说明集群是以单节点的模式启动的，那么如果当前是预投票阶段当前节点就能立刻开启投票流程、如果已经在投票流程中或是在Leader Transfer就直接当选leader即可。如果集群不是以单节点的模式运行的，那么就需要向其它有资格投票的节点发送投票请求： var ids []uint64 { idMap := r.prs.Voters.IDs() ids = make([]uint64, 0, len(idMap)) for id := range idMap { ids = append(ids, id) } sort.Slice(ids, func(i, j int) bool { return ids[i] \u003c ids[j] }) } for _, id := range ids { if id == r.id { continue } r.logger.Infof(\"%x [logterm: %d, index: %d] sent %s request to %x at term %d\", r.id, r.raftLog.lastTerm(), r.raftLog.lastIndex(), voteMsg, id, r.Term) var ctx []byte if t == campaignTransfer { ctx = []byte(t) } r.send(pb.Message{Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx}) } 请求的Term字段就是之前记录的term，即预投票阶段为当前Term + 1、投票阶段为当前的Term。 ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/3-election/:3:2","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x03 Raft选举","uri":"/posts/code-reading/etcdraft-made-simple/3-election/"},{"categories":["深入浅出etcd/raft"],"content":"2.3 Step方法与step 在前文中，笔者提到过Step函数是Raft状态机状态转移的入口方法，Step方法的参数是Raft消息。Step方法会检查消息的Term字段，对不同的情况进行不同的处理。Step方法还会对与选举相关的一些的消息进行特殊的处理。最后，Step会调用raft接口体step字段中记录的函数签名。step字段的定义如下： // Definition of `stepFunc` type stepFunc func(r *raft, m pb.Message) error // step field in struct `raft` step stepFunc 上一节中提到的becomeXXX函数会让状态机切换到相应角色，并切换raft结构体的step字段中记录的函数。让不同角色的节点能够用不同的逻辑来处理Raft消息。 在调用step字段记录的函数处理请求前，Step会根据消息的Term字段，进行一些预处理。 2.3.1 对Term为0的消息的预处理 // Handle the message term, which may result in our stepping down to a follower. switch { case m.Term == 0: // local message // case ... ... } etcd/raft使用Term为0的消息作为本地消息，Step不会对本地消息进行特殊处理，直接进入之后的逻辑。 2.3.2 对Term大于当前节点Term的消息的预处理 case m.Term \u003e r.Term: if m.Type == pb.MsgVote || m.Type == pb.MsgPreVote { force := bytes.Equal(m.Context, []byte(campaignTransfer)) inLease := r.checkQuorum \u0026\u0026 r.lead != None \u0026\u0026 r.electionElapsed \u003c r.electionTimeout if !force \u0026\u0026 inLease { // If a server receives a RequestVote request within the minimum election timeout // of hearing from a current leader, it does not update its term or grant its vote r.logger.Infof(\"%x [logterm: %d, index: %d, vote: %x] ignored %s from %x [logterm: %d, index: %d] at term %d: lease is not expired (remaining ticks: %d)\", r.id, r.raftLog.lastTerm(), r.raftLog.lastIndex(), r.Vote, m.Type, m.From, m.LogTerm, m.Index, r.Term, r.electionTimeout-r.electionElapsed) return nil } } switch { case m.Type == pb.MsgPreVote: // Never change our term in response to a PreVote case m.Type == pb.MsgPreVoteResp \u0026\u0026 !m.Reject: // We send pre-vote requests with a term in our future. If the // pre-vote is granted, we will increment our term when we get a // quorum. If it is not, the term comes from the node that // rejected our vote so we should become a follower at the new // term. default: r.logger.Infof(\"%x [term: %d] received a %s message with higher term from %x [term: %d]\", r.id, r.Term, m.Type, m.From, m.Term) if m.Type == pb.MsgApp || m.Type == pb.MsgHeartbeat || m.Type == pb.MsgSnap { r.becomeFollower(m.Term, m.From) } else { r.becomeFollower(m.Term, None) } } 对于Term大于当前节点的Term的消息，如果消息类型为MsgVote或MsgPreVote，先要检查这些消息是否需要处理。其判断规则如下： force：如果该消息的CampaignType为campaignTransfer，force为真，表示该消息必须被处理。 inLease：如果开启了Check Quorum（开启Check Quorum会自动开启Leader Lease），且election timeout超时前收到过leader的消息，那么inLease为真，表示当前Leader Lease还没有过期。 如果!force \u0026\u0026 inLease，说明该消息不需要被处理，可以直接返回。 对于Term大于当前节点的Term的消息，Step还需要判断是否需要切换自己的身份为follower，其判断规则如下： 如果消息为MsgPreVote消息，那么不需要转为follower。 如果消息为MsgPreVoteResp且Reject字段不为真时，那么不需要转为follower。 否则，转为follower。 在转为follower时，新的Term就是该消息的Term。如果消息类型是MsgApp、MsgHeartbeat、MsgSnap，说明这是来自leader的消息，那么将lead字段直接置为该消息的发送者的id，否则暂时不知道当前的leader节点是谁。 2.3.3 对Term小于当前节点Term的消息的预处理 最后，如果消息的Term比当前Term小，因存在1.4节中提到的问题，除了忽略消息外，还要做额外的处理： case m.Term \u003c r.Term: if (r.checkQuorum || r.preVote) \u0026\u0026 (m.Type == pb.MsgHeartbeat || m.Type == pb.MsgApp) { // We have received messages from a leader at a lower term. It is possible // that these messages were simply delayed in the network, but this could // also mean that this node has advanced its term number during a network // partition, and it is now unable to either win an election or to rejoin // the majority on the old term. If checkQuorum is false, this will be // handled by incrementing term numbers in response to MsgVote with a // higher term, but if checkQuorum is true we may not advance the term on // MsgVote and must generate other messages to advance the term. The net // result of these two features is to minimize the disruption caused by // nodes that have been removed from the cluster's configuration: a // removed node will send MsgVotes (or MsgPreVotes) which will be ignored, // but it will not receive MsgApp or MsgHeartbeat, so it will not create // disruptive term increases, by notifying leader of this node's activeness. // The above comments also true for Pre-Vote // // When follower gets isolated, it soon starts an election ending // up with a higher term than leader, although it won't receive enough // votes to w","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/3-election/:3:3","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x03 Raft选举","uri":"/posts/code-reading/etcdraft-made-simple/3-election/"},{"categories":["深入浅出etcd/raft"],"content":"2.4 becomeXXX与stepXXX 在上文中笔者介绍过，becomeXXX函数用于将切换Raft状态机的角色，stepXXX是Raft状态机的相应角色下状态转移的行为。etcd/raft中becomeXXX共有四种：becomeFollower、becomeCandidate、becomePreCandidate、becomeLeader，stepXXX共有三种：stepLeader、stepCandidate、stepFollower，becomeCandidate和becomePreCandidate相应的行为均为stepCandidate。 本节中，笔者将介绍becomeXXX和stepXXX中与选举相关的逻辑。 2.4.1 Candidate、PreCandidate Candidate和PreCandidate的行为有很多相似之处，本节笔者将分析二者行为并比对异同之处。 func (r *raft) becomeCandidate() { // ... ... r.step = stepCandidate r.reset(r.Term + 1) r.tick = r.tickElection r.Vote = r.id r.state = StateCandidate r.logger.Infof(\"%x became candidate at term %d\", r.id, r.Term) } func (r *raft) becomePreCandidate() { // ... ... // Becoming a pre-candidate changes our step functions and state, // but doesn't change anything else. In particular it does not increase // r.Term or change r.Vote. r.step = stepCandidate r.prs.ResetVotes() r.tick = r.tickElection r.lead = None r.state = StatePreCandidate r.logger.Infof(\"%x became pre-candidate at term %d\", r.id, r.Term) } func (r *raft) reset(term uint64) { if r.Term != term { r.Term = term r.Vote = None } r.lead = None r.electionElapsed = 0 r.heartbeatElapsed = 0 r.resetRandomizedElectionTimeout() r.abortLeaderTransfer() r.prs.ResetVotes() // ... ... } 预选举与选举的区别在主要在于预选举不会改变状态机的term也不会修改当前term的该节点投出的选票。下表列出了becomePreCandidate和becomeCandidate修改或未修改的与选举相关的重要字段： 重要字段 becomePreCandidate becomCandidate 描述 step stepCandidate stepCandidate step行为 tick tickElection tickElection tick行为 Vote mot modified current node.id 当前term将选票投给谁 state StatePreCandidate StateCandidate 状态机角色 lead None None 当前term的leader prs.Votes rest reset 收到的选票 无论是PreCandidate还是PreCandidate，其行为都是stepCandidate。其中，部分字段是通过reset函数修改的。reset方法用于状态机切换角色时初始化相关字段。因为切换到PreCandidate严格来说并不算真正地切换角色，因此becomePreCandidate中没有调用reset方法，而becomeCandidate、becomeLeader、becomeFollower都调用了reset方法。本文仅关注reset中与选举有关的部分，reset中还有一些与日志复制相关的逻辑，笔者会在后续的文章中分析。 接下来分析stepCandiate中与选举相关的逻辑： // stepCandidate is shared by StateCandidate and StatePreCandidate; the difference is // whether they respond to MsgVoteResp or MsgPreVoteResp. func stepCandidate(r *raft, m pb.Message) error { // Only handle vote responses corresponding to our candidacy (while in // StateCandidate, we may get stale MsgPreVoteResp messages in this term from // our pre-candidate state). var myVoteRespType pb.MessageType if r.state == StatePreCandidate { myVoteRespType = pb.MsgPreVoteResp } else { myVoteRespType = pb.MsgVoteResp } switch m.Type { case pb.MsgProp: r.logger.Infof(\"%x no leader at term %d; dropping proposal\", r.id, r.Term) return ErrProposalDropped case pb.MsgApp: r.becomeFollower(m.Term, m.From) // always m.Term == r.Term r.handleAppendEntries(m) case pb.MsgHeartbeat: r.becomeFollower(m.Term, m.From) // always m.Term == r.Term r.handleHeartbeat(m) case pb.MsgSnap: r.becomeFollower(m.Term, m.From) // always m.Term == r.Term r.handleSnapshot(m) case myVoteRespType: gr, rj, res := r.poll(m.From, m.Type, !m.Reject) r.logger.Infof(\"%x has received %d %s votes and %d vote rejections\", r.id, gr, m.Type, rj) switch res { case quorum.VoteWon: if r.state == StatePreCandidate { r.campaign(campaignElection) } else { r.becomeLeader() r.bcastAppend() } case quorum.VoteLost: // pb.MsgPreVoteResp contains future term of pre-candidate // m.Term \u003e r.Term; reuse r.Term r.becomeFollower(r.Term, None) } case pb.MsgTimeoutNow: r.logger.Debugf(\"%x [term %d state %v] ignored MsgTimeoutNow from %x\", r.id, r.Term, r.state, m.From) } return nil } 从如上代码中，可以看到PreCandidate或Candidate对不同种消息的处理方式： MsgProp、MsgTimeoutNow：丢弃。 MsgApp、MsgHeartbeat、MsgSnap：收到了来自leader的消息，转为follower。 相应地MsgPreVoteResp或MsgVoteResp：通过poll记录选票并获取当前选举状态。 在条件3中，当前节点在获取选举状态后，会根据不同的状态做出不同的处理： VotePending：暂无选举结果，不做处理。 VoteWon：赢得选举，如果当前状态机的角色是PreCandidate，那么调用campaign进行正式选举；如果当前状态机的角色是Candidate，那么当选leader，并向集群广播MsgAppend消息以通知集群中节点已有leader产生。 VoteLost：选举失败，变为follower。 提示 stepXXX中处理的消息都是已经在Step中预处理后的消息，有些消息可能被过滤掉了。其详细的逻辑见2.3节。 2.4.2 Leader leader中与选举相关逻辑的比重较少，这里简单介绍一下。 首先，是becomeLeader及其修改的选举相关的重要字段： func (r *raft)","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/3-election/:3:4","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x03 Raft选举","uri":"/posts/code-reading/etcdraft-made-simple/3-election/"},{"categories":["深入浅出etcd/raft"],"content":"3. 总结 本文首先介绍了etcd/raft实现的Raft选举优化，并介绍了使用选举优化后引入的新问题与解决方案，接着对etcd/raft中与选举有关的源码层层深入地分析。 由于选举是Raft算法中重要且复杂的部分，因此其代码分布比较零散。只想了解etcd/raft中对Raft算法做出的优化的读者可以只看本文的第一章。对于想要深入etcd/raft源码实现的读者，建议自己先按照自己的节奏阅读源码，对于不太理解的地方可以参考本文的分析，如果直接从本文的分析入手，可能会感觉有些绕。 ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/3-election/:4:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x03 Raft选举","uri":"/posts/code-reading/etcdraft-made-simple/3-election/"},{"categories":["深入浅出etcd/raft"],"content":"参考文献 [1] Ongaro D, Ousterhout J. In search of an understandable consensus algorithm[C]//2014 {USENIX} Annual Technical Conference ({USENIX}{ATC} 14). 2014: 305-319. [2] Ongaro D, Ousterhout J. In search of an understandable consensus algorithm (extended version)[J]. Retrieved July, 2016, 20: 2018. [3] Ongaro D. Consensus: Bridging theory and practice[D]. Stanford University, 2014. [4] Raft 笔记(四) – Leader election. 我叫尤加利（技术博客） ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/3-election/:5:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x03 Raft选举","uri":"/posts/code-reading/etcdraft-made-simple/3-election/"},{"categories":["深入浅出etcd/raft"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/2-overview/:0:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x02 etcd/raft总体设计","uri":"/posts/code-reading/etcdraft-made-simple/2-overview/"},{"categories":["深入浅出etcd/raft"],"content":"0. 引言 在《深入浅出etcd/raft —— 0x01 raftexample》中，我们通过对一个官方提供的基于etcd/raft实现的简单kvstore简单地介绍了etcd/raft的使用，以对etcd/raft有一个初步认识。想要深入分析etcd/raft中对Raft算法的实现与优化，首先，我们必须先要了解etcd/raft的总体设计。 etcd/raft将Raft算法的实现分为了3个模块：Raft状态机、存储模块、传输模块。 Raft状态机完全由etcd/raft负责，raft结构体即为其实现。使用etcd/raft的开发者不能直接操作raft结构体，只能通过etcd/raft提供的Node接口对其进行操作。 存储模块可以划分为两部分：对存储的读取与写入。etcd/raft只需要读取存储，etcd/raft依赖的Storage接口中只有读取存储的方法。而对存储的写入由用户负责，etcd/raft并不关心开发者如何写入存储，对存储的写入方法可以由开发者自己定义。etcd使用的存储模块是在与Storage接口同一文件下的MemoryStorage结构体。MemoryStorage既实现了Storage接口需要的读取存储的方法，也为用户提供了写入存储的方法。 说明 Storage接口定义的是稳定存储的读取方法。之所以etcd使用了基于内存的MemoryStorage，是因为etcd在写入MemoryStorage前，需要先写入预写日志（Write Ahead Log，WAL）或快照。而预写日志和快照是保存在稳定存储中的。这样，在每次重启时，etcd可以基于保存在稳定存储中的快照和预写日志恢复MemoryStorage的状态。也就是说，etcd的稳定存储是通过快照、预写日志、MemoryStorage三者共同实现的。 通信模块是完全由使用etcd/raft的开发者负责的。etcd/raft不关心开发者如何实现通信模块。 下图是一张关于etcd/raft的实现中，开发者与etcd/raft对这3个模块的职责的示意图。 etcd/raft职责示意图etcd/raft职责示意图 \" etcd/raft职责示意图 因为Node接口是开发者仅有的操作etcd/raft的方式，所以我们先来看看Node接口与其相关实现。 ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/2-overview/:1:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x02 etcd/raft总体设计","uri":"/posts/code-reading/etcdraft-made-simple/2-overview/"},{"categories":["深入浅出etcd/raft"],"content":"1. Node、node、rawnode Node接口为开发者提供了操作etcd/raft的方法。其接口定义如下： // Node represents a node in a raft cluster. type Node interface { // Tick increments the internal logical clock for the Node by a single tick. Election // timeouts and heartbeat timeouts are in units of ticks. Tick() // Campaign causes the Node to transition to candidate state and start campaigning to become leader. Campaign(ctx context.Context) error // Propose proposes that data be appended to the log. Note that proposals can be lost without // notice, therefore it is user's job to ensure proposal retries. Propose(ctx context.Context, data []byte) error // ProposeConfChange proposes a configuration change. Like any proposal, the // configuration change may be dropped with or without an error being // returned. In particular, configuration changes are dropped unless the // leader has certainty that there is no prior unapplied configuration // change in its log. // // The method accepts either a pb.ConfChange (deprecated) or pb.ConfChangeV2 // message. The latter allows arbitrary configuration changes via joint // consensus, notably including replacing a voter. Passing a ConfChangeV2 // message is only allowed if all Nodes participating in the cluster run a // version of this library aware of the V2 API. See pb.ConfChangeV2 for // usage details and semantics. ProposeConfChange(ctx context.Context, cc pb.ConfChangeI) error // Step advances the state machine using the given message. ctx.Err() will be returned, if any. Step(ctx context.Context, msg pb.Message) error // Ready returns a channel that returns the current point-in-time state. // Users of the Node must call Advance after retrieving the state returned by Ready. // // NOTE: No committed entries from the next Ready may be applied until all committed entries // and snapshots from the previous one have finished. Ready() \u003c-chan Ready // Advance notifies the Node that the application has saved progress up to the last Ready. // It prepares the node to return the next available Ready. // // The application should generally call Advance after it applies the entries in last Ready. // // However, as an optimization, the application may call Advance while it is applying the // commands. For example. when the last Ready contains a snapshot, the application might take // a long time to apply the snapshot data. To continue receiving Ready without blocking raft // progress, it can call Advance before finishing applying the last ready. Advance() // ApplyConfChange applies a config change (previously passed to // ProposeConfChange) to the node. This must be called whenever a config // change is observed in Ready.CommittedEntries, except when the app decides // to reject the configuration change (i.e. treats it as a noop instead), in // which case it must not be called. // // Returns an opaque non-nil ConfState protobuf which must be recorded in // snapshots. ApplyConfChange(cc pb.ConfChangeI) *pb.ConfState // TransferLeadership attempts to transfer leadership to the given transferee. TransferLeadership(ctx context.Context, lead, transferee uint64) // ReadIndex request a read state. The read state will be set in the ready. // Read state has a read index. Once the application advances further than the read // index, any linearizable read requests issued before the read request can be // processed safely. The read state will have the same rctx attached. ReadIndex(ctx context.Context, rctx []byte) error // Status returns the current status of the raft state machine. Status() Status // ReportUnreachable reports the given node is not reachable for the last send. ReportUnreachable(id uint64) // ReportSnapshot reports the status of the sent snapshot. The id is the raft ID of the follower // who is meant to receive the snapshot, and the status is SnapshotFinish or SnapshotFailure. // Calling ReportSnapshot with SnapshotFinish is a no-op. But, any failure in applying a // snapshot (for e.g., while streaming it from leader to follower), sho","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/2-overview/:2:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x02 etcd/raft总体设计","uri":"/posts/code-reading/etcdraft-made-simple/2-overview/"},{"categories":["深入浅出etcd/raft"],"content":"2. Raft状态机——raft etcd/raft的实现的优雅之处之一，在于其很好地剥离了各模块的职责。在etcd/raft的实现中，raft结构体是一个Raft状态机，其通过Step方法进行状态转移。只要涉及到Raft状态机的状态转移，最终都会通过Step方法完成。Step方法的参数是Raft消息（在etcd/raft/raftpb中，是直接通过.proto文件生成的Protocol Buffers的go语言实现）。 这里我们以Node接口的Tick方法为例。其实Tick方法并不是一个很好地例子，但是由于Tick方法比较特殊，后续文章中不会对其做额外的分析，因此这里就以该方法为例。 在rawnode的Tick方法实现中，其调用了raft结构体的tick“方法”。 // Tick advances the internal logical clock by a single tick. func (rn *RawNode) Tick() { rn.raft.tick() } 这里之所以给“方法”打上了引号，是因为tick其实并非一个真正的方法，而是raft的一个字段，其类型为一个无参无返回值的函数。 type raft struct { // ... ... tick func() // ... ... } 这样设计的原因，是leader和follower在tick被调用时的行为不同。tick字段可能的值有两个，分别为tickElection()和tickHeartbeat()，二者分别对应follower（或candidate、pre candidate）和leader的tick行为。我们可以在如下4个方法中找到相应的证据： func (r *raft) becomeFollower(term uint64, lead uint64) { // ... ... r.tick = r.tickElection // ... ... } func (r *raft) becomeCandidate() { // ... ... r.tick = r.tickElection // ... ... } func (r *raft) becomePreCandidate() { // ... ... r.tick = r.tickElection // ... ... } func (r *raft) becomeLeader() { // ... ... r.tick = r.tickHeartbeat // ... ... } 这里我们先以tickElection为例，分析其实如何将这一方法转为对Step方法的调用的。 // tickElection is run by followers and candidates after r.electionTimeout. func (r *raft) tickElection() { r.electionElapsed++ if r.promotable() \u0026\u0026 r.pastElectionTimeout() { r.electionElapsed = 0 r.Step(pb.Message{From: r.id, Type: pb.MsgHup}) } } 我们可以看到，tickElection方法会增大electionElapsed的值。当其超过了选举超时且当前节点可提拔为leader时（具体实现会在后续的文章中分析），重置其值，并创建一条MsgHup消息，传给Step方法。Step方法会对该消息进行处理，并适当地转移Raft状态机的状态。 raft结构体中的字段和相应的方法有很多。在后续的文章中，我们会在介绍etcd/raft中Raft算法的各部分实现时，介绍相应的字段与方法。这里仅给出创建node或rawnode时所需的Config结构体的结构，其大部分字段都与raft结构体中的有关字段相对应。 // Config contains the parameters to start a raft. type Config struct { // ID is the identity of the local raft. ID cannot be 0. ID uint64 // peers contains the IDs of all nodes (including self) in the raft cluster. It // should only be set when starting a new raft cluster. Restarting raft from // previous configuration will panic if peers is set. peer is private and only // used for testing right now. peers []uint64 // learners contains the IDs of all learner nodes (including self if the // local node is a learner) in the raft cluster. learners only receives // entries from the leader node. It does not vote or promote itself. learners []uint64 // ElectionTick is the number of Node.Tick invocations that must pass between // elections. That is, if a follower does not receive any message from the // leader of current term before ElectionTick has elapsed, it will become // candidate and start an election. ElectionTick must be greater than // HeartbeatTick. We suggest ElectionTick = 10 * HeartbeatTick to avoid // unnecessary leader switching. ElectionTick int // HeartbeatTick is the number of Node.Tick invocations that must pass between // heartbeats. That is, a leader sends heartbeat messages to maintain its // leadership every HeartbeatTick ticks. HeartbeatTick int // Storage is the storage for raft. raft generates entries and states to be // stored in storage. raft reads the persisted entries and states out of // Storage when it needs. raft reads out the previous state and configuration // out of storage when restarting. Storage Storage // Applied is the last applied index. It should only be set when restarting // raft. raft will not return entries to the application smaller or equal to // Applied. If Applied is unset when restarting, raft might return previous // applied entries. This is a very application dependent configuration. Applied uint64 // MaxSizePerMsg limits the max byte size of each append message. Smaller // value lowers the raft recovery cost(initial probing and message lost // during normal operation). On the other side, it might affect the // throughput during normal replication. Note: math.MaxUint64 for unlimited, // 0 for at most one entry per message. MaxSizePerMsg uint64 // MaxCommittedSizePerReady limits the size of the committed entries which // can be applied. MaxCommittedSizePer","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/2-overview/:3:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x02 etcd/raft总体设计","uri":"/posts/code-reading/etcdraft-made-simple/2-overview/"},{"categories":["深入浅出etcd/raft"],"content":"3. 总结 本文主要从顶层的视角，简单地分析了etcd/raft的总体设计。本文主要目的是给读者对etcd/raft的结构的整体认识，便于读者接下来学习etcd/raft中Raft算法的实现与优化。 ","date":"2020-12-17","objectID":"/posts/code-reading/etcdraft-made-simple/2-overview/:4:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x02 etcd/raft总体设计","uri":"/posts/code-reading/etcdraft-made-simple/2-overview/"},{"categories":["深入浅出etcd/raft"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/1-raftexample/:0:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x01 raftexample","uri":"/posts/code-reading/etcdraft-made-simple/1-raftexample/"},{"categories":["深入浅出etcd/raft"],"content":"0. 前言 在深入学习etcd中raft的源码之前，首先应该学会使用etcd的raft模块。幸运的是，etcd官方提供了一个基于etcd/raft的简单kvstore的实现，该实现在etcd/contrib/raftexample下。raftexample使用了raft模块的一些基本功能，实现了简单的分布式kv存储。该项目的根目录下还提供了该分布式kv存储示例的使用方法和基本设计思路，这里建议读者先按照其READ.md完整运行一遍示例，再继续接下来的学习。 ","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/1-raftexample/:1:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x01 raftexample","uri":"/posts/code-reading/etcdraft-made-simple/1-raftexample/"},{"categories":["深入浅出etcd/raft"],"content":"1. raftexample设计思路 该项目的README.md的Design一节简单介绍了其设计思路： 警告 注意，为了叙述的一致性，本文将“submit”或“issue”译为“提出”，以与raft协议中的“提交（commit）”进行区分。 引用 raftexample由三个组件组成：由raft支持的kv存储、REST API服务器、和基于etcd/raft实现的共识服务器。 由raft支持的kv存储是一个持有所有已提交的键值对的map。该存储建立了raft服务器和REST服务器间的通信桥梁。键值对的更新通过该存储提交给raft服务器。当raft服务器报告有更新被提交后，该存储便会更新其map。 REST服务器通过访问由raft支持的kv存储的方式暴露出当前raft达成的共识。 GET命令会在存储中查找键，如果键存在则会返回该键的值。 一个带键值的PUT命令会向存储提出一个更新提议。 raft服务器和其集群中的对等节点（peer）会参与共识的达成。 当REST服务器提交提议时，raft服务器会将该提议发送给其对等节点。 当raft达成共识时，服务器会通过一个提交信道来发布所有已提交的更新。 在raftexample中，提交信道的消费者是键值存储。 简单来说，raftexample的设计可以用一张图来表示。 raftexample设计图raftexample设计图 \" raftexample设计图 接下来，我们自下而上地学习raftexample的实现。 ","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/1-raftexample/:2:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x01 raftexample","uri":"/posts/code-reading/etcdraft-made-simple/1-raftexample/"},{"categories":["深入浅出etcd/raft"],"content":"2. httpapi httpapi.go是REST服务器的实现。这并不是我们关注的重点。我们需要关注的只是其通过kvstore中的哪些方法来提供服务。 func (h *httpKVAPI) ServeHTTP(w http.ResponseWriter, r *http.Request) { //... ... switch { case r.Method == \"PUT\": // ... ... h.store.Propose(key, string(v)) // ... ... case r.Method == \"GET\": if v, ok := h.store.Lookup(key); ok { // ... ... } // ... ... case r.Method == \"POST\": url, err := ioutil.ReadAll(r.Body) // ... ... cc := raftpb.ConfChange{ Type: raftpb.ConfChangeAddNode, NodeID: nodeId, Context: url, } h.confChangeC \u003c- cc // ... ... case r.Method == \"DELETE\": // ... ... cc := raftpb.ConfChange{ Type: raftpb.ConfChangeRemoveNode, NodeID: nodeId, } h.confChangeC \u003c- cc // ... ... default: // ... ... } } 上面的代码中，省略的一些对消息的序列化与反序列化及对性能的小优化，我们主要关注其通过哪些方法为请求提供服务。 请求方法 处理方式 功能 PUT kvstore.Propose(k,v) 更新键值对 GET kvstore.Lookup(k) 查找键对应的值 POST confChangeC \u003c- cc 将新节点加入集群 DELETE confChangeC \u003c- cc 从集群中移除节点 从表中我们可以看出，与键值对相关的请求都会通过kvstore提供的方法处理，而有关集群配置的请求则是会编码为etcd/raft/v3/raftpb中proto定义的消息格式，直接传入confChangeC信道。从main.go可以看出，该信道的消费者是raft模块，我们在介绍该模块时会详细介绍其作用。（因此，前文中给出的raftexample设计图并不准确，REST服务器会将有关集群配置变更的消息直接交给raft服务器处理。） ","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/1-raftexample/:3:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x01 raftexample","uri":"/posts/code-reading/etcdraft-made-simple/1-raftexample/"},{"categories":["深入浅出etcd/raft"],"content":"3. kvstore kvstore是连接raft服务器与REST服务器的桥梁，是实现键值存储功能的重要组件，但是其实现很简单。接下来，我们来看看kvstore.go中的实现。 kvstore.go中kvstore结构体非常简单，其只有4个字段。 // a key-value store backed by raft type kvstore struct { proposeC chan\u003c- string // channel for proposing updates mu sync.RWMutex kvStore map[string]string // current committed key-value pairs snapshotter *snap.Snapshotter } 其中，proposeC信道我们将在之后讲解。除此之外，该结构体中只有一个读写锁mu、一个由map实现的键值存储kvStore，和一个etcd提供的默认快照管理模块实现的指针snapshotter。 目前，从kvstore中，我们看不到多少有用的信息。接下来，我们关注一下创建kv存储的函数。 func newKVStore(snapshotter *snap.Snapshotter, proposeC chan\u003c- string, commitC \u003c-chan *string, errorC \u003c-chan error) *kvstore { s := \u0026kvstore{proposeC: proposeC, kvStore: make(map[string]string), snapshotter: snapshotter} // replay log into key-value map s.readCommits(commitC, errorC) // read commits from raft into kvStore map until error go s.readCommits(commitC, errorC) return s } newKVStore函数的参数除了snapshotter外，proposeC、commitC、errorC均为信道。其中propseC为输入信道，commitC和errorC为输出信道。我们可以推断出，kvstore会通过proposeC与raft模块交互，并通过commitC与errorC接收来自raft模块的消息。（可以在main.go中证实，这里不再赘述。）这种方式在etcd的实现中随处可见，因此对于go语言和channel不是很熟悉的小伙伴建议预先学习一下相关概念与使用方法。（当熟悉了这种设计后，便会发现go语言并发编程的魅力所在。） newKVStore中的逻辑也非常简单，将传入的参数写入kvstore结构体相应的字段中。然后先调用一次kvstore的readCommits方法，等待raft模块重放日志完成的信号；然后启动一个goroutine来循环处理来自raft模块发送过来的消息。 readCommits方法稍有些复杂，我们先来看看其它较为简单的部分。 func (s *kvstore) Lookup(key string) (string, bool) { s.mu.RLock() defer s.mu.RUnlock() v, ok := s.kvStore[key] return v, ok } func (s *kvstore) Propose(k string, v string) { var buf bytes.Buffer if err := gob.NewEncoder(\u0026buf).Encode(kv{k, v}); err != nil { log.Fatal(err) } s.proposeC \u003c- buf.String() } func (s *kvstore) getSnapshot() ([]byte, error) { s.mu.RLock() defer s.mu.RUnlock() return json.Marshal(s.kvStore) } func (s *kvstore) recoverFromSnapshot(snapshot []byte) error { var store map[string]string if err := json.Unmarshal(snapshot, \u0026store); err != nil { return err } s.mu.Lock() defer s.mu.Unlock() s.kvStore = store return nil } Lookup方法会通过读锁来访问其用来记录键值的map，防止查找时数据被修改返回错误的结果。Propose方法将要更新的键值对编码为string，并传入proposeC信道，交给raft模块处理。getSnapshot和recoverFromSnapshot方法分别将记录键值的map序列化与反序列化，并加锁防止争用。 func (s *kvstore) readCommits(commitC \u003c-chan *string, errorC \u003c-chan error) { for data := range commitC { if data == nil { // done replaying log; new data incoming // OR signaled to load snapshot snapshot, err := s.snapshotter.Load() if err == snap.ErrNoSnapshot { return } if err != nil { log.Panic(err) } log.Printf(\"loading snapshot at term %d and index %d\", snapshot.Metadata.Term, snapshot.Metadata.Index) if err := s.recoverFromSnapshot(snapshot.Data); err != nil { log.Panic(err) } continue } var dataKv kv dec := gob.NewDecoder(bytes.NewBufferString(*data)) if err := dec.Decode(\u0026dataKv); err != nil { log.Fatalf(\"raftexample: could not decode message (%v)\", err) } s.mu.Lock() s.kvStore[dataKv.Key] = dataKv.Val s.mu.Unlock() } if err, ok := \u003c-errorC; ok { log.Fatal(err) } } 接下来，我们深入readCommits的实现。该方法会循环遍历commitC信道中raft模块传来的消息。从commitC中收到的消息可能为nil或非nil。因为raftexample功能简单，因此其通过nil表示raft模块完成重放日志的信号或用来通知kvstore从上一个快照恢复的信号。 当data为nil时，该方法会通过kvstore的快照管理模块snapshotter尝试加载上一个快照。如果快照存在，说明这是通知其恢复快照的信号，接下来会调用recoverFromSnapshot方法从该快照中恢复，随后进入下一次循环，等待日志重放完成的信号；如果没找到快照，那么说明这是raft模块通知其日志重放完成的信号，因此直接返回。 警告 这段代码实际上有bug，结合raft.go中replayWAL的实现，可以发现当节点从已有快照的状态重启时，newKVStore中第一个readCommits循环无法正确跳出，导致其无法执行main函数中后续逻辑。截止2020-12-12T20:21:48+08:00时，该bug仍未修复。 为了便于读者理解raftexample其它部分的实现，在下文中不再分析该bug的产生原因。 当data非nil时，说明这是raft模块发布的已经通过共识提交了的键值对。此时，先从字节数组数据中反序列化出键值对，并加锁修改map中的键值对。 我们可以看到，kvstore中基本上没有多少与raft相关的处理逻辑，大部分代码是对键值存储抽象本身的实现。 ","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/1-raftexample/:4:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x01 raftexample","uri":"/posts/code-reading/etcdraft-made-simple/1-raftexample/"},{"categories":["深入浅出etcd/raft"],"content":"4. raft 最后，我们来学习raft服务器的实现，这也是raftexample中最重要也是最需要好好学习的部分。 raft服务器的实现在raft.go中，其对etcd/raft提供的接口进行了一层封装，以便于kvstore使用。 ","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/1-raftexample/:5:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x01 raftexample","uri":"/posts/code-reading/etcdraft-made-simple/1-raftexample/"},{"categories":["深入浅出etcd/raft"],"content":"4.1 raftNode结构体 在raftexample中，raft服务器被封装成了一个raftNode结构体。我们先来看看该结构体中的字段内容。 // A key-value stream backed by raft type raftNode struct { proposeC \u003c-chan string // proposed messages (k,v) confChangeC \u003c-chan raftpb.ConfChange // proposed cluster config changes commitC chan\u003c- *string // entries committed to log (k,v) errorC chan\u003c- error // errors from raft session id int // client ID for raft session peers []string // raft peer URLs join bool // node is joining an existing cluster waldir string // path to WAL directory snapdir string // path to snapshot directory getSnapshot func() ([]byte, error) lastIndex uint64 // index of log at start confState raftpb.ConfState snapshotIndex uint64 appliedIndex uint64 // raft backing for the commit/error channel node raft.Node raftStorage *raft.MemoryStorage wal *wal.WAL snapshotter *snap.Snapshotter snapshotterReady chan *snap.Snapshotter // signals when snapshotter is ready snapCount uint64 transport *rafthttp.Transport stopc chan struct{} // signals proposal channel closed httpstopc chan struct{} // signals http server to shutdown httpdonec chan struct{} // signals http server shutdown complete } 在结构体中，有4个用于与其它组件交互的信道： 信道 描述 proposeC \u003c-chan string 接收来自其它组件传入的需要通过raft达成共识的普通提议。 confChangeC \u003c-chan raftpb.ConfChange 接收来自其它组件的需要通过raft达成共识的集群变更提议。 commitC chan\u003c- *string 用来已通过raft达成共识的已提交的提议通知给其它组件的信道。 errorC chan\u003c- error 用来将错误报告给其它组件的信道。 结构体中，还有一些用来记录节点信息的字段： 字段 描述 id int 节点id，同样也作为raft会话中的client id。 peer []string 对等raft节点的url。 join bool 如果该节点是以加入已有集群的方式启动，那么该值为true；否则是false。 waldir string 预写日志（Write Ahead Log，WAL）的路径。 snapdir stirng 保存快照的目录路径 getSnapshot func()([]byte, error) 获取快照的方法签名。 lastIndex uint64 节点启动重启后会先恢复状态，其恢复到的状态的最后一条日志的索引。 confState raftpb.ConfState 集群配置状态（详见其声明）。 snapshotIndex uint64 快照中的状态下最后一条日志的索引。 appliedIndex uint64 已应用的最后一条日志的索引。 在结构体中，还保存了etcd/raft提供的接口与其所需的相关组件： 字段 描述 node raft.Node etcd/raft的核心接口，对于一个最简单的实现来说，开发者只需要与该接口打交道即可实现基于raft的服务。 raftStorage *raft.MemoryStorage 用来保存raft状态的接口，etcd/raft/storage.go中定义了etcd/raft模块所需的稳定存储接口，并提供了一个实现了该接口的内存存储MemoryStorage注1，raftexample中就使用了该实现。 wal *wal.WAL 预写日志实现，raftexample直接使用了etcd/wal模块中的实现。 snapshotter *snap.Snapshotter 快照管理器的指针 snapshotterReady chan *snap.Snapshotter 一个用来发送snapshotter加载完毕的信号的“一次性”信道。因为snapshotter的创建对于新建raftNode来说是一个异步的过程，因此需要通过该信道来通知创建者snapshotter已经加载完成。 snapCount uint64 当wal中的日志超过该值时，触发快照操作并压缩日志。 transport *rafthttp.Transport etcd/raft模块通信时使用的接口。同样，这里使用了基于http的默认实现。 注1 etcd/raft中的Storage接口和Transport接口让用户能够根据需求自定义稳定存储模块与通信模块。 使用Storage存储的数据需要被稳定存储，也就是说，即使服务器因掉电等问题关机，在服务器重启后其也能够恢复到掉电前的最终状态。有些读者可能会有疑惑，这里的raftexample使用的是MemoryStorage，而内存是易失存储，为什么可以当做稳定存储使用？这是因为在raftexample的实现中，每次重启时都会通过快照和预写日志恢复MemoryStorage，而快照和预写日志是保存在稳定存储上的。这样，通过快照、预写日志、MemoryStorage的结合，可以实现稳定存储。这样做的好处之一是，预写日志是仅追加（Append-Only）的且快照写入的是连续的空间，这样可以减少对稳定存储的随机写入，提高系统吞吐量。 此外，在结构体中，还有一些通过chan struct{}信道实现的信号量： 信号 描述 stopc 提议信道关闭信号 httpstopc 通知用于raft通信的http服务器关闭的信号 httpdonec 用于raft通信的http服务器关闭的信号 为了方便不熟悉go语言并发编程或channel的读者理解，下面将简单介绍这种信号量的设计与意义。 ","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/1-raftexample/:5:1","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x01 raftexample","uri":"/posts/code-reading/etcdraft-made-simple/1-raftexample/"},{"categories":["深入浅出etcd/raft"],"content":"4.2 go语言中将struct{}信道用作信号的tips 这里为那些对go语言并发编程或channel使用不熟悉的读者简单介绍一下这种信号设计的使用方式。 首先，我们需要了解的是，在go语言中，无缓冲信道的读写是阻塞的。只有当在不同goroutine的生产者和消费者分别通过信道写入和读取时，它们所在的goroutine才能继续执行： c := make(chan int) go func(){ c \u003c- 1 // block until there is a consumer }() go func(){ \u003c- c // block until there is a producer } 然而，在读取被关闭的信道时，该信道会立即返回空值（如果采用两个变量接收信道的值，第二个布尔变量的值为false）。 close(c) msg, ok := \u003c- c // return immediately, msg is a empty value, ok is false 假设我们需要循环等待来自一个或多个无缓冲信道的消息，且需要在想要关闭程序时能够退出循环。如果我们通过基于一个标识符的方式判断是否需要退出循环，我们需要这样编写代码： // flag : false -\u003e break loop for { if !flag { break } select { case \u003c- c1: // ... ... case \u003c- c2: // ... ... } } 这种写法实际上是有问题的。在每次循环中，当if判断后会执行select，此时当前goroutine会阻塞，直到有一个信道可以操作。假如在当前goroutine进入select时将flag置为了false，此时，该goroutine无法立刻检测到flag的变化，因此需要等到下一次循环后才能退出循环。 显然，这不是我们想要的行为。我们希望当我们想要关闭时，如果该goroutine正阻塞在select中时，能够立即退出。此时，我们只需要通过一个chan struct{}信道来表示该信号接收器。上面这段代码可以改写为： // doneC : chan struct{} for { select { case \u003c- doneC: break case \u003c- c1: // ... ... case \u003c- c2: // ... ... } } 此时，如想要通知该goroutine退出循环，只需要关闭doneC。在前文中我们提到，当读取已关闭的信道时，其会立即返回一个空值。因此，即使当前的goroutine阻塞在select中，一旦doneC被关闭，该select会进入case \u003c- doneC的分支中，退出循环。 在raft.go中和etcd中，都有很多这种传递信号的方式。这一方式的另一个好处是，只要关闭一次信道，所有等待该信号的select语句都会进入该信号对应的分支。我们会在后文的例子中看到将chan struct{}信道作为信号使用的例子。 ","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/1-raftexample/:5:2","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x01 raftexample","uri":"/posts/code-reading/etcdraft-made-simple/1-raftexample/"},{"categories":["深入浅出etcd/raft"],"content":"4.3 raftNode的创建与启动 在创建raftNode时，需要提供节点id、对等节点urlpeers、是否是要加入已存在的集群join、获取快照的函数签名getSnapshot、提议信道proposeC、配置变更提议信道confChangeC这些参数。 // newRaftNode initiates a raft instance and returns a committed log entry // channel and error channel. Proposals for log updates are sent over the // provided the proposal channel. All log entries are replayed over the // commit channel, followed by a nil message (to indicate the channel is // current), then new log entries. To shutdown, close proposeC and read errorC. func newRaftNode(id int, peers []string, join bool, getSnapshot func() ([]byte, error), proposeC \u003c-chan string, confChangeC \u003c-chan raftpb.ConfChange) (\u003c-chan *string, \u003c-chan error, \u003c-chan *snap.Snapshotter) { commitC := make(chan *string) errorC := make(chan error) rc := \u0026raftNode{ proposeC: proposeC, confChangeC: confChangeC, commitC: commitC, errorC: errorC, id: id, peers: peers, join: join, waldir: fmt.Sprintf(\"raftexample-%d\", id), snapdir: fmt.Sprintf(\"raftexample-%d-snap\", id), getSnapshot: getSnapshot, snapCount: defaultSnapshotCount, stopc: make(chan struct{}), httpstopc: make(chan struct{}), httpdonec: make(chan struct{}), snapshotterReady: make(chan *snap.Snapshotter, 1), // rest of structure populated after WAL replay } go rc.startRaft() return commitC, errorC, rc.snapshotterReady } 在newRaftNode函数中，仅初始化了raftNode的部分参数，其余的参数会在重放预写日志后配置。随后，该函数启动了一个协程，该协程调用了raftNode的startRaft()方法来启动raft节点。当前函数会将raft模块用来通知已提交的提议的信道、报错信道、和快照管理器加载完成信号的信道返回给调用者。 接下来，我们跟随该方法进入raftNode.startRaft()方法中。 func (rc *raftNode) startRaft() { if !fileutil.Exist(rc.snapdir) { if err := os.Mkdir(rc.snapdir, 0750); err != nil { log.Fatalf(\"raftexample: cannot create dir for snapshot (%v)\", err) } } rc.snapshotter = snap.New(zap.NewExample(), rc.snapdir) rc.snapshotterReady \u003c- rc.snapshotter oldwal := wal.Exist(rc.waldir) rc.wal = rc.replayWAL() rpeers := make([]raft.Peer, len(rc.peers)) for i := range rpeers { rpeers[i] = raft.Peer{ID: uint64(i + 1)} } c := \u0026raft.Config{ ID: uint64(rc.id), ElectionTick: 10, HeartbeatTick: 1, Storage: rc.raftStorage, MaxSizePerMsg: 1024 * 1024, MaxInflightMsgs: 256, MaxUncommittedEntriesSize: 1 \u003c\u003c 30, } if oldwal { rc.node = raft.RestartNode(c) } else { startPeers := rpeers if rc.join { startPeers = nil } rc.node = raft.StartNode(c, startPeers) } rc.transport = \u0026rafthttp.Transport{ Logger: zap.NewExample(), ID: types.ID(rc.id), ClusterID: 0x1000, Raft: rc, ServerStats: stats.NewServerStats(\"\", \"\"), LeaderStats: stats.NewLeaderStats(zap.NewExample(), strconv.Itoa(rc.id)), ErrorC: make(chan error), } rc.transport.Start() for i := range rc.peers { if i+1 != rc.id { rc.transport.AddPeer(types.ID(i+1), []string{rc.peers[i]}) } } go rc.serveRaft() go rc.serveChannels() } startRaft方法虽然看上去很长，但是实现的功能很简单。 首先，startRaft方法检查快照目录是否存在，如果不存在为其创建目录。然后创建基于该目录的快照管理器。创建完成后，向snapshotterReady信道写入该快照管理器，通知其快照管理器已经创建完成。 接着，程序检查是否有旧的预写日志存在，并重放旧的预写日志，重放代码在下文中会进一步分析。 在重放完成后，程序设置了etcd/raft模块所需的配置，并从该配置上启动或重启节点（取决于有没有旧的预写日志文件）。etcd/raft中的raft.StartNode和raft.RestartNode函数分别会根据配置启动或重启raft服务器节点，并返回一个Node接口的实例。正如前文中提到的，Node接口是开发者依赖etcd/raft实现时唯一需要与其打交道的接口。程序将Node接口的实例记录在了raftNode的node字段中。 在node创建完成后，程序配置并开启了通信模块，开始与集群中的其它raft节点通信。 在一切接续后，程序启动了两个goroutine，分别是raftNode.serveRaft()和raftNode.serveChannels()。其中raftNode.serveRaft()用来监听来自其它raft节点的消息，消息的处理主要在Transport接口的实现中编写，因此在这里不对其进行详细的分析，感兴趣的读者可以自行参考源码中的实现；raftNode.serveChannels()用来处理raftNode中各种信道，后文会对其进行详细分析。 下面，我们先来分析一下重放预写日志的逻辑。 // replayWAL replays WAL entries into the raft instance. func (rc *raftNode) replayWAL() *wal.WAL { log.Printf(\"replaying WAL of member %d\", rc.id) snapshot := rc.loadSnapshot() w := rc.openWAL(snapshot) _, st, ents, err := w.ReadAll() if err != nil { log.Fatalf(\"raftexample: failed to read WAL (%v)\", err) } rc.raftStorage = raft.NewMemoryStorage() if snapshot != nil { rc.raftStorage.ApplySnapshot(*snapshot) } rc.raftStorage.SetHardState(st) // append to storage so raft starts at the right place in log rc.raftStorage.Append(ents) // send nil once lastIndex is published so client knows c","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/1-raftexample/:5:3","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x01 raftexample","uri":"/posts/code-reading/etcdraft-made-simple/1-raftexample/"},{"categories":["深入浅出etcd/raft"],"content":"4.4 信道的处理 raftNode.serveChannels()是raft服务器用来处理各种信道的输入输出的方法，也是与etcd/raft模块中Node接口的实现交互的方法。 serverChannels()方法可以分为两个部分，该方法本身会循环处理raft有关的逻辑，如处理定时器信号驱动Node、处理Node传入的Ready结构体、处理通信模块报告的错误或停止信号灯等；该方法还启动了一个goroutine，该goroutine中循环处理来自proposeC和confChangeC两个信道的消息。 在这两部分开始前，该方法先做了一些初始化。 func (rc *raftNode) serveChannels() { snap, err := rc.raftStorage.Snapshot() if err != nil { panic(err) } rc.confState = snap.Metadata.ConfState rc.snapshotIndex = snap.Metadata.Index rc.appliedIndex = snap.Metadata.Index defer rc.wal.Close() ticker := time.NewTicker(100 * time.Millisecond) defer ticker.Stop() // ... ... } 首先，该方法从当前的快照的元数据设置raftNode的相关字段，并设置一个每100毫秒产生一个信号的循环定时器。serveChannels的循环会根据这个信号调用Node接口的Tick()方法，驱动Node执行。 接下来，我们先来看serveChannels中启动的用来处理来自proposeC和confChangeC两个信道的消息的goroutine。 func (rc *raftNode) serveChannels() { // ... ... // send proposals over raft go func() { confChangeCount := uint64(0) for rc.proposeC != nil \u0026\u0026 rc.confChangeC != nil { select { case prop, ok := \u003c-rc.proposeC: if !ok { rc.proposeC = nil } else { // blocks until accepted by raft state machine rc.node.Propose(context.TODO(), []byte(prop)) } case cc, ok := \u003c-rc.confChangeC: if !ok { rc.confChangeC = nil } else { confChangeCount++ cc.ID = confChangeCount rc.node.ProposeConfChange(context.TODO(), cc) } } } // client closed channel; shutdown raft if not already close(rc.stopc) }() // ... ... } 这部分逻辑很简单。因为在循环中，如果proposeC或confChangeC中的一个被关闭，程序会将其置为nil，所以只有二者均不是nil时才执行循环。每次循环会通过select选取一个有消息传入的信道，通过Node接口提交给raft服务器。当循环结束后，关闭stopc信道，即发送关闭信号。这种方式在4.2-go语言中将struct{}信道用作信号的tips中介绍过。 serveChannels中的循环是与Node接口交互的重要逻辑。 func (rc *raftNode) serveChannels() { // ... ... // event loop on raft state machine updates for { select { case \u003c-ticker.C: rc.node.Tick() // store raft entries to wal, then publish over commit channel case rd := \u003c-rc.node.Ready(): rc.wal.Save(rd.HardState, rd.Entries) if !raft.IsEmptySnap(rd.Snapshot) { rc.saveSnap(rd.Snapshot) rc.raftStorage.ApplySnapshot(rd.Snapshot) rc.publishSnapshot(rd.Snapshot) } rc.raftStorage.Append(rd.Entries) rc.transport.Send(rd.Messages) if ok := rc.publishEntries(rc.entriesToApply(rd.CommittedEntries)); !ok { rc.stop() return } rc.maybeTriggerSnapshot() rc.node.Advance() case err := \u003c-rc.transport.ErrorC: rc.writeError(err) return case \u003c-rc.stopc: rc.stop() return } } // ... ... } 该循环同时监听4个信道： 循环定时器的信道，每次收到信号后，调用Node接口的Tick函数驱动Node。 Node.Ready()返回的信道，每当Node准备好一批数据后，会将数据通过该信道发布。开发者需要对该信道收到的Ready结构体中的各字段进行处理。在处理完成一批数据后，开发者还需要调用Node.Advance()告知Node这批数据已处理完成，可以继续传入下一批数据。 通信模块报错信道，收到来自该信道的错误后raftNode会继续上报该错误，并关闭节点。 用来表示停止信号的信道，当该信道被关闭时，阻塞的逻辑会从该分支运行，关闭节点。 其中，Node.Ready()返回的信道逻辑最为复杂。因为其需要处理raft状态机传入的各种数据，并交付给相应的模块处理。etcd/raft的Ready结构体中包含如下数据： // Ready encapsulates the entries and messages that are ready to read, // be saved to stable storage, committed or sent to other peers. // All fields in Ready are read-only. type Ready struct { // The current volatile state of a Node. // SoftState will be nil if there is no update. // It is not required to consume or store SoftState. *SoftState // The current state of a Node to be saved to stable storage BEFORE // Messages are sent. // HardState will be equal to empty state if there is no update. pb.HardState // ReadStates can be used for node to serve linearizable read requests locally // when its applied index is greater than the index in ReadState. // Note that the readState will be returned when raft receives msgReadIndex. // The returned is only valid for the request that requested to read. ReadStates []ReadState // Entries specifies entries to be saved to stable storage BEFORE // Messages are sent. Entries []pb.Entry // Snapshot specifies the snapshot to be saved to stable storage. Snapshot pb.Snapshot // CommittedEntries specifies entries to be committed to a // store/state-machine. These have previously been committed to stable // store. CommittedEntries []pb.Entry // Messages specifies outbound messages to be sent AFTER Entries are // committed to stable storage. // If it contains a MsgSnap me","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/1-raftexample/:5:4","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x01 raftexample","uri":"/posts/code-reading/etcdraft-made-simple/1-raftexample/"},{"categories":["深入浅出etcd/raft"],"content":"5. 总结 至此，我们已经分析了raftexample大部分的主要逻辑。在main.go中有raftexample中各模块的创建于初始化逻辑、raft.go中还有一些如关闭服务器的逻辑。 raftexample是官方提供的使用了etcd/raft的最基本的功能的简单的kv存储的示例。通过分析学习这段代码，可以简单了解etcd/raft的基本使用方式。当然，etcd/raft还提供了很多高级功能，我们会在后续的文章中介绍与分析。 技巧 在介绍kvstore时，我们提到了在当前版本的raftexample中有一个与快照相关的bug。在学习了完整的raftexample的例子后，可以尝试用自己的方式修复这个bug。（一种最简单的方式只需要3行代码就能修复这一bug，虽然那种方式逻辑很丑。） ","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/1-raftexample/:6:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x01 raftexample","uri":"/posts/code-reading/etcdraft-made-simple/1-raftexample/"},{"categories":["深入浅出etcd/raft"],"content":"本文为原创文章，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/0-introduction/:0:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x00 引言","uri":"/posts/code-reading/etcdraft-made-simple/0-introduction/"},{"categories":["深入浅出etcd/raft"],"content":"1. 引言 Raft算法，Diego Ongaro在《In search of an understandable consensus algorithm》中提出的一种新型故障容错共识算法。正如这篇论文的标题所说，Raft算法比经典的Paxos算法族更容易理解。 然而，即使读过Raft的论文、做过MIT6.824的Lab2，也很难理解成熟的工业级产品中Raft实现的一些细节。本系列文章旨在由浅入深地分析Etcd中Raft算法的实现，从Raft论文中的实现过渡到成熟的工业级产品中的经典Raft实现。 在阅读本系列文章前，读者需要： 阅读《In search of an understandable consensus algorithm (extended version)》，理解其中有关Raft的内容，本系列不会赘述Raft的一些基本概念。 学习go的基本语法，学习go语言并发编程与channel的使用方式。 准备Diego Ongaro的博士论文作为参考资料，在Etcd的实现中，引用了很多其中的优化方式。 本系列文章龟速更新。笔者也是第一次试图将对这种工业级产品的分析写出来分享给读者，因此难免把握不好分析的粒度。在更新后面的文章的同时，我也会对之前的文章进行更正与优化，使其更容易理解。 另外，本系列不会对引用的代码中的注释进行翻译，其原因有二：一来，etcd/raft模块中的注释描述的十分详细，建议读者要详细地阅读一遍etcd/raft模块中所有的注释；二来，笔者的水平有限，翻译的过程中难免会有词不达意的情况，而etcd/raft模块中的注释往往会提及很多细节，为了避免误导读者，就不做翻译了。不过相信能看到这里的读者都有丰富的英文论文阅读经验了，不需要笔者多此一举的翻译。 ","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/0-introduction/:1:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x00 引言","uri":"/posts/code-reading/etcdraft-made-simple/0-introduction/"},{"categories":["深入浅出etcd/raft"],"content":"2. 目录 深入浅出etcd/raft —— 0x00 引言 深入浅出etcd/raft —— 0x01 raftexample 深入浅出etcd/raft —— 0x02 etcd/raft总体设计 深入浅出etcd/raft —— 0x03 Raft选举 深入浅出etcd/raft —— 0x04 Raft日志 深入浅出etcd/raft —— 0x05 Raft成员变更 深入浅出etcd/raft —— 0x06 只读请求优化 ","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/0-introduction/:2:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x00 引言","uri":"/posts/code-reading/etcdraft-made-simple/0-introduction/"},{"categories":["深入浅出etcd/raft"],"content":"3. 施工路线图 本系列仍在施工中，之后可能反复修改其中内容与顺序等。本节中保存了当前的施工路线图。 引言 raftexample分析 etcd/raft整体架构与状态机简要分析 选举 —— etcd/raft中选举优化 选举 —— etcd/raft实现分析 日志 —— etcd/raft中日志实现 日志 —— etcd/raft中日志复制 日志 —— etcd/raft中快照 集群变更 —— simple 集群变更 —— joint Linearizable Read —— Log Read、ReadIndex、Lease Read 集群变更 —— joint（apply-time confchange修复issue#12359） 附录 —— etcd/raft中所有消息使用的字段描述（80%鸽了） ","date":"2020-12-10","objectID":"/posts/code-reading/etcdraft-made-simple/0-introduction/:3:0","tags":["etcd","Raft"],"title":"深入浅出etcd/raft —— 0x00 引言","uri":"/posts/code-reading/etcdraft-made-simple/0-introduction/"},{"categories":["Paper Reading"],"content":"本篇文章是对论文Spanner: Google’s Globally-Distributed Database的原创翻译，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:0:0","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"作者 James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost, JJ Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser, Peter Hochschild, Wilson Hsieh,Sebastian Kanthak, Eugene Kogan, Hongyi Li, Alexander Lloyd, Sergey Melnik, David Mwaura, David Nagle, Sean Quinlan, Rajesh Rao, Lindsay Rolig, Yasushi Saito, Michal Szymaniak, Christopher Taylor, Ruth Wang, Dale Woodford Google, Inc. ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:1:0","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"摘要 Spanner是Google的可伸缩（scalable）、多版本（multi-version）、全球化分布式（globally-distributed）、同步多副本（synchronously-replicated）数据库。它是首个能将数据分布到全球范围内且支持外部一致性（externally-consistent）分布式事务的系统。本文描述了Spanner的结构、特性、多种底层设计决策的原理、和一种暴露时钟不确定度（uncertainty）的新型时间API。该API和它的实现对支持外部一致性和多种强大的特性来说非常重要，这些特性包括：对过去数据的非阻塞读取、无锁只读事务、和原子性模型（schema）修改，所有的这写操作都是跨所有Spanner的。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:2:0","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"1. 引言 Spanner是一个可伸缩、全球化分布的数据库，其由Google设计、构建、并部署。在抽象的最高层，Spanner是一个将数据分片（shard）到分布在全世界的多个数据中心中的跨多个Paxos[21]状态机集合上的数据库。Spanner采用多副本以提供全球化的可用性和地理位置优化；客户端自动地在副本间进行故障转移。在数据总量或服务器的数量变化时，Spanner自动地在机器间重分片数据，并自动地在机器间（甚至在数据中心间）迁移数据来平衡负载和应对故障。按照设计，Spanner扩展到跨数百个数据中心的数百万台机器与数万亿个数据库行。 应用程序可以使用Spanner来实现高可用，即使在面对大范围自然灾害时，Spanner也可以通过在大洲内甚至跨大洲间备份数据。我们最初的使用者是F1[35]，F1是对Google的广告后端的重写。F1使用了5份分布在美国各地的副本。大部分的其它的应用程序可能在同一个地理区域但故障模式相对独立的3到5个数据中心中备份数据。也就是说，大多数应用程序选择了低延迟而不是高可用，只要它们能够容忍1或2个数据中心故障即可。 Spanner的主要目标是管理跨数中心的副本数据，但是我们还花了很多时间设计并实现了在我们的分布式系统基础架构之上的重要的数据库特性。尽管Bigtable[9]能够很好地满足很多项目的需求，但我们还是不断收到用户的抱怨，他们表示Bigtable对一些类型的应用程序来说难以使用：如那些有复杂、不断演进的模型的程序或那些想要在广域副本中维护强一致性的程序。（其他作者也提出了类似的主张[37]。）Google的许多应用程序选择使用Megastore[5]，因为它支持半结构化数据模型和副本同步，尽管它的写入吞吐量相对较弱。为此，Spanner从一个类似Bigtable的版本化键值存储（versioned key-value store）演进成了一个多版本时态数据库（temporal multi-version database）。数据被存储在模型化的半关系型表中；数据被版本化，且每个版本自动按照提交时间标记时间戳；旧版本遵循可配置的垃圾回收策略；应用程序可以读取时间戳较老的数据。Spanner支持通用的事务，且提供了基于SQL的查询语言。 作为全球化分布的数据库，Spanner提供了许多有趣的特性。第一，应用程序可以细粒度地动态控制数据的副本配置。应用程序可以通过指定约束来控制那个数据中心包含哪些数据、数据离它的用户多远（以控制读取延迟）、副本间多远（以控制写入延迟）、维护了多少份副本（以控制持久性、可用性、和读取性能）。数据还能被系统动态、透明地在数据中心间迁移以平衡数据中心间的资源使用率。第二，Spanner有两个在分布式数据库中难以实现的两个特性：Spanner提供了外部一致性（externally-consistent）[16]读写和对某个时间戳上的跨数据库全局一致性读取。这些特性让Spanner能支持一致性备份（consistent backup）、一致性MapReduce执行[12]和原子性模型更新，这些操作都是全局的，甚至可以出现在正在执行的事务中。 这些特性有效的原因在于，Spanner会为事务分配在全局有效的提交时间戳，尽管事务可能是分布式的。该时间戳反映了串行顺序。另外，串行顺序满足外部一致性（或等价的线性一致性[20]）：如果事务$T_1$在另一个事务$T_2$开始前提交，那么$T_1$的时间戳比$T_2$的小。Spanner是首个能在全球范围提供这些保证的系统。 实现这些属性的关键是一个新的TrueTime API及其实现。该API直接暴露了时钟不确定度，且对Spanner的时间戳的保证基于该API的实现提供的界限内。如果不确定度较大，Spanner会减速以等待该不确定度。Google的集群管理软件提供了TureTime API的一种实现。该实现通过使用多种现代参考时钟（GPS和原子时钟）来让不确定度保持较小（通常小于10ms）。 第二章描述了Spanner实现的结构、它的特定集合、和融入到了设计中的工程决策。第三章描述了我们的新TureTime API并概述了其实现。第四章描述了Spanner如何使用TrueTime来实现具有外部一致性的分布式事务、无锁只读事务、和原子性模型更新。第五章提供了Spanner性能和TrueTime表现的一些benchmark，并讨论了F1的经验。第六、七、八章，描述了相关工作和展望，并总结了我们的结论。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:3:0","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"2. 实现 本章描述了Spanner的结构和Spanner底层实现的原理。然后，我们描述了dircetory（目录） 抽象，directory用来管理副本和局部性（locality），它还是数据移动的单位。最后，我们描述了我们的数据模型、为什么Spanner看上去像关系型数据库而不是键值存储、怎样能让应用程序控制数据的局部性。 一份Spanner的部署被称为一个universe。因为Spanner在全球范围管理数据，所以只有少数的几个运行中的universe。我们目前运行了一个测试/练习场universe、一个开发/生产universe、和一个仅生产的universe。 Spanner被组织为zone的集合，每个zone都大致类似于一份Bigtable服务器集群[9]的部署。zone是部署管理的单位。zone的集合还是数据副本能够跨位置分布的位置集合。当有新的数据中心加入服务或旧的数据中心被关闭时，zone可以加入运行中的系统或从运行中的系统移除。zone也是物理隔离的单位：例如，如果不同的应用程序的数据必须跨同数据中心的不同的服务器的集合分区时，那么在一个数据中心中可能有一个或多个zone。 图1 spanner服务器组织结构。图1 spanner服务器组织结构。 \" 图1 spanner服务器组织结构。 图1描述了Sppanner universe中的服务器。一个zone有一个zonemaster和几百到几千个spanserver。前者为spannerserver分配数据，后者向客户端提供数据服务。客户端使用每个zone的location proxy来定位给它分配的为其提供数据服务的spanserver。universe master和placement driver目前是单例。universe master主要是一个控制台，其显示了所有zone的状态信息，以用来交互式调试。placement driver分钟级地处理zone间的自动化迁移。placement driver定期与spanserver交互来查找需要移动的数据，以满足更新后的副本约束或进行负载均衡。出于篇幅的原因，我们仅详细描述spanserver。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:4:0","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"2.1 spanserver软件栈 本节着眼于spanserver的实现以阐述副本和分布式事务如何被分层到我们的基于Bigtable的实现中。软件栈如图2所示。在最底层，每个spanserver负责100到1000个被称为tablet的数据结构实例。每个tablet都类似于Bigtable的tablet抽象，其实现了一系列如下的映射： $$ (key:string, timestamp:int64) \\rightarrow string $$ 图2 spanserver软件栈。图2 spanserver软件栈。 \" 图2 spanserver软件栈。 不像Bigtable，Spannner为数据分配时间戳，这是Spanner更像多版本数据库而不是键值存储的重要原因之一。tablet的状态被保存在一系列类B树的文件和一个预写日志（write-ahead log，WAL）中，它们都在一个被称为Colossus的分布式文件系统中（Google File System[15]的继任者）。 为了支持副本，每个spanserver在每个tablet上实现了一个Paxos状态机。（早期的Spanner原型支持为每个tablet实现多个Paxos状态机，这让副本配置更加灵活。但是其复杂性让我们放弃了它。）每个状态机在它相关的tablet中保存其元数据和日志。我们的Paxos实现通过基于定时的leader租约（lease）来支持长期领导者，租约的默认长度为10秒。目前，在Spanner的实现中，每次Paxos write会被记录两次：一次在tablet的日志中，一次在Paxos的日志中。这种选择是权宜之策，我们最终很可能会改进这一点。我们的Paxos实现是流水线化的，以在有WAN延迟的情况下提高Spanner的吞吐量；但是Paxos会按顺序应用写入（第四章会依赖这一点）。 在实现一致性的多副本映射的集合时，使用了Paxos状态机。每个副本的键值映射状态被保存在其对应的tablet中。写操作必须在leader处启动Paxos协议；读操作直接从任意足够新的副本处访问其底层tablet的状态。副本的集合是一个Paxos group。 在每个spanserver的每个leader副本中，都实现了一个lock table来实现并发控制。lock table包括2阶段锁（two-phase lock）状态：它将键的区间映射到锁状态。（值得注意的是，长期Paxos leader对高效管理lock table来说十分重要。）在Bigtable和Spanner中，lock table都是为长期事务设计的（例如报告生成，其可能需要几分钟的时间），它在存在冲突的乐观并发控制协议下表现不佳。需要获取同步的操作（如事务性读取）会在lock table中请求锁；其它的操作会绕过lock table。 在每个spanserver的每个leader副本中，还实现了一个transaction manager来提供分布式事务支持。实现participant leader时使用了transaction manager；group中的其它副本称为participant slave。如果事务仅有一个Paxos group参与（大多数事务都是这种情况），它可以绕过transaction manager，因为lock table和Paxos在一起能够提供事务性。如果事务有超过一个Paxos group参与，那些group的leader会相互配合执行两阶段提交（two-phase commit，2PC）。参与的group之一会被选为coordinator：该group的participant leader会作为coordinator leader，该group的salve会作为coordinator slave。每个transaction manager的状态会被保存在底层Paxos group中（因此它也是多副本的）。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:4:1","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"2.2 directory与放置 在键值映射集合的上层，Spanner的实现支持一种被称为directory（目录）的bucket（桶）抽象，它是一系列共享相同的前缀（prefix）的连续的键的集合。（术语directory的选择处于历史上的偶然，更好的术语可能是bucket。）我们将在章节2.3中解释前缀的来源。对directory的支持让应用程序能够通过谨慎地选取键以控制它们的数据的局部性。 directory是数据放置（placement）的单位。在同一个directory的所有数据都有相同的副本配置。当数据在Paxos group间移动时，它是以directory为单位移动，如图3所示。Spanner可能会为分流Paxos group的负载而移动directory、可能为了把经常被一起访问的directory放在同一个group中而移动directory、或为了使directory靠近其访问者而移动directory。directory可以在客户端操作正在运行时移动。50MB的directory的移动期望在几秒内完成。 图3 directory是Paxos group间数据移动的单位。图3 directory是Paxos group间数据移动的单位。 \" 图3 directory是Paxos group间数据移动的单位。 一个Paxos group可能包含多个directory，这意味着Spanner的tablet与Bigtable的tablet不同：Spanner的tablet并非必须是行空间上按字典序连续的分区。Spanner的tablet是一个装有多个行空间的分区的容器。因为这样做可以一起定位多个经常被一起访问的directory，所以我们做了这样的决策。 movedir是用来在Paxos group间移动directory的后台任务[14]。movedir也被用作为Paxos group添加或移除副本[25]，因为Spanner目前不支持Paxos内的配置修改。movedir没被实现为单个事务，这样可以避免阻塞大量数据移动时进行的读写。取而代之的是，moveidr会在开始移动数据时注册该事件，并在后台移动数据。当它已经移动完几乎所有数据时，它会启动一个事务来原子性地移动剩余的少量数据，并更新两个Paxos group的元数据。 directory也是应用程序能够指定副本地理属性（geographic-replication property，或者简称”放置“，placement）的最小单位。我们的放置专用语言（placement-specification language）分离了管理副本配置的职责。管理员能控制两个维度：副本的数量和类型、和副本的地理上的放置。管理员会在这两个维度上创建由命名选项组成的菜单（例如，北美，5路副本与1个witness）。应用程序通过通过给每个数据库和（或）每个独立的directory打上一个由这些选项组合而成的标签来控制数据副本策略。例如，应用程序可能将每个终端用户的数据保存在各自的directory中，这让用户A的数据能在欧洲有3个副本，并让用户B的数据能在北美有5个副本。 为了让我们的描述简介，我们对其做了简化。事实上，如果directory增长得过大，Spanner会将其分片成多个fragment（段）。fragment可能由不同的Paxos group提供服务（即，由不同的服务器提供）。事实上，movedir在group之间移动的是fragment，而不是整个directory。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:4:2","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"2.3 数据模型 Spanner为应用程序暴露了如下的一系列数据特性：数据模型基于模型化的（schematized）半关系型表、一种查询语言、和通用的事务。在设计时，让Spanner支持这些特性的原因有很多。支持模型化半关系型表和副本同步的需求源于Megastore的流行[5]。Google内部至少有300个应用程序使用Megastore（尽管其性能相对较低），因为它的数据模型管理起来比Bigtable的更简单，且它支持跨数据中心的副本同步。（Bigtable仅支持跨数据中心的副本最终一致性。）使用Megastore的比较出名的Google应用程序的例子有Gmail、Picase、Calendar、Android Market、AppEngine。类SQL查询语言的需求也很明确，其源于交互式数据分析工具Dremel[28]的流行。最后，Bigtable因缺少跨行事务而经常被抱怨，构建Percolator[32]的部分原因就是为了解决这一问题。一些作者声称支持通用的两段提交带来的性能或可用性问题导致的开销太过昂贵[9, 10, 19]。我们认为让应用程序开发者解决因过度使用事务而导致的性能瓶颈更好，而不是让开发者总是围绕缺少事务的问题编写代码。 应用程序数据模型在Spanner的实现提供的“directory-bucket”键值映射（directory-bucketed key-value mapping）的上层。应用程序会在universe中创建一个或多个数据库（database）。每个数据库可以容纳数量无限的模型化的表（table）。表看上去像关系型数据库的表，它有行、列、和版本号。我们不会深入Spanner的查询语言的细节。它看上去像支持以protocol-buffer为值的字段的SQL。 Spanner的数据模型不是纯关系型的，行必须有行名。更精确地说，每个表要求有一个由一个或多个主键列组成的有序集合。这一需求让Spanner看起来仍然像一个键值存储：主键构成了行名，每张表定义的是主键列到非主键列的映射。行仅当其键的某个值（即使是NULL）被定义时才存在。采用这种结构很有用，因为它让应用程序能够通过它们对键的选择来控制数据的局部性。 图4 照片元数据的Spanner模型示例，其交错结构通过INTERLEAVE IN实现。图4 照片元数据的Spanner模型示例，其交错结构通过INTERLEAVE IN实现。 \" 图4 照片元数据的Spanner模型示例，其交错结构通过INTERLEAVE IN实现。 图4中有一个照片元数据的Spanner模型的示例，每个用户的每个相册（album）都有有一条元数据。该模型语言与Megastore的类似，另外它还要求每个Spanner数据库必须通过客户端分区到一个或多个有层次结构的表。客户端程序通过INTERLEAVE IN在数据库模型中声明该结构层次。结构层次上层的表是directory table。directory table的每行的键为$K$，它与所有后继（descendant）表中按字典序以$K$开头的行一起构成一个directory。ON DELETE CASCADE表示删除directory table中的行时删除所有相关的子行。图中还阐释了样例数据库的交错结构（interleave）：例如，$Albums(2,1)$表示$Albums$表中$user\\ id\\ 2, album\\ id\\ 1$的行。这种通过表交错形成directory的方式十分重要，因为这让客户端可以描述多个表间存在的局部性的关系，这是高性能分布式分片数据库必须的。如果没有它，Spanner将无从得知最重要的局部性关系。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:4:3","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"3. TrueTime 表1 TrueTime API。参数t是TTstamp的类型。表1 TrueTime API。参数t是TTstamp的类型。 \" 表1 TrueTime API。参数t是TTstamp的类型。 本章描述了TrueTime API并概述了其实现。我们将大部分的细节放在了另一篇论文中，本文的目标是证明这一API的能力。表1列出了API的方法。TrueTime显式地将时间表示为$TTinterval$，它是一个有时间不确定度界限的时间范围（不像标准的时间接口，标准时间接口不会给客户端不确定度的概念）。$TTinterval$的接入点（endpoint）是$TTstamp$类型。$TT.now()$方法返回一个$TTinterval$，该时间范围保证了包含$TT.now()$被调用的绝对时间。该时间类似于带有闰秒（leap-second）的UNIX时间。（译注：此处原文为“The time epoch is analogous to UNIX time with leap-second smearing.”）定义瞬时误差的界限为$\\epsilon$，其为时间范围宽度的一半，定义平均错误界限为$\\bar{\\epsilon}$。$TT.after()$和$TT.before()$是对$TT.now()$的方便的封装。 函数$t_{abs}(e)$表示事件$e$的绝对时间。用更加正式的术语来说，TrueTime能够保证，对于一次调用$tt = TT.now()$来说，$tt.earliest \\le t_{abs}(e_{now}) \\le tt.latest$，其中$e_{now}$表示“调用”事件。 TrueTime在底层使用的参考时间为GPS和原子时钟。TrueTime使用了两种形式的参考时间，因为它们有不同的故障模式。GPS参考源的弱点有天线和接收器故障、本地无线电干扰、相关故障（例如，如闰秒处理不正确的设计故障、和欺骗等）、和GPS系统停机。原子时钟可能会以与GPS和彼此不相关的方式发生故障，且在长时间后会由于频繁错误而发生明显的漂移。 TrueTime通过每个数据中心的time server机器集合和每个机器的timeslave daemon的实现。大多数的master都有带专用天线的GPS接收器；这些master在物理上被划分开，以减少天线故障、无线电干扰、和欺骗的影响。其余的master（我们称其为Armageddon master）配备了原子时钟。原子时钟并没有那么贵：Armageddon master的成本与GPS master的成本在同一数量级。所有master的参考时间通常彼此不同。每个master还会通过它自己的本地时钟较差验证其参考时间提前的速率，如果二者有实质性的分期，则自己退出集合。在同步期间，Armageddom master会保守地给出从最坏的情况下的时钟漂移得出的缓慢增长的时间不确定性。GPS master会给出通常接近零的的不确定性。 每个daemon会轮询各种master[29]来减少任意一个master的错误的影响。一些是从就近的数据中心选取的GPS master，一些是从更远的数据中心的GPS master，对Armageddon master来说也是一样。daemon使用一种Marzullo算法的变体[27]来检测并拒绝说谎者，并与没说谎的机器同步本地的机器时钟。为了防止本地时钟故障，应该淘汰掉发生偏移频率大于从组件规格和操作环境得出的界限的机器。 在同步期间，daemon会给出缓慢增长的时间不确定性。$\\epsilon$保守地从最坏的本地市中偏移得出。$\\epilson$还依赖time master的不确定性和到time master的通信延迟。在我们的生产环境中，$\\epsilon$通常是时间的锯齿波函数（sawtooth functon），每次轮询的间隔大概在1ms到7ms间。因此，大部分时间里$\\bar{\\epsilon}$为4ms。目前，daemon的轮询间隔为30秒，且当前的漂移速率被设置为200ms/s，二者一起组成了0到6ms的锯齿边界。剩下的1ms来自于到time master的通信延迟。在出现故障时，锯齿波可能会出现偏移。例如，偶尔的time master的不可用可能导致数据中心范围的$\\epsilon$增加。同样，机器和网络连接过载可能导致$\\epsilon$偶尔出现局部峰值。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:5:0","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"4. 并发控制 本章描述了如何使用TrueTime确保并发控制相关的正确性属性与如何使用那些属性来实现如外部一致事务、无锁只读事务、和过去数据的非阻塞读取等特性。这些特性能实现例如确保整个数据库在时间戳$t$时刻的读取能够准确的看到截止$t$时刻的每个提交的事务的影响等功能。 此外，将Paxos可见的写入（我们称之为Paxos write，除非上下文明确提到）与Spanner客户端写入区分开非常重要。例如，两阶段提交会在就绪阶段（prepare phase）生成一个Paxos write，而没有相关的Spanner客户端写入。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:6:0","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"4.1 时间戳管理 表2 Spanner中的读写类型与对比。表2 Spanner中的读写类型与对比。 \" 表2 Spanner中的读写类型与对比。 表2列出了Spanner支持的操作类型。Spanner的实现支持读写事务（read-write transaction）、只读事务（read-only transaction）（即预先声明了的快照隔离事务，(predeclared snapshot-isolation transactions））、和快照读取（snapshot read）。单独的写入作为读写事务实现；单独的非快照读作为只读事务实现。二者都在内部重试。（客户端不需要自己编写重试循环。） 只读事务是一种有快照隔离[6]的优势的事务。只读事务必须预先声明其没有任何写入，它并不只是没有任何写入操作的读写事务。只读事务中的读取会以系统选取的时间戳无锁执行，这样可以让到来写入不会被阻塞。只读事务中的读取操作可以在任何足够新的副本上执行（见章节4.1.3）。 快照读取是无锁执行的对过去数据的读取操作。客户端可能为每个快照读取制定一个时间戳，也可能提供一个所需的时间戳的过期上限并让Spanner选取一个时间戳。在任一种情况下，快照读取都可以在任何足够新的副本上执行。 对于只读事务和快照读取来说，一旦时间戳被选取后，不可避免地需要提交，除非该时间戳的数据已经被垃圾回收掉了。因此，客户端可以避免在重试循环中缓冲结果。当一个服务器故障时，客户端可以在内部对另一台服务器重复该时间戳和当前读取的位置继续执行查询。 4.1.1 Paxos leader租约 Spanner的Paxos实现使用了基于定时的租约来长期保持领导权（默认为10秒）。潜在的leader会发送请求以获得基于定时的lease vote（租约投票），当leader收到一定数量的lease vote后，leader会得知它持有了租约。副本会在成功的写入操作中隐式地延长其lease vote，且leader会在lease vote快要过期时请求延长lease vote。定义leader的lease interval（租约时间范围） 的起始时间为leader发现了它收到了一定数量的lease vote的时间，结束时间为它不再有一定数量的lease vote的时间（因为一些lease vote过期了）。Spanner依赖如下的不相交的定理（invariant）：在每个Paxos group中，每个Paxos的leader的lease interval与所有其它的leader的lease interval不相交。附录A描述了该定理是如何成立的。 Spanner的实现允许Paxos leader通过让slave释放其lease vote的方式来退位（abdicate）。为了保持不相交性不变，Spanner对可以退位的时间进行了约束。定义$s_{max}$为leader使用的最大的时间戳。后面的章节会说明何时可以增大$s_{max}$的值。在退位前，leader必须等到$TT.after(s_{max})$为true。 4.1.2 为读写事务分配时间戳 事务的读写使用两阶段锁。因此，可以在已经获取了所有锁之后与任何锁被释放之前的任意时间里为其分配时间戳。对一个给定的事务，Spanner为其分配的时间戳是Paxos为Paxos write分配的表示事务提交的时间戳。 Spanner依赖如下的单调定理：在每个Paxos group内，Spanner以单调增加的顺序为Paxos write分配时间戳，即使跨leader也是如此。单个leader副本可以单调递增地分配时间戳。通过使用不相交定理，可以在跨leader的情况下保证该定理：leader必须仅在它的leader租约的期限内分配时间戳。注意，每当时间戳$s$被分配时，$s_{max}$会增大到$s$，以保持不相交性。 Spanner还保证了如下的的外部一致性定理：如果事务$T_2$在事务$T_1$提交之后开始，那么$T_2$的提交时间戳一定比$T_1$的提交时间戳大。定义事务$T_i$的开始事件与提交事件分别为$e_i^{start}$和$e_i^{commit}$、事务$T_i$的提交时间戳为$s_i$。该定理可以使用$t_{abs}(e_1^{commit}) \u003c t_{abs}(e_2^{start}) \\implies s_1 \u003c s_2$表示。这一用来执行事务与分配时间戳的协议遵循两条规则，二者共同保证了定理，如下所示。定义写入事务$T_i$的提交请求到达coordinator leader的事件为$e_i^{server}$。 开始（Start）： 写入事务$T_i$的coordinator leader在$e_i^{server}$会为其计算并分配值不小于$TT.now().latest$的时间戳$s_i$。注意，participant leader于此无关；章节4.2.1描述了participant如何参与下一条规则的实现。 提交等待（Commit Wait）： coordinator leader确保了客户端在$TT.after(s_i)$为true之前无法看到任何由$T_i$提交的数据。提交等待确保了$s_i$比$T_i$的提交的绝对时间小，或者说$s_i \u003c t_{abs}(e_i^{commit})$。该提交等待的实现在章节4.2.1中描述。证明： $$ s_1 \u003c t_{abs}(e_1^{commit}) \\tag{commit wait} $$ $$ t_{abs}(e_1^{commit}) \u003c t_{abs}(e_2^{start}) \\tag{assumption} $$ $$ t_{abs}(e_2^{start}) \\le t_{abs}(e_2^{server}) \\tag{causality} $$ $$ t_{abs}(e_2^{server}) \\le s_2 \\tag{start} $$ $$ s_1 \u003c s_2 \\tag{transitivity} $$ 4.1.3 在某时间戳处提供读取服务 章节4.1.2中描述的单调性定理让Spanner能够正确地确定副本的状态对一个读取操作来说是否足够新。每个副本会追踪一个被称为safe time（安全时间） 的值$t_{safe}$，它是最新的副本中的最大时间戳。如果读操作的时间戳为$t$，那么当$t \\le t_{safe}$时，副本可以满足该读操作。 定义$t_{safe} = \\min(t_{safe}^{Paxos},t_{safe}^{TM})$，其中每个Paxos状态机有safe time $t_{safe}^{Paxos}$，每个transaction manager有safe time $t_{safe}^{TM}$。$t_{safe}^{Paxos}$简单一些：它是被应用的序号最高的Paxos write的时间戳。因为时间戳单调增加，且写入操作按顺序应用，对于Paxos来说，写入操作不会发生在$t_{safe}^{Paxos}$或更低的时间。 如果没有就绪（prepared）的（还没提交的）事务（即处于两阶段提交的两个阶段中间的事务），那么$t_{safe}^{TM}$为$\\infty$。（对于participant slave，$t_{safe}^{TM}$实际上表示副本的leader的transaction manager的safe time，slave可以通过Paxos write中传递的元数据来推断其状态。）如果有任何的这样的事务存在，那么受这些事务影响的状态是不确定的：particaipant副本还不知道这样的事务是否将会提交。如我们在章节4.2.1中讨论的那样，提交协议确保了每个participant知道就绪事务的时间戳的下界。对group $g$来说，每个事务$T_i$的participant leader会给其就绪记录（prepare record）分配一个就绪时间戳（prepare timestamp）$s_{i,g}^{prepare}$。coordinator leader确保了在整个participant group $g$中，事务的提交时间戳$s_i \\ge s_{i,g}^{prepare} $。因此，对于group $g$中的每个副本，对$g$中的所有事务$T_i$，$t_{safe}^{TM} = \\min_i(s_{i,g^{prepare}})-1$。 4.1.4 为只读事务分配时间戳 只读事务以两阶段执行：分配时间戳$s_{read}$[8]，然后在$s_{read}$处以快照读取的方式执行事务的读取。快照读取能够在任何足够新的副本上执行。 $s_{read}=TT.now()$在事务开始后的任意时间分配，它可以通过像章节4.1.2中针对写入操作提供的参数的方式来保证外部一致性。然而，对这样的时间戳来说，如果$t_{safe}$还没有足够大，在$s_{read}$时对块的读取操作可能需要被阻塞。（另外，在选取$s_{read}$的值的时候，可能还需要增大$s_{max}$的值来保证不相交性。）为了减少阻塞的可能性，Spanner应该分配能保证外部一致性的最老的时间戳。章节4.2.2解释了如何选取这样的时间戳。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:6:1","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"4.2 细节分析 本节解释了读写事务和只读事务中之前省略的一些使用的细节，以及用来实现原子性模型修改的特殊事务类型的实现。然后还描述了对之前描述的基本方案的一些改进 4.2.1 读写事务 像Bigtable一样，事务中的写入操作在提交前会在客户端缓冲。这样，事务中的读取操作无法看到事务的写入操作的效果。在Spanner中，这也是很好的设计，因为读操作会返回任何读取的数据的时间戳，而未提交的写入操作还没有被分配时间戳。 读写事务中的读操作使用了伤停等待（wound-wait）[33]来避免死锁。客户端将读取提交给相应的group中的leader副本，它会获取读取锁并读取最新的数据。当事务保持打开（open）时，它会发送保活消息（keepalive message）以避免participant leader将其事务超时。当客户端完成了所有的读取并缓冲了所有的写入后，它会开始两阶段提交。客户端选取一个coordinator group并向每个participant的leader发送带有该coordinator的标识和和所有缓冲的写入的提交消息。让客户端驱动两阶段提交能够避免跨广域链路发送两次数据。 非coordinator participant的leader会先获取写入锁。然后它会选取一个必须大于任意它已经分配给之前的事务的就绪时间戳（以保证单调性），并通过Paxos将就绪记录写入日志。然后每个participant会通知coordinator其就绪时间戳。 coordinator leader同样会获取写入锁，但是跳过就绪阶段。它在收到其它所有的participant leader的消息后为整个事务选取一个时间戳。该提交时间戳$s$必须大于或等于所有的就绪时间戳（以满足章节4.1.3中讨论的约束）、大于coordinator收到其提交消息的时间$TT.now().latest$、并大于任何该leader已经分配给之前事务的时间戳（同样为了保证单调性）。然后，coordinator leader会通过Paxos将提交记录写入日志（或者，如果在等待其它participant是超时，那么会打断它）。 在允许任何coordinator副本应用该提交记录之前，coordinator leader会等到$TT.after(s)$，以遵循章节4.1.2中描述的提交等待规则。因为coordinator leader基于$TT.now().latest$选取$s$，且等待该时间戳变成过去时，所以期望等待时间至少为$2*\\bar{\\epsilon}$。这一等待时间通常会与Paxos通信重叠。在提交等待后，coordinator会将提交时间戳发送给客户端和所有其它的participant leader。每个participant leader会将事务的结果通过该Paxos记录。所有的participant会在相同的时间戳处应用事务，然后释放锁。 4.2.2 只读事务 分配时间戳是在所有参与读取的的Paxos group间的协商阶段（negotiation phase）执行的。这样，对每个只读事务，Spanner都需要一个作用域（scope）表达式 ，该表达式总结了将将要被整个事务读取的键。Spanner自动地为单独的查询推导作用域。 如果作用域的值通过单个Paxos group提供服务，那么客户端会向该group的leader提出只读事务。（当前的Spanner只会在Paxos leader为一个只读事务选取一个时间戳。）该leader分配$s_{read}$并执行读取操作。对于单站点（single-site）的读取操作，Spanner通常能提供比$TT.now().latest$更好的支持。定义$LastTS()$为一个Paxos group最后一次已提交的写入的时间戳。如果该没有就绪的事务，则分配$s_{read}=LastTS()$就能满足外部一致性：事务将会看到最后一次写入的结果，也因此它发生在写入之后。 如果作用于的值由多个Paxos group提供服务，那么有很多种选择。最复杂的选择是与所有的group的leader做一轮通信来基于$LastTS()$协商$s_{read}$。目前，Spanner实现了一个更简单的一种选择。客户端避免了一轮通信，仅让它的读操作在$s_{read}=TT.now().latest$时执行（可能需要等到safe time增加）。事务中的所有读取能被发送到足够新的副本。 4.2.3 模型修改事务 TrueTime让Spanner能够支持原子模型修改。使用标准的事务执行模型修改是不可性的，因为participant的数量（数据库中group的数量）可能有上百万个。Bigtable支持在一个数据中心中的原子性模型修改，但是其模型修改会阻塞所有操作。 Spanner的模型修改事务是更加通用的非阻塞标准事务的变体。第一，它会显式地分配一个未来的时间戳，该时间戳是在就绪阶段注册的。因此，跨数千台服务器的模型修改对其它并发活动的干扰最小。第二，读取和写入操作隐式依赖于模型，它们与所有注册时间为$t$的模型修改时间戳是同步的：如果它们在时间戳$t$之前，那么它们能继续执行；但是如果它们的时间戳在$t$之后，那么必须阻塞到模型修改事务之后。如果没有TrueTime，定义在时间$t$发生的模型修改是没有意义的。 4.2.4 改进 之前定义的$t_{safe}^{TM}$有一个弱点，单个就绪的事务会阻止$t_{safe}$增长。这样，即使时间戳在后面的读取操作与事务不冲突，读取操作也不会发生。通过使用从键区间到就绪的事务的时间戳的细粒度的映射来增加$t_{safe}^{TM}$，可以避免这种假冲突。该信息可以保存在lock table中，该表中已经有键区间到锁元数据的映射了。当读取操作到达时，只需要检查与读操作冲突的键区间的细粒度的safe time。 之前定义的$LastTS()$也用类似的弱点：如果有事务刚被提交，无冲突的只读事务仍必须被分配时间戳$s_{read}$并在该事务之后执行。这样，读操作会被推迟。这一弱点可通过相似的手段解决，通过lock table中细粒度的从键区间到提交时间戳的映射来增强$LastTS()$。（目前我们还没有实现这一优化。）当只读事务到达时，可将与该事务冲突的键区间的最大$LastTS()$的值作为时间戳分配给该事务，除非存在与它冲突的就绪事务（可以通过细粒度的safe time确定）。 之前定义的$t_{safe}^{Paxos}$的弱点是，如果没有Paxos write，它将无法增大。也就是说，如果一个Paxos group的最后一次写入操作发生在$t$之前，那么该group中发生在时间$t$的快照读取无法执行。Spanner通过利用leader租约时间范围不相交定理解决了这一问题。每个Paxos leader会通过维持一个比将来会发生的写入的时间戳更大的阈值来增大$t_{safe}^{Paxos}$：Paxos leader维护了一个从Paxos序号$n$到可分配给Paxos序号为$n+1$的最小时间戳的映射$MinNextTS(n)$。当副本应用到$n$时，它可以将$t_{safe}^{Paxos}$增大到$MinNextTS(n)$。 单个leader实现其$MinNextTS()$约定很容易。因为$MinNextTS()$约定的时间戳在一个leader租约内，不相交定理能保证在leader间的$MinNextTS()$约定。如果leader希望将$MinNextTS()$增大到超过其leader租约之外，那么它必须先延长其leader租约。注意，$s_{max}$总是要增大到$MinNextTS()$中最大的值，以保证不相交定理。 leader默认每8秒增大一次$MinNextTS()$的值。因此，如果没有就绪事务，空闲的Paxos group中的健康的slave在最坏情况下会为读操作提供超过8秒后的时间戳。leader也会依照来自slave的需求增大$MinNextTS()$的值。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:6:2","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"5. 评估 首先，我们测量了Spanner在副本、事务和可用性方面的性能。接着，我们提供了一些有关TrueTime的表现的数据，以及对我们的第一个使用者F1的案例研究。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:7:0","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"5.1 小批量benchmark 表3展示了Spanner的一些小批量benchmark。这些测量是在分时机器上运行的：每个spanserver都在4GB RAM和4核（AMD Barcelona 2200MHz）的调度单元上运行。客户端运行在不同的机器上。每个zone中包含1个spanserver。client和zone被放置在网络距离小于1ms的一系列数据中心中。（这种布局是很常见的：大多数应用程序不需要将它们的数据分布到全球范围内。）测试数据库由50个Paxos group和2500个directory构成。操作有单独的读取操作和4KB写入操作。为所有的读操作提供服务即使在内存规整后也会用尽内存，因此我们仅测量了Spanner调用栈的开销。另外，我们首先进行了一轮没有测量性能的读操作来为本地缓存热身。 表3 操作的小批量benchmark。10次运行的均值与标准差。1D表示1个副本禁用了提交等待。表3 操作的小批量benchmark。10次运行的均值与标准差。1D表示1个副本禁用了提交等待。 \" 表3 操作的小批量benchmark。10次运行的均值与标准差。1D表示1个副本禁用了提交等待。 对于延迟实验，客户端会发出很少的操作，以避免在服务器上排队。从1路副本实验得出，提交等待打野为5ms，Paxos的延迟大约为9ms。随着副本数的增加，延迟大概恒定，且标准差更小，因为Paxos在一个group的副本汇总并行执行。随着副本数的增加，达到大多数投票（quorum）的延迟不再对单个较慢的slave副本敏感。 对于吞吐量实验，客户端会发出很多的操作，以使服务器的CPU饱和。快照读取可以在任何足够新的副本上执行，因此它们的吞吐量几乎随着副本数量线性增加。只有一次读取的只读事务仅在leader执行，因为时间戳分配必须在leader中发生。只读事务的吞吐量会随着本书增加而增加，因为有效的spanserver的数量增加了：在实验的配置中，spanserver的数量等于副本的数量，leader随机地分布在zone中。写入的吞吐量受益于相同的实验因素（这解释了副本数从3增长到5时的吞吐量增加），但是随着副本数的增加，每次写入执行的工作量线性增加，其开销超过了带来的好处。 表4 两阶段提交的伸缩性。10次运行的均值与标准差。表4 两阶段提交的伸缩性。10次运行的均值与标准差。 \" 表4 两阶段提交的伸缩性。10次运行的均值与标准差。 表4展示了两阶段提交能够扩展到合理的参与者数量：其对跨3个zone运行的一系列实验进行了总结，每个实验有25个spanserver。在扩展到50个participant时，均值和99%比例的延迟都很合理，而扩展到100个participant时延迟开始显著增加。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:7:1","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"5.2 可用性 图5阐释了在多个数据中心运行Spanner在可用性上的好处。其展示了出现数据中心故障时的三个实验的结果，所有的实验都在相同的时间范围内。该测试universe有5个zone $Z_i$组成，每个Zone有25个spanserver。测试数据库被分片到了1250个Paxos group中，100个测试客户端持续地以总速率50K次读取/秒发出非快照读取操作。所有的leader都被显式地放置在$Z_1$中。在每个实验的5秒后，一个zone内的所有服务器都被杀掉，具体情况如下：非leader杀掉$Z_2$；leader强行杀掉$Z_1$（hard kill）；leader杀掉$Z_1$（soft kill），但是它会通知所有的服务器应先移交领导权。 图5 杀掉服务器对吞吐量的影响。图5 杀掉服务器对吞吐量的影响。 \" 图5 杀掉服务器对吞吐量的影响。 杀掉$Z_2$对读取吞吐量没有影响。在杀掉$Z_1$时给leader时间来将领导权移交给另一个zone的影响最小：其吞吐量的减小在图中看不出，大概在3~4%。而另一方面，不进行警告就杀掉$Z_1$的影响最严重：完成率几乎降到了0.然而，随着leader被重新选举出，系统的吞吐量升高到了约100K读取/秒，其原因在于我们实验中的2个因素：系统还有余量、leader不可用时操作会排队。因此，系统的吞吐量会增加，然后再慢慢回到其稳定状态下的速率。 我们还能看出Paxos的leader租约被设置为10秒带来的影响。当我们杀掉zone的时候，leader租约的过期时间应在接下来的10秒中均匀分布。在每个死去的leader的租约过期不久后，新的leader会被选举出来。大概在杀掉的时间的10秒后，所有的group都有的leader，且吞吐量也恢复了。更短的租约时间会减少服务器死亡对可用性的影响，但是需要个更多的刷新租约使用的网络流量总量。我们正在设计并实现一种机制，让slave能在leader故障时释放Paxos leader租约。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:7:2","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"5.3 TrueTime 关于TrueTime，必须回答两个问题：$\\epsilon$真的是时钟不确定度的界限吗？$\\epsilon$最坏是多少？对于前者，虽重要的问题是，本地时钟漂移是否会大约200us/sec：这回打破TrueTime的假设。根据我们对机器的统计，CPU的故障率是时钟故障率的6倍。也就是说，相对于更严重的硬件问题而言，时钟问题极少发生。因此，我们认为TrueTime的实现与Spanner依赖的所有软件一样值得信赖。 图6 TrueTime的ε值的分布，在timeslave daemon查询time master后立即采样。图中分别绘制了第90%、99%、99.9%个数据的情况。图6 TrueTime的ε值的分布，在timeslave daemon查询time master后立即采样。图中分别绘制了第90%、99%、99.9%个数据的情况。 \" 图6 TrueTime的ε值的分布，在timeslave daemon查询time master后立即采样。图中分别绘制了第90%、99%、99.9%个数据的情况。 图6给出了在距离高达2200km的数据中心间的几千台spanserver机器上获取的TrueTime数据。图中绘出了第90%、99%和99.9%个的$\\epsilon$，其在timeslave daemon查询time master后立即采样。采样中去掉了$\\epsilon$因本地时钟的不确定度而产生的锯齿波，因此其测量的是time master的不确定度（通常为0）加上到time master的通信延迟的值。 数据表明，通常来说，决定了$\\epsilon$的这两个因素通常不是问题。然而，其中存在明显的尾延迟（tail-latency）问题，这会导致$\\epsilon$的值更高。尾延迟在3月30日减少了，这时由于网络得到了改进，其减少了瞬时的网络链路拥堵。$\\epsilon$在4月13日变大了约一个小时，这是由于一个数据中心例行维护中关闭了master两次。我们将继续调查并消除TrueTime峰值的原因。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:7:3","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"5.4 F1 Spanner于2011年初开始在生产负载下进行实验评估，其作为F1（Google重写的广告系统后端系统）的一部分[35]。起初，该后端基于MySQL数据库，并手动将其按多种方式分片。其未压缩的数据集有数十TB，虽然这与许多NoSQL的实例相比很小，但是已经足够大以至于需要MySQL中的难用的分片机制。MySQL的分片策略会将每个消费者与所有相关数据分配到一个固定的分片中。这种布局让每个消费者可以使用索引与复杂的查询，但是这需要有对程序的业务逻辑的分片有所了解。随着消费者的数量与其数据量的增长，重新分片的开销对数据库来说十分昂贵。最后一次重分片花费了两年多的时间，涉及到数十个团队的协作与测试以降低其风险。这样的操作太过复杂而不能定期执行：因此，该团队不得不将一些数据存储在额外的Bigtable中以限制MySQL数据库的增长，这对影响了事务表现和跨所有数据的查询能力。 F1团队选择使用Spanner的原因有很多。第一，Spanner消除了手动重分片的需求。第二，Spanner提供了副本同步和自动化故障转移。在MySQL的master-slave的副本策略下，实现故障转移很困难，且有数据丢失的风险与停机时间。第三，F1需要强事务语义，这使得其无法使用其它的NoSQL系统。应用程序的语义需要跨任意数据上的事务和一致性读取。F1团队还需要在他们的数据上使用辅助索引（secondary index）（因为Spanner尚未为辅助索引提供自动支持），而他们可以使用Spanner的事务来实现他们自己的一致全局索引。 目前，所有应用程序的写入操作默认通过F1发送给Spanner，以取代基于MySQL的程序栈。F1在美国的西海岸有2份副本，在东海岸有3份副本。副本站点的选择基于潜在的重大自然灾害造成停电的可能性与它们的前端站点位置。有趣的是，Spanner的自动故障转移对它们来说几乎是不可见的。尽管最近几个月发生了计划外的集群故障，但是F1团队需要做的最大的工作是更新他们的数据库模型，以让Spanner知道在哪里优先放置Paxos leader，从而使其接近其前端移动后的位置。 Spanner的时间戳语义让F1可以高效地维护从数据库状态计算出的内存数据结构。F1维护了所有修改的逻辑历史纪录，其作为每个事务的一部分写入了Spanner本身。F1会获取某一时间戳上完整的数据快照以初始化它的数据结构，然后读取增量的修改并更新数据结构。 表5 F1中directory与fragment的数量分布。表5 F1中directory-fragment的数量分布。 \" 表5 F1中directory与fragment的数量分布。 表5阐述了F1中每个directory中的fragment的数量的分布。每个directory通常对应于一个F1上的应用程序栈的消费者。绝大多数的directory（即对绝大多数消费者来说）仅包含一个fragment，这意味着对那些消费者数据的读写操作能保证仅发生在单个服务器上。包含100多个fragment的directory都是包含F1辅助索引的表：对于这种不止有几个fragment的表的写入是极为少见的。F1团队仅在他们以事务的方式处理未优化的批数据负载时见到过这种行为。 表6 在24小时内测量的F1感知到的操作延迟。表6 在24小时内测量的F1感知到的操作延迟。 \" 表6 在24小时内测量的F1感知到的操作延迟。 表6给出了从F1服务器测出的Spanner操作延迟。在东海岸的数据中心在选取Paxos leader方面有更高的优先级。表中的数据是从这些数据中心内的F1服务器测量的。写入延迟的标准差更大，这是由因锁冲突而导致的一个长尾操作导致的。读取延迟的标准差甚至更大，其部分原因是，Paxos leader分布在两个数据中心中，其中只有一个数据中心有装有SSD的机器。此外，我们还对两个数据中心中的系统的每个读取操作进行了测量：字节读取量的均值与标准差分别约为1.6KB和119KB。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:7:4","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"6. 相关工作 Megastore[5]和DynamoDB[3]中提供了跨数据中新的一直副本服务。DynamoDB给出了键值接口，且副本仅在一个区域内。Spanner像Megastore一样提供了半结构化的数据模型和一个与其相似的模型语言。Megastore没有达到很高的性能。因为Megastore位于Bigtable之上，这增加了高昂的通信开销。Megastore还不支持长期leader：可能有多个副本启动写入操作。在Paxos协议中，来自不同副本的所有写入必将发生冲突，即使它们在逻辑上并不冲突，这会导致单个Paxos group上的每秒钟写入吞吐量下降。Spanner提供了更高的性能、通用的事务、和外部一致性。 Pavol等人[31]对比了数据库和MapReduce[12]的性能。他们指出，在分布式键值存储上探索数据库功能一些其它工作[1, 4, 7, 41]是这两个领域正在融合的证据。我们同意这一结论，但是我们证明了在多层上进行集成也有它特有的优势：例如，在多副本上集成并发控制减少了Spanner中提交等待的开销。 在多副本存储上的分层事务的概念至少可以追溯到Gifford的论文[16]。Scatter[17]是一个最近出现的基于DHT的键值存储，它在一致性副本上实现了分层事务。Spanner着眼于提供比Scatter更高层的接口。Gray和Lamport[18]描述了一直基于Paxos的非阻塞提交协议。与两阶段提交相比，他们的协议产生了更多的消息开销，这将增加分布更广的group的提交开销的总量。Walter[36]提供了一种快照隔离的变体，其适用于数据中心内，而不适用于跨数据中心的场景。相反，我们的只读事务提供了更自然的语义，因为我们的所有操作都支持外部一致性。 最近有大量关于减少或消除锁开销的工作。Calvin[40]去掉了并发控制：它预先分配时间戳并按时间戳的顺序执行事务。HStore[39]和Granola[11]都支持它们自己的事务类型，其中一些事务可以避免锁。这些系统都没有提供外部一致性。Spanner通过提供快照隔离的方式解决了争用问题。 VoltDB[42]是一个内存式分片数据库，其支持广域下的master-slave的多副本策略以支持容灾恢复，但是不支持更通用的副本配置。它是NewSQL的一个例子，支持可伸缩的SQL[38]是其亮点。大量的商业数据库（如MarkLogic[26]和Oracle的Total Recall[30]）都实现了对过去数据的读取。Lomet和Li[24]描述了一种用于这种时态数据库的实现策略。 对于可信参考时钟方面，Farsite得出了时钟不确定度的界限（比TrueTime的界限宽松很多）[13]：Farsite中的服务器租约与Spanner维护Paxos租约的方式相同。在之前的工作中[2, 23]，松散的时钟同步已经被用于并发控制。我们已经给出了使用TrueTime作为Paxos状态机间全局时间的原因之一。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:8:0","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"7. 后续工作 在去年的大部分时间里，我们都在与F1团队合作，将Google的广告后端从MySQL迁移到Spanner。我们正在积极地提供监控与支持工具，并对其性能调优。另外，我们一直在改进我们的备份/还原系统的功能与性能。目前，我们正在实现Spanner的模型预言、辅助索引的自动化维护、和基于负载的自动化分片。对于更长期来说，我们计划去调研一些功能。乐观地并行读取可能是一个很有价值的策略，但是初步试验表示想要正确地实现它并非易事。此外，我们计划最终支持对Paxos配置的直接修改[22, 34]。 因为我们期望许多用应程序会将数据副本分布到彼此较近的数据中心中，TrueTime $\\epsilon$可能会明显影响西能。我们认为，将$\\epsilon$降低到1ms以内没有不可逾越的障碍。可以减小time master的查询间隔时间，并使用相对便宜的石英钟。可以通过改进网络技术的方式减小time master的查询延迟，或者，甚至可以通过其它分布式时钟技术来避免这一问题。 最后，还有很多明显需要改进的地方。尽管Spanner能扩展到大量节点上，节点内的本地数据结构在在执行复杂的SQL查询时性能相对较低，因为它们是为简单的键值访问设计的。数据库领域的文献中的算法与数据结构可以大幅改进单节点的性能。其次，能够自动化地在数据中心间移动数据以响应客户端中负载的变化长期以来一直是我们的目标之一，但是为了实现这一目标，我们还需要能够自动化、协作地在数据中心间移动客户端程序进程的能力。移动进程会让数据中心间的资源获取与分配的管理更加困难。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:9:0","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"8. 结论 总而言之，Spanner结合并扩展了两个研究领域的观点：在更接近的数据库领域，需要易用的半结构化接口、事务、和基于SQL的查询语言；在系统领域，需要可伸缩、自动分片、容错、一致性副本、外部一致性、和广域分布。自从Spanner诞生以来，我们花了5年多的时间迭代设计与实现。这漫长的迭代部分原因是，人们很久才意识到Spanner应该做的不仅仅是解决全球化多副本命名空间的问题，还应该着眼于Bigtable锁缺少的数据库特性。 我们的设计中的一方面十分重要：Spanner的特性的关键是TrueTime。我们证明了，通过消除时间API中的始终不确定度，=能够构建时间语义更强的分布式系统。此外，因为底层系统对时钟不确定度做了更严格的限制，所以实现更强的语义的开销减少了。在这一领域中，在设计分布式算法时，我们应该不再依赖宽松的时钟同步和较弱的时间API。 ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:10:0","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"致谢 Many people have helped to improve this paper: our shepherd Jon Howell, who went above and beyond his responsibilities; the anonymous referees; and many Googlers: Atul Adya, Fay Chang, Frank Dabek, Sean Dorward, Bob Gruber, David Held, Nick Kline, Alex Thomson, and Joel Wein. Our management has been very supportive of both our work and of publishing this paper: Aristotle Balogh, Bill Coughran, Urs Holzle, Doron Meyer, Cos Nicolaou, Kathy Polizzi, Sridhar Ramaswany, and Shivakumar Venkataraman. We have built upon the work of the Bigtable and Megastore teams. The F1 team, and Jeff Shute in particular, worked closely with us in developing our data model and helped immensely in tracking down performance and correctness bugs. The Platforms team, and Luiz Barroso and Bob Felderman in particular, helped to make TrueTime happen. Finally, a lot of Googlers used to be on our team: Ken Ashcraft, Paul Cychosz, Krzysztof Ostrowski, Amir Voskoboynik, Matthew Weaver, Theo Vassilakis, and Eric Veach; or have joined our team recently: Nathan Bales, Adam Beberg, Vadim Borisov, Ken Chen, Brian Cooper, Cian Cullinan, Robert-Jan Huijsman, Milind Joshi, Andrey Khorlin, Dawid Kuroczko, Laramie Leavitt, Eric Li, Mike Mammarella, Sunil Mushran, Simon Nielsen, Ovidiu Platon, Ananth Shrinivas, Vadim Suvorov, and Marcel van der Holst. ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:11:0","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"参考文献 [1] Azza Abouzeid et al. “HadoopDB: an architectural hybrid of MapReduce and DBMS technologies for analytical workloads”. Proc. of VLDB. 2009, pp. 922–933. [2] A. Adya et al. “Efficient optimistic concurrency control using loosely synchronized clocks”. Proc. of SIGMOD. 1995, pp. 23–34. [3] Amazon. Amazon DynamoDB. 2012. [4] Michael Armbrust et al. “PIQL: Success-Tolerant Query Processing in the Cloud”. Proc. of VLDB. 2011, pp. 181–192. [5] Jason Baker et al. “Megastore: Providing Scalable, Highly Available Storage for Interactive Services”. Proc. of CIDR. 2011, pp. 223–234. [6] Hal Berenson et al. “A critique of ANSI SQL isolation levels”. Proc. of SIGMOD. 1995, pp. 1–10. [7] Matthias Brantner et al. “Building a database on S3”. Proc. of SIGMOD. 2008, pp. 251–264. [8] A. Chan and R. Gray. “Implementing Distributed Read-Only Transactions”. IEEE TOSE SE-11.2 (Feb. 1985), pp. 205–212. [9] Fay Chang et al. “Bigtable: A Distributed Storage System for Structured Data”. ACM TOCS 26.2 (June 2008), 4:1–4:26. [10] Brian F. Cooper et al. “PNUTS: Yahoo!’s hosted data serving platform”. Proc. of VLDB. 2008, pp. 1277–1288. [11] James Cowling and Barbara Liskov. “Granola: Low-Overhead Distributed Transaction Coordination”. Proc. of USENIX ATC. 2012, pp. 223–236. [12] Jeffrey Dean and Sanjay Ghemawat. “MapReduce: a flexible data processing tool”. CACM 53.1 (Jan. 2010), pp. 72–77. [13] John Douceur and Jon Howell. Scalable Byzantine-FaultQuantifying Clock Synchronization. Tech. rep. MSR-TR-2003- 67. MS Research, 2003. [14] John R. Douceur and Jon Howell. “Distributed directory service in the Farsite file system”. Proc. of OSDI. 2006, pp. 321–334. [15] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. “The Google file system”. Proc. of SOSP. Dec. 2003, pp. 29–43. [16] David K. Gifford. Information Storage in a Decentralized Computer System. Tech. rep. CSL-81-8. PhD dissertation. Xerox PARC, July 1982. [17] Lisa Glendenning et al. “Scalable consistency in Scatter”. Proc. of SOSP. 2011. [18] Jim Gray and Leslie Lamport. “Consensus on transaction commit”. ACM TODS 31.1 (Mar. 2006), pp. 133–160. [19] Pat Helland. “Life beyond Distributed Transactions: an Apostate’s Opinion”. Proc. of CIDR. 2007, pp. 132–141. [20] Maurice P. Herlihy and Jeannette M. Wing. “Linearizability: a correctness condition for concurrent objects”. ACM TOPLAS 12.3 (July 1990), pp. 463–492. [21] Leslie Lamport. “The part-time parliament”. ACM TOCS 16.2 (May 1998), pp. 133–169. [22] Leslie Lamport, Dahlia Malkhi, and Lidong Zhou. “Reconfiguring a state machine”. SIGACT News 41.1 (Mar. 2010), pp. 63–73. [23] Barbara Liskov. “Practical uses of synchronized clocks in distributed systems”. Distrib. Comput. 6.4 (July 1993), pp. 211–219. [24] David B. Lomet and Feifei Li. “Improving Transaction-Time DBMS Performance and Functionality”. Proc. of ICDE (2009), pp. 581–591. [25] Jacob R. Lorch et al. “The SMART way to migrate replicated stateful services”. Proc. of EuroSys. 2006, pp. 103–115. [26] MarkLogic. MarkLogic 5 Product Documentation. 2012. [27] Keith Marzullo and Susan Owicki. “Maintaining the time in a distributed system”. Proc. of PODC. 1983, pp. 295–305. [28] Sergey Melnik et al. “Dremel: Interactive Analysis of WebScale Datasets”. Proc. of VLDB. 2010, pp. 330–339. [29] D.L. Mills. Time synchronization in DCNET hosts. Internet Project Report IEN–173. COMSAT Laboratories, Feb. 1981. [30] Oracle. Oracle Total Recall. 2012. [31] Andrew Pavlo et al. “A comparison of approaches to large-scale data analysis”. Proc. of SIGMOD. 2009, pp. 165–178. [32] Daniel Peng and Frank Dabek. “Large-scale incremental processing using distributed transactions and notifications”. Proc. of OSDI. 2010, pp. 1–15. [33] Daniel J. Rosenkrantz, Richard E. Stearns, and Philip M. Lewis II. “System level concurrency control for distributed database systems”. ACM TODS 3.2 (June 1978), pp. 178–198. [34] Alexander Shraer et al. “Dynamic Reconfiguration of Primary/Backup Clusters”. Proc. of USENIX ATC. 20","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:12:0","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"附录A Paxos leader租约管理 确保Paxos leader租约时间范围不相交的最简单的方法是，无论何时延长租约，都让leader提交一次同步的Paxos write。之后的leader会读取该时间范围并等到该范围过去。 使用TrueTime可以在不需要额外日志写入的情况下确保不相交性。潜在的第$i$个leader在有$r$个副本的情况下，会在lease vote的开始时设置下界$v_{i,r}^{leader}=TT.now().earliest$，其是在$e_{i,r}^{send}$（leader发出租约请求的时间）之前计算的。每个副本$r$在当前租约的$e_{i,r}^{grant}$时授权新租约，其发生在$e_{i,r}^{receive}$（副本收到租约请求的时间）之后；租约在$t_{i,r}^{end}=TT.now().latest_10$时结束，其是在$e_{i,r}^{receive}$之后计算的。副本$r$遵循**一次投票（single-vote）**规则：在$TT.after(t_{i,r}^{end})$为true之前，它不会再次授权lease vote。为了在不同的$r$之间保证这一规则，在授权租约之前，Spanner会在给出授权的副本中记录lease vote；这次日志写入可通过已有的Paxos协议日志写入一并完成。 当第$i$个leader收到一定数量的lease vote时（$e_i^{quorum}$事件），它会计算它的租约时间范围$lease_i=[TT.now().latest, \\min_r(v_{i,r}^{leader})+10]$。当$TT.before(\\min_r(v_{i,r}^{leader})+10)$为false，那么该leader会认为该租约过期。为了证明不相交性，我们使用了如下事实：第$i$个和第$(i+1)$个leader必须在它们的“大多数（quorum）”有一个副本的共用的。我们称该副本为$r_0$。证明： $$ lease_i.end=\\min_r(v_{i,r}^{leader}) \\tag{by definition} $$ $$ min_r(v_{i,r}^{leader})+10 \\le \\min_r(v_{i,r}^{leader})+10 \\tag{min} $$ $$ v_{i,r_0}^{leader}+10 \\le t_{abs}(e_{i,r_0}^{send})+10 \\tag{by definition} $$ $$ t_{abs}(e_{i,r_0}^{send})+10 \\le t_{abs}(e_{i,r_0}^{receive})+10 \\tag{causality} $$ $$ t_{abs}(e_{i,r_0}^{receive})+10 \\le t_{i,r_0}^{end} \\tag{by definition} $$ $$ t_{i,r_0}^{end} \u003c t_{abs}(e_{i+1,r_0}^{grant}) \\tag{single-vote} $$ $$ t_{abs}(e_{i+1,r_0}^{grant}) \\le t_{abs}(e_{i+1}^{quorum}) \\tag{causality} $$ $$ t_{abs}(e_{i+1}^{quorum}) \\le lease_{i+1}.start \\tag{by definition} $$ ","date":"2020-10-23","objectID":"/posts/paper-reading/spanner-osdi2012/:13:0","tags":["Spanner","Translation"],"title":"《Spanner: Google’s Globally-Distributed Database》论文翻译","uri":"/posts/paper-reading/spanner-osdi2012/"},{"categories":["Paper Reading"],"content":"本篇文章是对论文An Empirical Evaluation of In-Memory Multi-Version Concurrency Control的原创翻译，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:0:0","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"作者 Yingjun Wu National University of Singapore yingjun@comp.nus.edu.sg Joy Arulraj Carnegie Mellon University jarulraj@cs.cmu.edu Jiexi Lin Carnegie Mellon University jiexil@cs.cmu.edu Ran Xian Carnegie Mellon University rxian@cs.cmu.edu Andrew Pavlo Carnegie Mellon University pavlo@cs.cmu.edu ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:1:0","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"摘要 多版本并发控制（Multi-version concurrency control，MVCC）目前是现代数据库管理系统（DBMS）中最热门的事务管理策略。尽管MVCC在1970年代晚期就已经被发明出来了，但是在过去的十年中，在几乎所有主要的关系型DBMS中都使用了它。在处理事务时，维护数据的多个版本可以在不牺牲串行性的同时提高并行性。但是在多核和内存的配置中扩展MVCC并非易事：当有大量线程并行运行时，同步带来的开销可能超过多版本带来的好处。 为了理解在现代的硬件配置下处理事务时MVCC如何执行，我们对MVCC的4个关键设计决策进行了大量研究：并发控制协议、版本存储、垃圾回收、和索引管理。我们在内存式DBMS中以最高水平实现了这些所有内容的变体，并通过OLTP负载对它们进行了评估。我们的分析确定了每种设计选择的基本瓶颈。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:2:0","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"1. 引言 计算机体系结构的进步导致了多核内存式DBMS的兴起，它们使用了高效的事务管理机制以在不牺牲串行性的同时提高并行性。在最近十年的里，在DBMS开发中使用的最流行的策略是多版本并发控制（multi-version concurrency control，MVCC） 。MVCC的基本想法是，DBMS为数据库中的每个逻辑对象维护多个物理版本，让对同一个对象的操作能够并行执行。这些对象可以是任何粒度上的，但是几乎所有的MVCC DBMS都使用了元组（tuple），因为它在并行性和版本追踪（version tracking）的开销间提供了很好的平衡。多版本化可以让只读的事务访问元组的旧版本，而不会阻止读写事务在同事生成新的版本。这与单版本的系统不同，在单版本系统中，事务总是会在更新一个元组时时用新数据覆写它。 最近DBMS使用MVCC的这一趋势的有趣之处在于，MVCC策略并不是新技术。第一次提到MVCC似乎是在1979年的一篇论文中[38]，它的第一个实现始于1981年的InterBase DBMS[22]（现在作为Firebird开源）。如今，MVCC还用于一些部署最广泛的面向磁盘的DBMS中，包括Oracle（自1984年起[4]），Postgres（自1985年起[41]）和MySQL的InnoDB引擎（自2001年起）。但是，尽管有很多与这些较早的系统同时代的系统使用了单版本策略（例如，IBM DB2、Sybase），但是几乎所有新的支持事务的DBMS都避开了单版本策略转而使用MVCC[37]。无论商业系统（例如，Microsoft Hekaton[16]、SAP HANA[40]、MemSQL[1]、NuoDB[3]）还是学术系统（例如，HYRISE[21]、HyPer[36]）都是如此。 尽管所有的这些新系统都使用了MVCC，但是MVCC并没有一个“标准”实现。在一些设计中选择了不同的权衡点（trade-off）和性能表现。直到现在，都没有在现代DBMS操作环境中的对MVCC的全面的评估。最近的大量的研究在1980年代[13]，但是它在单核CPU上运行的面向磁盘的DBMS中使用了模拟的负载。古老的面向磁盘的DBMS的设计上的选择并不适用于运行在有更多CPU核数的机器上的内存式DBMS。因此，这项过去的研究并不能反映出最近的无闩（latch-free）[27]和串行[20]的并发控制与内存式存储[36]和混合负载[40]的趋势。 在本文中，我们对MVCC DBMS中关键的事务管理设计决策进行了研究：（1）并发控制协议（concurrency control protocol）（2）版本存储（version storage）（3）垃圾回收（garbage collection）（4）索引管理（index management）。对于每一个主题，我们都描述了内存式DBMS中的最先进的实现，并讨论了它们的做出的权衡。我们还重点介绍了阻止它们扩展以支持更多线程数和更复杂的负载的问题。作为调研的一部分，我们在内存式MVCC DBMS Peloton[5]中实现了所有的这些方法。这为我们提供了可以比较这些实现的统一的平台，且不受没实现的架构设计所影响。我们在40核的机器上部署了Peloton，并通过两个OLTP benchmark对其进行评估。我们的分析确定了对我们的实现造成压力的场景，并讨论了缓解它们的方式（如果可能的话）。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:3:0","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"2. 背景 我们首先提供了MVCC的上层概念的总览。然后讨论了DBMS追踪事务与维护版本信息用的元数据。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:4:0","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"2.1 MVCC总览 事务管理策略让终端用户能够通过多个程序访问数据库且让每个用户以为自己在一个单独的专用系统上执行[9]。它确保了DBMS的原子性和隔离性的保证。 对于现代数据库程序来说，多版本系统有一些优势。其中最重要的是，多版本系统比单版本系统能支持更高的并发。例如，MVCC DBMS允许在一个事务读一个对象的较旧的版本的同时另一个事务更新该对象。这一点十分重要，因为数据库执行只读查询的同时读写事务能够继续更新它。如果DBMS永远都不移除旧版本，那么系统就能够支持“时间旅行”操作，让应用程序能够像在过去的某个时间点一样查询数据库的快照[8]。 以上的好处让MVCC成为近些年DBMS的实现中最流行的选择。表1 提供了过去30年中MVCC的实现的总结。然而，在DBMS中实现多版本有很多种方式，每种方法都会产生不同的额外计算与存储开销。这些设计上的决策也高度互相依赖。因此，判断哪些设计决策比其它的好且为什么比其它的好并非易事。而在磁盘不再成为瓶颈的内存式DBMS中更是这样。 表1 对商业和研究用MVCC DBMS中的设计决策的总结。每个系统的“Year”（Oracle除外）是它首次发布或宣布的时间。对于Oracle，“Year”是其首次包含了MVCC的年份。除了Oracle、MySQL和Postgres外，所有的系统都宣称它们的数据库的主要存储位置是内存。表1 对商业和研究用MVCC DBMS中的设计决策的总结。每个系统的“Year”（Oracle除外）是它首次发布或宣布的时间。对于Oracle，“Year”是其首次包含了MVCC的年份。除了Oracle、MySQL和Postgres外，所有的系统都宣称它们的数据库的主要存储位置是内存。 \" 表1 对商业和研究用MVCC DBMS中的设计决策的总结。每个系统的“Year”（Oracle除外）是它首次发布或宣布的时间。对于Oracle，“Year”是其首次包含了MVCC的年份。除了Oracle、MySQL和Postgres外，所有的系统都宣称它们的数据库的主要存储位置是内存。 在接下来的章节中，我们将讨论这些设计决策实现上的问题和性能权衡点。接着我们在第7章对它们进行了全面的评估。我们注意到，本文中仅考虑了串行事务执行。尽管日志和恢复是DBMS架构中很重要方面，但是我们的研究中并没有包括它们，因为它们与单版本系统中的没什么不同，且内存式DBMS日志已经在别处被研究过了[39, 49]。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:4:1","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"2.2 DBMS元数据 无论实现方式如何，MVCC DBMS都会维护用于事务和数据库元组的通用的元数据。 事务： 当事务$T$首次进入DBMS系统时，DBMS会为该事务分配一个唯一、单调递增的时间戳作为它的标识符（$ T_{id} $）。并发控制协议使用这种标识符来标记事务访问的元组的版本。一些协议还用它作为事务串行的顺序。 元组： 如图1所示，每个“物理”版本在它的头部中都包括4个元数据字段，DBMS用它来协调并发事务的执行（下一章中讨论的一些并发控制协议包括了额外的字段）。txn-id字段被用作版本的写入锁。如果元组没被锁定写入，那么该元组的这一字段会被置零。大部分的DBMS都使用64位的txn-id，这样它可以通过一次compare-and-swap（CaS）指令原子性的更新该值。如果标识符为$ T_{id} $的事务$T$想要更新元组$A$，那么DBMS会检查$A$的txn-id字段是否为零。如果是，那么DBMS会通过一个CaS指令将txn-id置为$ T_{id} $[27, 44]。如果一个事务试图更新$A$，当这一txn-id既不是0也不等于该事务的$ T_{id} $时，该事务会被打断。接下来的两个元数据字段是begin-ts和end-ts，它们记录了表示元组版本生命周期的时间戳。这两个字段最初都会被置零。DBMS会在事务删除元组时把改元组的begin-ts置为INF。最后一个元数据字段是一个保存相邻（前一个或后一个）版本的地址的指针pointer（如果有的话）。 图1 元组格式——元组的一个物理版本的基本布局。图1 元组格式——元组的一个物理版本的基本布局。 \" 图1 元组格式——元组的一个物理版本的基本布局。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:4:2","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"3. 并发控制协议 每个DBMS都有一个用来协调并发事务执行的并发控制协议 [11]。该协议确定了（1）在数据库运行时中，是否允许某个事务访问或修改一个特定的元组版本（2）是否允许某个事务提交它的修改。尽管这些协议的基本原理从1980年代就没改变过，但是在没有磁盘操作的多核和内存为主的配置下，它们的性能特征已经发生了巨大的变化[42]。因此，有些新兴的高性能的DBMS变体移除了锁（lock）/闩（latch）和中央的数据结构，并为字节可寻址存储（byte-addressable storage）进行了优化。 本章中，我们描述了MVCC DBMS的4中核心并发控制协议。我们仅考虑了使用元组级锁的协议，因为这足以确保串行化执行。我们忽略了范围查询，因为多版本没有为防止幻读带来任何好处[7]。已有的提供了串行事务处理的方法，（1）或者在索引上使用了额外的闩[35, 44]，（2）或者在事务提交时使用了额外的校验步骤[27]。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:5:0","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"3.1 时间戳排序（Timestamp Ordering）——MVTO 1979年的MVTO算法被认为是最初的并发控制协议[38, 39]。该方法的核心在于使用事务的标识符（$ T_{id} $）来预计算它们的串行顺序。除了章节2.2中描述的字段以外，其版本头中还包括最后一次读取了它的事务的标识符（read-ts）。当事务视图读取或更新一个被其它事务持有其写入锁的版本时，DBMS会打断该事务。 当事务$T$在逻辑元组$A$上调用了一个读操作时，DBMS会搜索一个能使该$ T_{id} $在其begin-ts和end-ts字段的范围间的物理版本。如**图2a**所示，如果版本$A_x$的写入锁没有被另一个活动的事务持有（即，txn-id的值为0或等于$ T_{id} $），那么$T$会被允许读取该版本，因为MVTO永远不会允许一个事务读取未提交的版本。一旦事务读取了$A_x$，如果$A_x$的read-ts字段的当前值小于$ T_{id} $，那么DBMS会将其置为$ T_{id} $。否则，该事务将读取一个较旧的版本，且不会更新该字段。 图2 并发控制协议——协议如何处理先执行READ然后执行UPDATE的事务的示例。图2 并发控制协议——协议如何处理先执行READ然后执行UPDATE的事务的示例。 \" 图2 并发控制协议——协议如何处理先执行READ然后执行UPDATE的事务的示例。 通过MVTO，事务总能更新元组的最新版本。如果（1）没有持有$B_x$的写入锁的活动的事务，且（2）$ T_{id} $的值比$B_x$的read-ts字段大，事务$T$会创建一个新版本$ B_{x+1} $。如果这些条件被满足，那么DBMS会创建一个新版本$ B_{x+1} $并将其txn-id置为$ T_{id} $。当$T$提交时，DBMS会分别将$ B_{x+1} $的begin-ts和end-ts字段置为$ T_{id} $和INF，并将$B_x$的end-ts字段置为$ T_{id} $。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:5:1","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"3.2 乐观并发控制（Optimistic Concurrency Control）——MVOCC 接下来的协议基于1981年提出的乐观并发控制策略（OCC）[26]。OCC背后的动机是，DBMS假设事务冲突的可能性不大，因此当事务读取或更新元组时没有必要获取元组上的锁。这减少了事务持有锁的总时间。为了使原始的OCC协议适应多版本，需要对它作出一些修改[27]。其中最重要的是，DBMS不会为事务维护一个私有的工作空间（workspace），因为元组的版本信息已经防止了事务读取或更新应对它们不可见的版本。 MVOCC协议将事务分为了3个阶段。当事务开始时，它处于读阶段（read phase） 。这是事务在数据库上调用读或更新操作的阶段。与MVTO相似，为了在元组$A$上执行读操作，DBMS首先会基于元组的begin-ts和end-ts字段搜索一个可见的版本$A_x$。如果版本$A_x$的写入锁没被获取，那么$T$会被允许更新该版本。在多版本配置下，如果事务更新了版本$B_x$，那么DBMS会创建版本$ B_{x+1} $并将其txn-id置为$ T_{id} $。 当事务告知DBMS它要提交时，事务会进入校验阶段（validation phase） 。首先，DBMS会为该事务分配另一个时间戳（$ T_{commit} $）来确定该事务读取的元组的集合是否被一个已提交的事务更新了。如果该事务通过过了这些检查，那么它会进入*写阶段（write phase）* ，在这一阶段DBMS会安装（install）所有的新版本且将它们的begin-ts置为$ T_{commit} $并将end-ts置为INF。 事务只能更新元组的最新的版本。但是事务在其它事务创建新版本的提交前，它都不能读该新版本。读到过时版本的事务仅会在校验阶段发现它应该终止。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:5:2","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"3.3 两阶段锁（Two-phase Locking）——MV2PL 该协议使用了两阶段锁（2PL）的方法[11]来确保事务的串行性。每个事务在被允许读取或修改逻辑元组的当前版本前，需要获取适当的锁。在基于磁盘的DBMS中，锁会与元组分开存储，这样锁永远不会被换出（swap）到磁盘中。而在内存式DBMS中，分开存储时不必要的，因此使用MV2PL时，锁被嵌入在了元组的头部中。元组的写入锁 是txn-id字段。而对于读取锁 ，DBMS使用read-cnt字段对读取该元组的活动的事务计数。尽管不是必需的，DBMS还是将txn-id和read-cnt装入连续的64位字中，这样DBMS可以使用一个CaS指令来同时更新它们。 为了对元组$A$执行读操作，DBMS会通过将事务的$ T_{id} $和元组的begin-ts进行比较来搜索一个可见的版本。如果它找到了一个有效地版本，DBMS会在它的txn-id字段等于零时（这意味着没有其它的事务持有着它的写入锁）将该元组的read-cnt字段加一。类似地，当事务仅在版本$B_x$的read-cnt和txn-id都被置为零使才被允许更新它。当事务提交时，DBMS会为它分配一个唯一的时间戳$ T_{commit} $，其被用于更新由该事务创建的其它版本的begin-ts字段，并随后释放该事务所有的锁。 2PL协议间的关键区别在于它们如何处理死锁。之前的研究表明无等待（no-wait） 策略[9]是可伸缩性最好的防死锁技术[48]。使用这一策略，DBMS会在事务不能获取元组的锁时立刻打断它（与等待并看锁是否被释放相反）。因为事务永远不会等待，DBMS没必要使用一个后台线程来检测并打破死锁。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:5:3","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"3.4 串行验证者（Serialization Certifier） 在最后一个协议中，DBMS会维护一个串行图（serialization graph）来检测并移除当前事务形成的“危险结构”[12, 20, 45]。在较弱的隔离界别之上可以采用基于验证着的方法，这能提供更好的性能但是允许出现某些异常情况。 第一个提出验证者方法的是可串行快照隔离（serializable snapshot isolation，SSI）[12]。这种方法通过避免写倾斜的异常（write-skew anomly）来确保串行性以提供快照隔离。SSI使用事务的标识符来索索元组的可见版本。事务仅当元组的txn-id字段被置为零时才能更新其版本。为了确保串行性，当事务创建了一个元组的新版本同时该元组之前的版本被另一个事务读取时，DBMS会在其内部的图中追踪反依赖（anti-dependency） 边。DBMS会为每个事务维护一些标识（flag）以追踪反依赖边的入度和出度。当DBMS检测到两个事务间有两个连续的反依赖边时，它会打断其中一个事务。 串行安全网络（serial safety net，SSN）是一种新兴的基于验证着的协议[45]。不像仅适用于快照隔离的SSI，SSN能在强度至少与READ COMMITTED相同的任何隔离界别中工作。它还是用了更精确的异常检测机制，减少了被不必要打断的事务的数量。SSN将事务依赖信息编码为元数据字段，并通过计算一个能汇总在$T$之前提交但必须在T之后串行执行的“危险”事务的“低水位标记（low watermark）”来校验事务$T$的一致性[45]。SSn能够减少错误打断的数量，这使它更适用于只读或以读取为主的事务负载。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:5:4","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"3.5 探讨 这些协议处理冲突的方式不同，因此在某些负载下，一些协议会比其它协议更好。MV2PL使用每个版本的读取锁来记录读取。因此，对一个元组版本执行读或写的事务会导致另一个试图对该版本做相同操作的事务终止。而MVTO使用了每个版本的read-ts字段来记录读取。MVOCC在处理读操作时不会更新元组版本头部的任何字段。这避免了线程间不必要的协调，且读取某一版本的事务不会导致其它更新相同版本的事务被打断。但是MVOCC要求DBMS检验事务的读集合以验证该事务读操作的正确性。这可能导致长期运行的只读事务长时间不会被执行（译注：原文为“starvation”）[24]。而验证者协议因不需要验证读取而减少了事务打断，但是它们的反依赖检查策略可能会带来额外的开销。 一些协议优化了上面的协议以改进它们在MVCC DBMS中的功效[10, 27]。一种方法让事务能够预先读取（speculatively） 其它事务创建的未提交的版本。而作为权衡，协议必须追踪事务的读依赖以确保可以串行排序。每个工作线程会维护一个依赖计数器（dependency counter） ，以计数该事务读取的未提交的数据对应的事务的个数。事务仅在它的依赖计数器为零时才被允许提交，因此，在事务提交时DBMS会遍历其以来列表并减小所有等待它完成的事务的计数器。类似地，另一种优化机制让事务能够预先更新（eagerly update） 被其它未提交的事务读取的版本。这一优化也需要DBMS维护中央数据结构以追踪事务间的依赖。一个事务仅当所有它依赖的事务已经提交后才能提交。 以上描述的两种优化都能减少某些负载下不必要的打断数量，但是它们都收级联终止（cascading abort）影响。此外，我们发现维护中央数据结构可能成为主要的性能瓶颈，这会让DBMS不能扩展到几十个核心的配置。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:5:5","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"4. 版本存储 在MVCC下，当事务更新一个元组时DBMS总是会为该元组构造一个新的物理版本。DBMS的版本存储策略（version storage scheme） 决定了系统如何保存这些版本与每个版本包含什么信息。DBNS使用元组的pointer字段创建了一个被称为版本链（version chain） 的无闩链表。版本链让DBMS能够定位所需的对事务可见的元组版本。正如我们下面讨论的一样，链头（HEAD）既可能是最新版本也可能是最旧版本。 现在我们将更详细地描述这些策略。我们的讨论将聚焦于策略对UPDATE操作的权衡，因为这时DBMS处理版本控制之处。DBMS将新元组插入到表时不必更新其它的版本。同样，DBMS通过在当前版本的begin-ts字段设置一个标识来删除元组。在后续的章节中，我们将讨论这些存储策略对DBMS如何执行垃圾回收与DBMS如何在索引中维护指针的影响。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:6:0","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"4.1 仅追加存储（Append-only Storage） 在第一个策略中，表的所有元组版本被保存到相同的存储空间中。这种方法在Postgres与Hekaton、NuoDB、和MemSQL等内存式DBMS中使用。为了更新已有的元组，DBMS首先从表中为新版本请求一个空的槽（slot）。然后它会将当前版本的内容复制到新版本中。最后，它会修改应用到元组中新配的的版本槽上。 仅追加策略的关键决策在于DBMS如何对元组的版本链进行排序。因为不可能维护无闩双向链表，所以版本链仅指向一个方向。这一顺序对事务修改元组时DBMS更新索引的频率有影响。 从老到新（Oldest-to-Newest，O2N）： 在这种顺序下，链头是元组现存的最老的版本（如图3a所示）。这一版本可能对任何活动的事务都不可见，但是DBMS还没有回收它。O2N的优势在于，在元组被修改时，DBMS不需要更新索引使其指向元组的新版本。但是DBMS在查询处理期间可能要遍历很长的版本链来找到最新版本。因为这一操作是pointer-chasing（译注：指遍历由指针链接在一起的数据结构，因为下一个元组总是不在缓存中，因此在遍历过程中会不断引起内存操作）的且这一操作会因读取了不需要的版本而污染CPU缓存，所以这一操作很慢。因此，通过O2N实现良好的性能高度依赖系统对旧版本剪枝的能力。 从新到老（Newest-to-Oldest，N2O）： 这种顺序将元组最新的版本作为版本链的链头保存（如图3b所示）。因为大部分事务都访问元组的最新版本，所以DBMS不必遍历整个链。然而，其缺点是在于，每当元组被修改后链头都会变化。随后DBMS会更新该表的所有索引（索引的前项（primary）和后项（secondary）都要更新）以指向新版本。正如我们在章节6.1中讨论的那样，可以通过一个间接层来避免这一个问题，该层提供了一个将元组的最新版本映射到物理地址的位置。在这种配置下，索引指向元组的映射条目而不是它们的物理位置。这在有很多后项索引的表中表现良好，但是增加了额外的存储开销。 仅追加存储的另一个问题是，如何处理非内联（non-inline）属性（例如BLOB）。考虑一个有两个属性的表（一个是整型，一个是BLOB）。在仅追加策略下，当一个事务更新该表的一个元组时，DBMS会创建该BLOB属性的一份拷贝（即使事务么有修改它），且随后新版本会指向这份拷贝。因为这创建了冗余的拷贝，所以这一操作很浪费。为了避免这一个问题，一种优化是让相同元组的多个物理版本指向同一个非内联数据。DBMS维护该数据的引用计数来确保其值仅在它们不再被任意版本引用时才会被删除。 图3 版本存储——该图概括了在MVCC DBMS中这些策略如何组织不同诗句结构中的版本与它们的指针如何创建版本链。注意仅追加方案的两个变体的版本链顺序有所不同。图3 版本存储——该图概括了在MVCC DBMS中这些策略如何组织不同诗句结构中的版本与它们的指针如何创建版本链。注意仅追加方案的两个变体的版本链顺序有所不同。 \" 图3 版本存储——该图概括了在MVCC DBMS中这些策略如何组织不同诗句结构中的版本与它们的指针如何创建版本链。注意仅追加方案的两个变体的版本链顺序有所不同。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:6:1","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"4.2 时间旅行存储（Time-Travel Storage） 下一种存储策略与仅追加方法相似，除了该策略中旧版本被保存在单独的表中。DBMS在主表（main table）中为每个元组维护一个master版本，并在单独的时间旅行表（time-travel table）中维护同一个元组的多个版本。在一些DBMS中（如SQL Server），master版本时元组的当前版本。其他系统（如SAP HANA）将元组最旧的版本作为master版本保存，以提供快照隔离（snapshot isolation）[29]。这会增加GC时的额外维护开销，因为在DBMS剪掉当前的master版本时，需要将数据从时间旅行表拷贝回主表中。为了简单起见，我们仅考虑第一种时间旅行方法，即master版本总是在主表中的方法。 为了更新一个元组，DBMS首先会在时间旅行表中获取一个槽，然后将master版本拷贝到这个位置上。然后，它会修改保存在主表上的master版本。因为索引总是指向master版本，所以索引不会受版本链更新的影响。因此，这避免了每次事务更新一个元组时维护数据库索引的额外开销，且对于访问元组当前版本的查询来说非常理想。 这种策略与仅追加方法一样，也受非内联属性问题影响。我们之前描述的数据共享优化同样适用于此。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:6:2","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"4.3 增量存储（Delta Storage） 在最后一种策略中，DBMS在主表中维护元组的master版本 ，并在一个独立的增量存储（delta storage） 中维护增量版本（delta storage） 的序列。这一存储在MySQL和Oracle中被称为回滚段（rollback segment） ，在HyPer中也有使用。大部分已有的DBMS将元组的当前版本保存在主表中。为了更新已有的元组，DBMS会从增量存储中获取一个连续的空间，来创建新的增量版本。这一增量版本中包含被修改的属性的原始值而不是整个元组。随后，DBMS直接对主表中的master版本就地更新。 这一策略对修改元组属性的子集的UPDATE操作来说很理想，因为它减少了内存分配。然而这种方法在读敏感负载中会导致更高的开销。为了执行访问一个元组的多个属性的读操作，DBMS不得不遍历版本链以拉取该操作访问的每个属性的数据。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:6:3","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"4.4 探讨 这些策略的不同特征影响了它们在OLTP负载下的表现。因此，这些策略中没有一个能在任何负载类型下都有最优的性能。仅追加策略在执行大规模扫描的分析查询中表现更好，因为不同版本在内存中连续存储，这能够减少CPU缓存的失配且很适合硬件的预取（prefetch）。但是访问元组较旧版本的查询有更高的开销，因为DBMS会顺着元组的版本链来查找适当的版本。仅追加策略还会将物理版本暴露给索引结构，这会造成额外的索引管理开销。 所有的这些存储策略都需要DBMS从中央数据结构中为每个事务分配内存（即表、增量存储）。多个线程会在同事访问并更新中央存储，因此这回导致访问竞争。为了避免这一问题，DBMS可以为每个中央数据结构（即表、增量存储）分别维护单独的内存空间，并以固定大小的增量扩展它们。随后每个工作线程将获取单独的内存空间。这本质上是对数据库分区，从而消除集中地争用点。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:6:4","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"5. 垃圾回收 因为MVCC会在事务更新元组时创建新版本，如果不回收不再需要的版本，那么系统会耗尽空间。这还会增加查询执行的时间，因为DBMS会花更多的时间来遍历较长的版本链。因此，MVCC DBMS的性能高度依赖它垃圾回收（garbage collection，GC） 组件在确保事务安全的条件下回收内存的能力。 GC过程被分为三个步骤：（1）探测过期的版本（2）移除这些版本与它们香瓜你的链和索引的链接（3）回收它们的存储空间。 如果一个版是无效的版本（即由被打断的事务创建）或对任何活动的事务都不可见，那么DBMS会认为该版本是过期的。对于后者，DBMS会检查版本的end-ts是否比所有活动的事务的$ T_{id} $都小。DBMS会维护一个中央数据结构来追踪这一信息，但是这会在多核系统中成为可伸缩性的一个瓶颈[27, 48]。 内存式的DBMS可以通过粗粒度的（coarse-grained）基于epoch（epoch-based）的内存管理来追踪被事务创建的版本已解决这一问题[44]。该方法中，系统总有一个活动的epoch和一个早期epoch的FIFO队列。在一定时间后，DBMS会将当前活动的epoch移动到早期epoch队列中，随后创建一个新的活动epoch。epoch的变换可以由后台线程执行，也可以由DBMS的工作线程以协作的方式执行。每个epoch都有对分配给它的事务数量的计数。DBMS会将每个新事务注册到活动的epoch中并增加其计数器。当事务完成时，DBMS将它从它的epoch中移除（其epoch可能已经不再是当前活动的epoch），并减小其计数器。如果一个非活动的epoch的计数器变为零且其之前的epoch也没有活动的事务了，那么DBMS可以安全地回收在这个epoch中被更新的版本。 MVCC中的GC有两种实现，它们的不同之处在于DBMS如何查找过期的版本。第一种方法是元组级（tuple-level） GC，DBMS会检验单个元组的可见性。第二种是事务级（transaction-level） GC，DBMS会检查被完成的事务创建的所有版本是否可见。需要注意的重要的点是，我们下面讨论的每个GC方案并非都与所有的版本存储策略兼容。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:7:0","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"5.1 元组级垃圾回收 在这一方法中，DBMS会通过两种方法之一来检验每个单独的元组版本的可见性： 后台清理（Background Vaccuuming，VAC）： DBMS使用后台线程定期扫描数据库以查找过期的版本。如表1所示，这时大多数MVCC DBMS中通用的方法，因为它实现起来很简单，且在所有的版本存储策略中都有效。但是这一机制无法为大型数据库伸缩，特别是GC线程数很少的数据库。一个更具有伸缩性的方法是，事务将无效的版本注册到无闩数据结构中[27]。随后GC线程使用之前描述的基于epoch的策略回收这些过期的版本。另一个优化是，DBMS维护脏块的bitmap，这样清理线程不需要检验从上一次GC后没被修改过的块。 协同清理（Cooperative Cleaning，COOP）： 当执行事务时，DBMS会遍历版本链以定位可见的版本。在遍历时，它会识别过期的版本并将它们记录在全局数据结构中。这一方法的伸缩性很好，因为GC线程不再需要探测过期的版本，但是它只适用于O2N的仅追加存储。其一个额外的挑战是，如果事务不遍历某个元组的版本链，那么系统永远都不会移除其过期的版本。这一问题在Hekaton中被称为“灰尘角（dusty corners）”[16]。DBMS通过定期在一个单独的线程中执行类似VAC的完整GC来克服这一问题。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:7:1","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"5.2 事务级垃圾回收 在这种GC机制下，DBMS会以事务级的粒度回收存储空间。这是用于所有的版本存储策略。当事务生成的版本对任何活动中的事务都不可见时，DBMS会认为该事务过期。在一个epoch结束后，所有被属于该epoch的事务生成的版本可以被安全地移除。这一策略比元组级GC策略更简单，且能与本地事务存储优化（transcation-local storage optimization）配合的更好（章节4.4），因为DBMS会立刻回收事务的存储空间。然而，这一方法的缺点是，DNMS需要为每个epoch追踪事务的读/写集合，而不是仅使用epoch的成员计数器。 图4 垃圾回收——检查数据库中过期版本方式的总览元组级GC死奥妙表的版本链，而事务级GC使用事务的写集合。图4 垃圾回收——检查数据库中过期版本方式的总览元组级GC死奥妙表的版本链，而事务级GC使用事务的写集合。 \" 图4 垃圾回收——检查数据库中过期版本方式的总览元组级GC死奥妙表的版本链，而事务级GC使用事务的写集合。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:7:2","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"5.3 探讨 使用后台清理的元组级GC是MVCC DBMS中最通用的实现方式。无论采用哪种方案，增加专用的GC线程数都可以加速GC过程。长时间运行的事务会导致DBMS性能下降。因为在这种事务的生命周期中生成的所有版本只有在该事务完成后才能被移除。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:7:3","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"6. 索引管理 所有的MVCC DBMS都会将数据库的版本信息和它的索引分开。也就是说，索引中键存在意味着该键下的某个版本存在，但是索引条目不包含有关元组的哪个版本与之匹配的信息。我们定义一个索引条目（index entry） 为一个键值对（key/value pair），其中键 是元组的一个（或多个）有索引的属性，值 是指向该元组的指针。DBMS会沿着该指针找到元组的版本链，并扫描该版本链以定位对事务可见的版本。DBMS永远都不会从一个索引中得到“假阴性（false negative）”的结果（即收到键值对不存在的结果，但实际上键值对存在），但是可能会得到“假阳性（false positive）”的结果（即收到键值对存在的结果，但实际上键值对不存在），因为索引中的键指向的版本可能对某个事务不可见。 主键索引（primary key index）总是指向元组的当前版本。但是DBMS更新主键索引的频率取决于它的版本存储策略是否会在元组被更新时创建新版本。例如，在增量策略下，主键索引总是指向主表中元组的master版本，因此索引不需要被更新。对于仅追加策略，这取决于版本链的顺序：N2O要求DBMS在每当有新版本被创建时更新主键索引。如果元组的主键（译注：的值）被修改，那么DBMS会将该修改通过先DELETE再INSERT的方式应用到索引中。 对于辅助索引（secondary index），这更为复杂，因为索引条目的键和指针都可以被修改。MVCC DBMS中的两种辅助索引管理策略与对这些指针的内容的管理策略有所不同。第一种方法使用了逻辑指针（logical pointers） ，其间接地映射到物理版本的位置上。与之相反的是物理指针（physical pointers） 的方法，其值就是元组的确切版本的位置。 图5 索引管理——MVCC中两种将键映射到元组的方式中，一种使用了指向版本链链头的非间接层的逻辑指针，一种使用而来指向一个确切版本的物理指针。图5 索引管理——MVCC中两种将键映射到元组的方式中，一种使用了指向版本链链头的非间接层的逻辑指针，一种使用而来指向一个确切版本的物理指针。 \" 图5 索引管理——MVCC中两种将键映射到元组的方式中，一种使用了指向版本链链头的非间接层的逻辑指针，一种使用而来指向一个确切版本的物理指针。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:8:0","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"6.1 逻辑指针 使用逻辑指针的主要思想是，DBMS使用了固定的标识符，每个元组的索引条目的标识符不会改变。那么，如图5a所示，DBMS使用了一个将元组的标识符映射到其版本链链头的间接层。这避免了当元组被修改时（即使被索引的属性没被修改）必须更新一个表所有指向新物理地址的索引的问题。每次只需要修改映射条目。但是因为索引指向确切的版本，DBMS需要从版本链的链头开始遍历来找到可见的版本。这种方法适用于所有的存储策略。就我们目前讨论的来说，这一映射的实现有两种选择： 主键（Primary Key，PKey）： 在这种方法中，标识符和对应的元组的主键相同。当DBMS从辅助索引中检索到一个条目时，它会在表的主键索引中执行另一次查找，来定位版本链的链头。如果辅助索引的属性与主键有重叠，那么BDMS不必在每个条目中保存完整的主键。 元组Id（Tuple Id，TupleId）： Pkey指针的一个缺点是，随着元组主键的增长，数据库的存储开销会越来越大，因为每个辅助索引都有主键的一份完整的拷贝。除此之外，因为大多数数据库的主键索引都使用一种“保序（order-preserving）”的数据结构，所以执行额外的查找的开销取决于条目的数量。另一种方式是，使用一个唯一的64位元组标识符替代主键，病使用一个独立的无闩哈希表来维护到元组版本链链头的映射信息。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:8:1","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"6.2 物理指针 在第二种策略下，BDMS在索引条目中保存版本的物理地址。这种方法仅适用于仅追加存储，因为DBMS会将版本保存到同一张中，所以所有索引都能指向这些版本。当更新表中任何元组时，DBMS会将新创建的版本插入到所有的辅助索引中。通过这种方式，DBMS不需要比较辅助索引和所有索引的版本，就能够从辅助索引中搜索元组。如MemSQL和Hekaton的一些MVCC DBMS就采用了这一策略。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:8:2","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"6.3 探讨 和其它设计决策类似，这些索引管理策略在不同负载下的表现各异。逻辑指针的方法更适用于写入敏感型负载，因为DBMS只需要在事务修改索引的属性时才需要更新辅助索引。然而，读取可能会变慢，因为DBMS会遍历版本链并执行额外的键比较。而物理指针的方法更适用于读取敏感型负载，因为索引条目会指向确切的版本。但是对于更新操作来说这样会更慢，因为该策略需要DBMS讲每个新版本插入到所有的辅助索引中，这会使更新操作变得更慢。 最后，一个很有趣的点是，除非元组的版本信息嵌入在每个索引中，否则在MVCC DBMS中不可能仅使用索引进行扫描。系统必须始终从元组本身中检索这一信息，以确定每个元组的版本是否对某个事物可见。NuoDB通过将头部的元数据与元组数据分开保存的方式，减少了版本检查时读取的数据总量。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:8:3","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"7. 实验分析 我们现在将给出对本文中我们讨论的事务管理设计决策的分析。我们付出了很多努力来在Peloton DBMS[5]中实现了每种设计的最先进的版本。Peloton将元组保存在基于行（row-oriented）的无序内存堆中。它使用了libcuckoo[19]哈希表作为它的内部数据结构，并使用Bw-Tree[32]作为数据库索引。我们还使用了无闩变成技术[15]来优化Peloton的性能。我们将所有的事务作为在SERIALIZABLE隔离级别下的存储过程执行。Peloton使用了基于epoch的内存管理（见第五章），我们将它的epoch配置为40ms[44]。 我们将Peloton部署在了4插槽的Intel Xeon E7-4820服务器上，它有128GB的DRAM，运行64位Ubuntu 14.04操作系统。每个插槽上有10个1.9GHz的核心和25MB的L3缓存。 我们首先比较了并发控制协议。然后选择了总体上最佳的协议，用它来评估版本存储、垃圾回收、和索引管理策略。对于每个实验，我们执行了60秒的负载让DBMS热身，然后再执行120秒的负载来测量吞吐量。我们为每个实验执行5次，并汇报平均执行时间。我们在第八章中总结了我们的发现。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:9:0","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"7.1 Benchmarks 接下来，我们将描述在我们的评估中使用的负载。 YCSB： 我们队YCSB[14] Benchmark进行了修改，以对OLTP应用程序在不同工作负载下的配置进行建模。数据库包含一个有一千万个元组的表，每个元组有64位主键和64位整型属性。每个操作相对独立，也就是说，操作的输入不依赖之前操作的输出。我们使用了三种负载的混合来让每个事务的读取或更新操作的次数不同：（1）只读（100%读取）（2）读取敏感型（80%读取、20%更新）（3）更新敏感型（20%读取、80%更新）。我们还让对元组的读取或更新操作的属性数不同。访问元组的操作服从Zipfan分布，该分布受一个参数（$ \\theta $）控制，该参数影响争用量（contention）（即倾斜，skew），当$ \\theta = 1.0 $时表示最高的倾斜配置。 TPC-C： 该benchmark目前是测量OLTP系统性能的标准负载[43]。它建模了一个有9张表和5种事务类型的中心仓库订单处理程序。我们修改了原始的TPC-C负载，使其包括一种新的表扫描查询StockScan，它会扫描Stock表并计数每个仓库中项的个数。整个负载的争用量由仓库的个数控制。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:9:1","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"7.2 并发控制协议 首先我们比较使用了第三章中的并发控制协议的DBMS的性能。对于串行验证者策略，我们在快照隔离上实现了SSN（SI+SSN）[45]。我们让DBMS固定使用（1）N2O顺序的仅追加存储（2）事务级GC（3）逻辑映射索引指针。 首先，我们的实验使用YCSB负载来评估协议。我们首先调研了阻碍这些协议扩展的瓶颈。然后通过不同的负载争用比较了它们的性能。之后，我们展示了每种协议在处理既包括读写事务又包括只读事务的异构负载时的表现如何。最后，我们使用了TPC-C benchmark来每种协议在真实负载下的表现。 伸缩性瓶颈（Scalability Bottlenecks）： 本实验展示了协议在高线程数时的表现。我们通过配置，让只读YCSB负载执行的事务既有很短的（每个事务只有一个操作）也有很长的（每个事务有100个操作）。我们使用了较低的倾斜因子，并扩展线程数。 图6 伸缩性瓶颈——使用了每个事务的操作数不同的只读YCSB负载的并发控制协议吞吐量的比较图6 伸缩性瓶颈——使用了每个事务的操作数不同的只读YCSB负载的并发控制协议吞吐量的比较。 \" 图6 伸缩性瓶颈——使用了每个事务的操作数不同的只读YCSB负载的并发控制协议吞吐量的比较 短事务负载导致了图6a中展示的结果，所有的协议几乎都能线性地伸缩到24线程。所有的这些协议的主要瓶颈是缓存一致性（coherence）流量，这些流量来自于更新内存管理器的计数器和在事务提交时检查冲突的流量（尽管事务中没有写入）。SI+SSN性能低的原因在于，它为了追踪事务维护了一个中央哈希表。通过预分配与复用事务上下文结构[24]的方式可以移除该瓶颈。当我们把事务长度增加到100个操作时，如图6b所示，协议的吞吐量减小了高达30倍，但是能够线性地伸缩到40线程。这符合预期，因为执行的事务变少时，对DBMS中心数据结构的争用就减少了。 事务争用（Transaction Contention）： 接下来，我们在不同的争用级别下比较了这些协议。我们固定DBMS的线程数为40。我们使用了每个事务有10个操作的读取敏感型和更新敏感型负载。我们在每个负载的事务访问模式中使用了不同的争用级别（$ \\theta $）。 图7 事务争用——在YCSB负载的不同种负载（争用）混合下的并发控制协议（40线程）的对比。每个事务包含10个操作。图7 事务争用——在YCSB负载的不同种负载（争用）混合下的并发控制协议（40线程）的对比。每个事务包含10个操作。 \" 图7 事务争用——在YCSB负载的不同种负载（争用）混合下的并发控制协议（40线程）的对比。每个事务包含10个操作。 图7a展示了读取敏感型负载下的DBMS吞吐量。当$ \\theta $小于0.7时，我们可以看到所有的协议的吞吐量相似。当超过该争用级别时，MVOCC的性能减小达50%。这是因为MVOCC直到事务已经执行了其操作时，才能发现事务会因冲突而被打断。多版本策略在这一情况下没有优势。如图7b所示，尽管在争用增加时，我们在更新敏感负载的结果中能看到相同的性能下降，但是除了MV2PL之外，这些协议都没有太大的区别。除了MV2PL之外的这些协议除了“写入-写入”冲突的方法相似，同样，多版本策略对于减少这类冲突没有帮助。 异构负载（Heterogeneous Workload）： 在接下来的这个实验中，我们评估了异构YCSB负载，该负载由读写和只读的SERIALIZABLE事务混合而成。每个事务包含100个操作，每个操作访问一个不同的元组。 DBMS使用了20个线程来执行读写事务，我们使用不同的线程数来专门处理只读的查询。所有操作的访问模式的分布都采用了很大的争用配置（$ \\theta = 0.8 $）。我们首先在让应用程序预先声明查询是“只读的”的情况下执行了该负载，然后让应用程序预先声明查询是“只读的”再次执行一次。 图8 异构负载（不预先声明只读）——在YCSB（$ \\theta = 0.8 $）下的并发控制协议的比较。读写部分在20个线程中执行了更新敏感型的混合负载，而处理只读负载的线程数是变化的。图8 异构负载（不预先声明只读）——在YCSB（$ \\theta = 0.8 $）下的并发控制协议的比较。读写部分在20个线程中执行了更新敏感型的混合负载，而处理只读负载的线程数是变化的。 \" 图8 异构负载（不预先声明只读）——在YCSB（$ \\theta = 0.8 $）下的并发控制协议的比较。读写部分在20个线程中执行了更新敏感型的混合负载，而处理只读负载的线程数是变化的。 在程序不预先声明只读查询时，有很多有趣的趋势。第一，随着只读线程数增加，MVTO和MV2PL协议中读写事务的吞吐量下降（如图8a所示），而只读事务的吞吐量上升（如图8b所示）。这是因为这些协议平等地对待读取和写入。因为任何读取或写入一个元组的事务会阻塞其它访问同一个元组的事务，只读查询数量的增加导致了读写事务被打断的概率更高。因为这些冲突，MV2PL在只读线程增加到20时仅完成了少量的事务。第二，尽管随着只读线程数增加，MVOCC协议在读写部分达到了稳定的性能，但是它们在只读部分的性能分别比MVTO低了2倍和28倍。因为MVOCC没有读取锁，这导致只读队列长时间不会被执行（译注：原文为“starvation”）。第三，SI+SSN读写事务性能高出很多。这是因为它减少了DBMS因精确的一致性验证打断事务的概率。 图9 异构负载（预先声明只读）——在YCSB（$ \\theta = 0.8 $）下的并发控制协议对比。读写部分在20个线程上执行了更新敏感型混合负载，而只读取敏感型负载部分的线程数不同。图9 异构负载（预先声明只读）——在YCSB（$ \\theta = 0.8 $）下的并发控制协议对比。读写部分在20个线程上执行了更新敏感型混合负载，而只读取敏感型负载部分的线程数不同。 \" 图9 异构负载（预先声明只读）——在YCSB（$ \\theta = 0.8 $）下的并发控制协议对比。读写部分在20个线程上执行了更新敏感型混合负载，而只读取敏感型负载部分的线程数不同。 图9中的结果显示，当负载预先声明了只读部分时，各种协议的表现不同。第一，如图9b所示，他们的只读吞吐量相同，因为DBMS在执行这些查询的时候不会检查冲突。且如图9a所示，当只读查询与读写事务隔离时，它们对读写事务的吞吐量很稳定，因此执行这些只读事务不会增加数据争用。SI+SSN的表现再次成为了最好的，因为它减少了打断的概率，它比NV2PL和MVTO快了1.6倍。而MVOCC的性能最差，因为它因验证失败的打断率最高。 TPC-C： 之后，我们使用了将仓库数设置为10的TPC-C benchmark比较了这些协议。这一配置产生了很高的争用负载。 图10 TPC-C——在TPC-C负载下的不同并发控制协议的吞吐量和打断率对比。图10 TPC-C——在TPC-C负载下的不同并发控制协议的吞吐量和打断率对比。 \" 图10 TPC-C——在TPC-C负载下的不同并发控制协议的吞吐量和打断率对比。 如图10a所示，与其它协议相比，MVTO的性能高出了了45%~120%。SI+SSN生成的吞吐量比其它的协议都高，因为它需要检测反依赖，而不是通过类似OCC的一致性检查盲目地打断。MVOCC造成了计算的浪费，因为它近在验证阶段检测冲突。在图10b中，更有趣的是，不同的协议打断的事务也不同。MVOCC更有可能打断NewOrder事务，而在MV2PL中，Payment被打断的概率比NewOrder高出了6.8倍。这两种事务访问同一张表，同样，乐观的协议仅在NewOrder事务的验证阶段检测读取冲突。SI+SSN因为有反依赖追踪，所以它的打断率低；而在MVTO中，因为分配给每个事务的时间戳直接决定了它们的顺序，所以它能够避免错误的打断。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:9:2","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"7.3 版本存储 接下来我们评估DBMS的版本存储策略。首先，我们分析了仅追加存储中非内联属性的存储机制。然后，我们讨论了版本链的顺序是如何影响仅追加存储的DBMS的性能的。接下来，我们通过使用不同的YCSB负载，将仅追加策略与时间旅行策略和增量策略进行了比较。最后，我们使用TPC-C benchmark再次比较了这些策略。在所有的这些实验中，我们让DBMS都使用了MVTO协议，因为它在之前的实验中有最均衡的性能。 非内联属性（Non-Inline Attributes）： 第一个实验评估了在仅追加存储中保存非内联属性的不同机制的性能。在本实验中，我们使用了混合的YCSB负载，数据库变为包含了一个有一千万个元组的表，每个元组有一个64位主键和一个用100字节的非内联VARCHAR类型的属性表示的可变数字。我们在40个线程中使用了低争用因子（$ \\theta = 0.2 $）的读取敏感型和更新敏感型负载，其中每个事务执行10个操作。每个操作仅访问元组中的一个属性。 图11 非内联属性——对在YCSB负载下如何在仅追加存储策略中保存非内联属性的评估，其中有40个DBMS线程，元组中的属性的数量不同。图11 非内联属性——对在YCSB负载下如何在仅追加存储策略中保存非内联属性的评估，其中有40个DBMS线程，元组中的属性的数量不同。 \" 图11 非内联属性——对在YCSB负载下如何在仅追加存储策略中保存非内联属性的评估，其中有40个DBMS线程，元组中的属性的数量不同。 图11表明，为没被修改的非内联属性维护引用计数器总是会有更好的性能。在读取敏感型负载下，当有这些计数器的非内联属性的个数增加到50个的时候，DBMS的吞吐量比常规的“完整元组拷贝（full-tuple-copy）”策略高出了多达40%。这是因为DBMS避免了更新操作中的冗余数据拷贝。这一差异在更新敏感型负载下更为突出，其性能差异超过了100%，如图11b所示。 版本链顺序（Version Chain Ordering）： 第二个实验测量了在章节4.1描述的N2O和O2N的版本链顺序的性能。我们使用了事务级的后台清理GC，并在YCSB混合负载下比较了不同的顺序。我们将事务的长度设为10。我们固定了DBMS的线程数为40，并使用了不同的负载争用等级。 图12 版本链顺序——对仅追加存储策略下不同版本链顺序的评估，其使用了YCSB负载，使用了40个DBMS线程，并使用不同的争用等级。图12 版本链顺序——对仅追加存储策略下不同版本链顺序的评估，其使用了YCSB负载，使用了40个DBMS线程，并使用不同的争用等级。 \" 图12 版本链顺序——对仅追加存储策略下不同版本链顺序的评估，其使用了YCSB负载，使用了40个DBMS线程，并使用不同的争用等级。 如图12所示，在两种负载下，N2O顺序的性能总是比O2N的好。尽管DBMS会为每个新版本更新索引的指针，但是这与O2N遍历更长的链的开销相比相形见绌。版本链长度的增加意味着事务需要执行更长时间，从而增大了一个事务与另一个事务冲突的可能性。这种现象在最高的争用级别下（$ \\theta = 0.9 $）更为明显，其中N2O顺序的性能达到了2.4~3.4倍。 事务占用（Transaction Footprint）： 在下一项存储策略的对比中，我们在元组中使用了不同的属性数。我们在40个线程上使用了低争用的（$ \\theta = 0.2 $）的YCSB负载，其中每个事务执行10个操作。每个读取或更新操作仅访问或修改元组中的一个属性。我们使用了N2O顺序的仅追加存储。对于所有的版本存储策略，我们都分别分配了单独的内存空间，以减少内存分配的开销。 图13 事务占用——对在YCSB负载下（$ \\theta = 0.2 $）的不同版本存储策略的评估，其使用了40个DBMS线程，每个事务中更新操作的比例不同。图13 事务占用——对在YCSB负载下（$ \\theta = 0.2 $）的不同版本存储策略的评估，其使用了40个DBMS线程，每个事务中更新操作的比例不同。 \" 图13 事务占用——对在YCSB负载下（$ \\theta = 0.2 $）的不同版本存储策略的评估，其使用了40个DBMS线程，每个事务中更新操作的比例不同。 如图13a所示，在表有10个属性时，仅追加策略和增量策略的性能相似。同样，仅追加策略和时间旅行策略的吞吐量几乎相同。图13b的结果表明，当表有100个属性时，查分策略的性能达到了仅追加策略和时间旅行策略的2倍，因为它使用的内存更少。 属性修改（Attributes Modified）： 我们将表中的属性数量修改到100，并在事务的每个更新操作中修改不同数量的属性。我们在40个线程上使用了低争用因子（$ \\theta = 0.2 $）的读取敏感型负载和更新敏感型负载，每个事务执行10个操作。像之前的实验一样，每个读操作访问一个属性。 图14 属性修改——对在YCSB负载下（$ \\theta = 0.2 $）的不同版本存储策略的评估，其使用了40个DBMS线程，每个更新操作中修改的元组的属性数不同。图14 属性修改——对在YCSB负载下（$ \\theta = 0.2 $）的不同版本存储策略的评估，其使用了40个DBMS线程，每个更新操作中修改的元组的属性数不同。 \" 图14 属性修改——对在YCSB负载下（$ \\theta = 0.2 $）的不同版本存储策略的评估，其使用了40个DBMS线程，每个更新操作中修改的元组的属性数不同。 如图14所示，无论修改的属性数量如何，仅追加策略和时间旅行策略的性能都很稳定。正如预期的一样，当修改的属性的数量很少时，增量策略的性能最好，因为它对每个版本拷贝的数据更少。但是随着更新操作的范围扩大，它的性能就与其它的相同，因为每个增量数据拷贝的数据量相同。 为了测量属性修改对读取的影响如何，我们在每个操作中访问了不同数量的属性。如图15a所示，当更新操作仅修改一个（随机的）属性时，读取的属性数的增加大幅影响了增量策略。这是符合预期的，因为DBMS必须用更多的时间遍历版本链以搜索目标列。仅追加存储和时间旅行存的性能也会下降，因为内部的socket通信开销与每个读操作访问的总数据量成正比地增长。这与图15b中观察到的结果一致，其中，更新操作会修改元组的所有属性，每个读操作访问的属性的增加会导致所有存储策略的性能下降。 内存分配（Memory Allocation）： 接下来的实验评估了内存分配会如何影响版本策略的性能。我们在40个线程上使用了低争用因子（$ \\theta = 0.2 $）的YCSB负载。每个事务执行10个操作，每个操作仅访问元组中的一个属性。我们改变了不同的内存空间的个数，并测量DBMS的吞吐量。DBMS每次按512KB大小扩展内存空间。 图16 内存分配——在YCSB负载下，对内存分配方式给不同版本存储策略的影响评估，其使用了40个DBMS线程，内存空间的数量不同。图16 内存分配——在YCSB负载下，对内存分配方式给不同版本存储策略的影响评估，其使用了40个DBMS线程，内存空间的数量不同。 \" 图16 内存分配——在YCSB负载下，对内存分配方式给不同版本存储策略的影响评估，其使用了40个DBMS线程，内存空间的数量不同。 如图16所示，无论DBMS分配的内存空间数如何，增量存储策略的性能都很稳定。相反，随着内存空间的数量从1个增加到20个时，仅追加策略和时间旅行策略的吞吐量的吞吐量提高了1.6~4倍。这是因为增量存储仅拷贝元组中被修改的属性，其需要的内存总量有限。相反，另外两种存储策略会频繁获取新的槽来保存每个新增的元组版本的完整的拷贝，这增加的DBMS内存分配的开销。 TPC-C： 最后，我们使用TPC-C比较了不同的策略。我们设置仓库的数量为40，并逐渐增大线程数来测量整个系统的吞吐量和StockScan查询的延迟。 图17 TPC-C——在TPC-C benchmark下不同版本存储策略的吞吐量与延迟对比。图17 TPC-C——在TPC-C benchmark下不同版本存储策略的吞吐量与延迟对比。 \" 图17 TPC-C——在TPC-C benchmark下不同版本存储策略的吞吐量与延迟对比。 如图17a中的结果所示，仅追加存储比其它两种策略有更好的性能。这是因为该策略在执行多属性读操作时的开销更小，这种操作在TPC-C benchmark中很普遍。尽管增量数据存储在创建新版本时分配的内存更少，这一优势并没有带来显著的性能提升，因为在我们的实现中我们通过维护多个内存空间优化了内存管理。时间旅行策略的吞吐量更低，因为它没有为读取或写入操作带来任何好处。在图17b中，我们可以看到，仅追加策略和时间旅行策略在表扫描查询中表现更好。在增量存储策略下，扫描查询的延迟随着线程数的增长近线性地增长（这是不好的），而仅追加策略和时间旅行策略在使用40个线程时延迟降低了25%~47%。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:9:3","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"7.4 垃圾回收 现在我们将对第五章中介绍的GC机制进行评估。在这些实验中，我们使用了MVTO并发控制协议。首先我们比较了元组级的后台清理和协同清理。然后我们将元组级的方法与事务级的方法进行了对比。 元组级比较（Tuple-level Comparison）： 我们使用了低争用因子和高争用因子的更新敏感型负载（每个事务10个操作）。DBMS使用了O2N顺序的仅追加存储，因为COOP仅适用于这一顺序。在我们的配置中，DBMS使用40个线程处理事务，使用1个线程做GC。我们给出了DBMS的吞吐量随时间的变化和系统中新分配的内存总量。为了更好地理解GC的影响，我们还在禁用GC的情况下执行了负载。 图18 元组级对比（吞吐量）——在YCSB负载下DBMS吞吐量随时间变化，其使用了40个线程与元组级GC机制。图18 元组级对比（吞吐量）——在YCSB负载下DBMS吞吐量随时间变化，其使用了40个线程与元组级GC机制。 \" 图18 元组级对比（吞吐量）——在YCSB负载下DBMS吞吐量随时间变化，其使用了40个线程与元组级GC机制。 图19 元组级对比（内存）——在YCSB负载下DBMS为每个事务分配的内存总量（越低越好）随时间变化，其使用了40个线程与元组级GC机制。图19 元组级对比（内存）——在YCSB负载下DBMS为每个事务分配的内存总量（越低越好）随时间变化，其使用了40个线程与元组级GC机制。 \" 图19 元组级对比（内存）——在YCSB负载下DBMS为每个事务分配的内存总量（越低越好）随时间变化，其使用了40个线程与元组级GC机制。 图18中的结果显示，在读取敏感负载下，COOP的吞吐量比VAC高出了45%。在图19中，我们可以看到，COOP中每个事务的内存占用比VAC少30%~60%。与VAC相比，COOP的性能更加稳定，因为它可以将GC的开销分摊到多个线程上，且内存回收更快。在两种负载下，我们都能看到，当GC被禁用时，性能会随着时间下降，因为DBMS需要遍历更长的版本链以检索版本。另外，因为系统永远不会回收内存，它需要为每个新版本分配新内存。 元组级vs事务级（Tuple-level vs. Transaction-level）： 下面，我们评估在执行两种YCSB负载（高争用因子）的混合时，使用元组级和事务级机制的DBMS的性能。在我们的配置下，DBNS使用N2O顺序的仅追加存储。我们将工作线程数设置为40，并使用1个线程来做后台清理（VAC）。我们还在使用40个线程但没有任何GC的配置下执行了相同的负载。 图20 元组级vs事务级（吞吐量）——在两种YCSB混合负载下（$ \\theta = 0.8 $）的持续吞吐量随时间变化，其使用了不同的GC机制。图20 元组级vs事务级（吞吐量）——在两种YCSB混合负载下（$ \\theta = 0.8 $）的持续吞吐量随时间变化，其使用了不同的GC机制。 \" 图20 元组级vs事务级（吞吐量）——在两种YCSB混合负载下（$ \\theta = 0.8 $）的持续吞吐量随时间变化，其使用了不同的GC机制。 图21 元组级vs事务级（内存）——在两种YCSB混合负载下（$ \\theta = 0.8 $）DBMS为每个事务分配的总内存（越低越好）随时间变化，其使用了不同的GC机制。图21 元组级vs事务级（内存）——在两种YCSB混合负载下（$ \\theta = 0.8 $）DBMS为每个事务分配的总内存（越低越好）随时间变化，其使用了不同的GC机制。 \" 图21 元组级vs事务级（内存）——在两种YCSB混合负载下（$ \\theta = 0.8 $）DBMS为每个事务分配的总内存（越低越好）随时间变化，其使用了不同的GC机制。 图20a中的结果显示，对于读取敏感型负载，事务级GC的性能比元组级GC的性能稍微好了一点，但在图20b的更新敏感型负载下，性能的差距增大到了20%。事务级GC会分批移除过期的版本，这减少了同步开销。与禁用GC的情况相比，两种机制的吞吐量都提高了20%~30%。如图21所示，两种机制都减少了内存使用量。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:9:4","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"7.5 索引管理 最后，我们比较了第六章中描述的索引指针策略。影响使用这些策略的DBMS的性能的主要方面是辅助所以。每当创建新版本时，DBMS就要更新指针。因此，我们在更新敏感型YCSB负载下，评估了这些策略随着数据库中辅助索引数增加时的性能表现。在所有试验的配置下，DBMS都使用了MVTO并发控制协议、N2O顺序的仅追加存储、和事务级COOP GC。我们使用仅追加存储的原因在于它是唯一支持物理索引指针的策略。对于逻辑指针，我们将每个索引键映射到了版本链链头。 图22 索引管理——在不同辅助索引数下事务达到的吞吐量。图22 索引管理——在不同辅助索引数下事务达到的吞吐量。 \" 图22 索引管理——在不同辅助索引数下事务达到的吞吐量。 图23 索引管理——在更新敏感型YCSB负载下，有8个辅助索引时，吞吐量随线程数的变化。图23 索引管理——在更新敏感型YCSB负载下，有8个辅助索引时，吞吐量随线程数的变化。 \" 图23 索引管理——在更新敏感型YCSB负载下，有8个辅助索引时，吞吐量随线程数的变化。 图22b中的结果表明，在高争用因子下，逻辑指针策略的性能比物理指针策略高出了25%。图22a中的结果表明，在低争用因子下，随着辅助索引数增加到20个，性能差距增大到了40%。图23进一步展示了逻辑指针的优势。其结果表明，在高争用负载下，使用逻辑指针的DBMS吞吐量比使用物理指针的高出45%。无论是在低争用因子下还是在高争用因子下，性能的差距都会醉着线程数的增加而下降。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:9:5","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"8. 探讨 我们在对MVCC DBMS中的这些事务管理设计策略的分析与实验中得出了4个发现。最重要的是，版本存储策略是多核环境下内存式MVCC DBMS伸缩性最重要的组件之一。这违背了传统的数据库研究中的常识，传统的数据库研究通常侧重于并发控制协议的优化[48]。我们观察到，仅追加策略和时间旅行策略的性能受下层内存分配策略的效率影响；主动为每个核划分内存空间可以解决这一问题。无论使用怎样的内存分配策略，增量存储策略都能保持相对较高的性能，特别是当只修改表中保存的属性的子集的时候。但是该策略表扫描性能很低，且可能不适合读取量很大的分析负载。 我们还发现，使用适合负载的并发控制协议能够提高性能，特别是在高争用的负载下。章节7.2的结果表明，协议的优化可能对这些负载下的性能有负面影响。总之，我们发现MVTO在各种负载下的表现都很好。我们在表1中列出的系统都没有使用这一协议。 我们还观察到MVCC DBMS的性能与GC的实现密切相关。特别是，我们发现事务级GC能提供最好的性能和最少的内存占用。这是因为它回收过期元组版本的同步开销比其它方法小。我们注意到，GC过程可能导致系统的吞吐量和内存占用波动。 最后，我们发现，对于构建了许多辅助索引的数据库来说，索引管理策略也会影响DBMS的性能。章节7.5的结果显示，逻辑指针策略总能达到更高的吞吐量，特别是当处理更新敏感型负载时。这证实了工业界中对这一问题的报告[25]。 为了验证这些发现，我摩恩在Peloton中做了最后一个实验，我们配置Peloton使用表1中列出的MVCC配置。我们执行了TPC-C负载，并使用一个线程反复执行StockScan查询。我们测量了DBMS的吞吐量和StockScan查询的平均延迟。虽然在真实地DBMS中有我们没比较的其它因素（例如，数据结果、存储架构、查询编译等），但这仍是对它们的能力的很好的近似。 图24 配置对比（吞吐量）——在TPC-C benchmark下，表1中MVCC配置的性能。图24 配置对比（吞吐量）——在TPC-C benchmark下，表1中MVCC配置的性能。 \" 图24 配置对比（吞吐量）——在TPC-C benchmark下，表1中MVCC配置的性能。 如图24所示，使用Oracle/MySQL和NuoBD的配置的DBMS在低争用和高争用负载下的表现都是最好的。这是因为这些系统的存储策略在多核和内存式系统中伸缩性最好，且它们的NV2PL协议无论在哪种争用的负载下都能提供相对高的性能。HYRISE、MemSQL和HyPer的配置的性能相对较低，因为它们使用的MVOCC协议在验证阶段需要的读取集合遍历会带来很高的开销。Postgres和Hekaton的配置的性能最差，其主要原因是它们使用的O2N顺序的仅追加存储严重限制了系统的可伸缩性。该实验表明并发控制协议和版本存储方案都可以对吞吐量产生很大影响。 图25 配置对比（扫描延迟）——在TPC-C benchmark下，表1中MVCC配置的性能。图25 配置对比（扫描延迟）——在TPC-C benchmark下，表1中MVCC配置的性能。 \" 图25 配置对比（扫描延迟）——在TPC-C benchmark下，表1中MVCC配置的性能。 但是图25中的延迟结果显示，使用增量存储的DBMS性能最差。这是因为增量存储必须花费更多时间遍历版本链，以找到目标元组版本的属性。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:10:0","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"9. 相关工作 最早提到MVCC的是1979年的Reed的论文[38]。在那之后，研究者专注于理论理解和MVCC在单核基于磁盘的DBMS中的性能[9, 11, 13]。我们着重介绍了时间更近一些的研究成果。 并发控制协议： 已有的一些工作提出了优化内存式事务处理的新技术[46, 47]。Larson等人[27]在Microsoft Hekaton DBMS的早期版本中比较了悲观协议（MV2PL）和乐观协议（MVOCC）[16]。Lomet等人[31]提出了一种使用时间戳范围解决事务间冲突的策略，Faleiro等人[18]等人分离了DBMS事务处理中的MVCC并发控制协议和版本管理。考虑到确保MVCC串行性的挑战，许多DBMS转而支持被称为快照隔离[8]的更弱的隔离级别，它不会排除写入倾斜异常（write-skew anomaly）。串行快照隔离（serializable snapshot isolation，SSI）通过消除快照隔离中可能发生的异常确保了串行性[12, 20]。Kim等人[24]使用SSN来在异构负载上扩展MVCC。我们在本文中的研究比它的范围更广。 版本存储： MVCC DBMS中的另一个重要的设计选择是版本存储策略。Herman等人[23]提出了一种不同的用于事务管理的结构，以在不妥协读取性能的同时实现高写入吞吐量。Neumann等人[36]通过本地事务存储优化减少了同步开销，提高了MVCC DBMS的性能。这些策略与传统的仅追加版本存储策略不同，传统的策略在内存为主的DBMS中有更高的内存分配开销。Arulraj等人[7]研究了在运行异构负载时，物理设计对混合DBMS性能的影响。 垃圾回收： 大多数DBMS都适用于元组级别的后台清理垃圾回收策略。Lee等人[29]评估了现代DBMS中使用的一系列不同的垃圾回收策略。他们提出了一种新的混合策略，减少了SAP HANA中的内存占用。Silo的基于epoch的内存管理方法让DBMS能够扩展到更多线程数量上[44]。这种方法仅在一个epoch（和之前的epoch）不再包含活动的事务后回收版本。 索引管理： 最新，有一些新的索引数据结构被提出，以支持可伸缩的内存为主的DBMS。Lomet等人[32]等人引入了一种无闩保序索引，称为Bw-Tree，目前在Microsoft的一些产品中使用。Leis等人[30]和Mao等人[34]分别提出了ART和Masstree，它们是基于Tries的可伸缩索引结构。本工作没有研究不同索引结构的性能，而是着眼于不同的辅助索引管理策略如何影响MVCC DBMS的性能。 ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:11:0","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"10. 总结 我们发表了对内存式MVCC的事务管理设计决策的评估。我们描述了每种设计决策的最先进的实现，并展示了已有的系统是如何使用它们的。然后我们在Peloton DBMS中实现了它们，并使用OLTP负载来突出显示它们的权衡点。我们还展示了阻碍DBMS支持更多CPU核心数和更复杂的负载的问题。 致谢： This work was supported (in part) by the National Science Foundation (CCF-1438955) and the Samsung Fellowship Program. We also thank Tianzheng Wang for his feedback. ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:12:0","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"参考文献 [1] MemSQL. http://www.memsql.com. [2] MySQL. http://www.mysql.com. [3] NuoDB. http://www.nuodb.com. [4] Oracle Timeline. http://oracle.com.edgesuite.net/timeline/oracle/. [5] Peloton. http://pelotondb.org. [6] PostgreSQL. http://www.postgresql.org. [7] J. Arulraj and et al. Bridging the Archipelago between Row-Stores and Column-Stores for Hybrid Workloads. SIGMOD, 2016. [8] H. Berenson and et al. A Critique of ANSI SQL Isolation Levels. SIGMOD’95. [9] P. A. Bernstein and N. Goodman. Concurrency Control in Distributed Database Systems. CSUR, 13(2), 1981. [10] P. A. Bernstein, C. W. Reid, and S. Das. Hyder-A Transactional Record Manager for Shared Flash. In CIDR, 2011. [11] P. A. Bernstein and et al. Concurrency Control and Recovery in Database Systems. 1987. [12] M. J. Cahill, U. Röhm, and A. D. Fekete. Serializable Isolation for Snapshot Databases. SIGMOD, 2008. [13] M. J. Carey and W. A. Muhanna. The Performance of Multiversion Concurrency Control Algorithms. TOCS, 4(4), 1986. [14] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears. Benchmarking cloud serving systems with YCSB. In SoCC, 2010. [15] T. David, R. Guerraoui, and V. Trigonakis. Everything You Always Wanted To Know About Synchronization But Were Afraid To Ask. In SOSP, 2013. [16] C. Diaconu and et al. Hekaton: SQL Server’s Memory-Optimized OLTP Engine. SIGMOD, 2013. [17] K. P. Eswaran and et al. The Notions of Consistency and Predicate Locks in a Database System. Communications of the ACM, 19(11), 1976. [18] J. M. Faleiro and D. J. Abadi. Rethinking Serializable Multiversion Concurrency Control. VLDB, 2014. [19] B. Fan, D. G. Andersen, and M. Kaminsky. MemC3: Compact and Concurrent MemCache with Dumber Caching and Smarter Hashing. In NSDI, 2013. [20] A. Fekete, D. Liarokapis, E. O’Neil, P. O’Neil, and D. Shasha. Making Snapshot Isolation Serializable. TODS, 30(2), 2005. [21] M. Grund, J. Krüger, H. Plattner, A. Zeier, P. Cudre-Mauroux, and S. Madden. HYRISE: A Main Memory Hybrid Storage Engine. VLDB, 2010. [22] A. Harrison. InterBase’s Beginnings. http://www.firebirdsql.org/en/annharrison-s-reminiscences-on-interbase-s-beginnings/. [23] S. Héman, M. Zukowski, N. J. Nes, L. Sidirourgos, and P. Boncz. Positional Update Handling in Column Stores. SIGMOD, 2010. [24] K. Kim, T. Wang, J. Ryan, and I. Pandis. ERMIA: Fast Memory-Optimized Database System for Heterogeneous Workloads. SIGMOD, 2016. [25] E. Klitzke. Why uber engineering switched from postgres to mysql. https://eng.uber.com/mysql-migration/, July 2016. [26] H.-T. Kung and J. T. Robinson. On Optimistic Methods for Concurrency Control. TODS, 6(2), 1981. [27] P.-Å. Larson and et al. High-Performance Concurrency Control Mechanisms for Main-Memory Databases. VLDB, 2011. [28] J. Lee, M. Muehle, N. May, F. Faerber, V. Sikka, H. Plattner, J. Krueger, and M. Grund. High-Performance Transaction Processing in SAP HANA. IEEE Data Eng. Bull., 36(2), 2013. [29] J. Lee and et al. Hybrid Garbage Collection for Multi-Version Concurrency Control in SAP HANA. SIGMOD, 2016. [30] V. Leis, A. Kemper, and T. Neumann. The Adaptive Radix Tree: ARTful Indexing for Main-Memory Databases. ICDE, 2013. [31] D. Lomet, A. Fekete, R. Wang, and P. Ward. Multi-Version Concurrency via Timestamp Range Conflict Management. ICDE, 2012. [32] D. B. Lomet, S. Sengupta, and J. J. Levandoski. The Bw-Tree: A B-tree for New Hardware Platforms. ICDE, 2013. [33] N. Malviya, A. Weisberg, S. Madden, and M. Stonebraker. Rethinking Main memory OLTP Recovery. ICDE, 2014. [34] Y. Mao, E. Kohler, and R. T. Morris. Cache Craftiness for Fast Multicore Key-Value Storage. In EuroSys, 2012. [35] C. Mohan. ARIES/KVL: A Key-Value Locking Method for Concurrency Control of Multiaction Transactions Operating on B-Tree Indexes. VLDB’90. [36] T. Neumann, T. Mühlbauer, and A. Kemper. Fast Serializable Multi-Version Concurrency Control for Main-Memory Database Systems. SIGMOD, 2015. [37] A. Pavlo and M. Aslett. What’s Really New with NewSQL? SIGMOD Rec., ","date":"2020-10-08","objectID":"/posts/paper-reading/wu-vldb2017/:13:0","tags":["MVCC","Translation"],"title":"《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》论文翻译","uri":"/posts/paper-reading/wu-vldb2017/"},{"categories":["Paper Reading"],"content":"本篇文章是对论文CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data的原创翻译，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:0:0","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"作者 Sage A. Weil Scott A. Brandt Ethan L. Miller Carlos Maltzahn Storage Systems Research Center University of California, Santa Cruz {sage, scott, elm, carlosm}@cs.ucsc.edu ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:1:0","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"摘要 新兴的大型分布式存储系统面临着将PB级的数据分布到数十、数百、甚至数千个存储设备上的问题。这样的系统必须均匀地分布数据和负载，以高效地利用可用资源并最大化系统性能，同时帮助处理增长并管理硬件故障。我们开发了CRUSH，一个可伸缩的伪随机数据分布函数，其为分布式对象存储系统设计，能高效地将数据对象映射到存储设备上，且而不依赖中央目录。因为大型系统有着固有的（inherently）动态性（译注：在Ceph中我将其翻译为“xxx本质上具有动态性”，本文中均翻译为“xxx具有固有的动态性”，以便于表达），CRUSh是为在帮助处理存储的增加与移除的同时减小不必要的数据移动而设计的。该算法适用于很多种类的数据副本和可靠性机制，同时根据用户定义的策略分布数据，这些策略可以强制将不同的副本分散到不同的故障域（failure domain）中。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:2:0","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"1. 引言 对象存储是一种新兴的架构，它可以改善可管理性、可伸缩性和性能[Azagury et al. 2003]。与传统的基于块的硬盘驱动器不同，对象存储设备（object-based storage device，OSD）在内部管理磁盘块的分配，暴露可以让其它设备读取或写入到边长、命名对象的接口。在这种系统中，每个文件的数据通常会分条（strip）到数量相对少的命名对象中，而命名对象会分布在整个集群中。对象在有多个在多个设备上的副本（或者采用某些其他的数据冗余策略），以防止故障发生时数据丢失。基于对象的存储系统通过用较小的对象列表替代较大的块列表，以简化数据布局并分摊下层的块分配问题。虽然这通过减少了文件分配的元数据和复杂性大大提高了可伸缩性，但是将数据分布到数千个存储设备上（通常其容量和性能都不同）这一基本问题仍然存在。 大部分系统简单地将新数据写入到未被充分利用的设备上。这种方法的根本问题是，当数据被写入后，它很少甚至不会被移动。即使是完美的分布也会在存储系统被扩展后变得不均衡，因为新的磁盘或者是空的或者仅含有新的数据。旧的磁盘和新的磁盘可能都是繁忙的，这取决于系统负载，但是这很少能够平等地利用二者并得到所有可用资源的优势。 一种健壮的解决方案是将所有数据随机分布到系统中可用的存储设备上。这样可以使分布在概率上是均衡的，且新数据和旧数据会均匀地混合在一起。当添加新的存储时，已有数据的中的一份随机采样会被迁移到新的存储设备上以保持平衡。这种方法最重要的优势是平均，所有设备将会有相似的负载，让系统在任何潜在负载下表现良好[Santos et al. 2000]。另外，在大型存储系统中，一个大文件会被随机的分布到很多个可用的设备上，这可以提供很高的并行性和整体带宽。然而，简单的基于哈希的分布方法无法很好应对设备数量变化的情况，它会导致数据大量重新调配（reshuffle）。此外，已有的随机分布策略会将每个磁盘上的副本分散到许多其它设备上，它在多个设备同时故障时由很大的数据丢失的可能性。 我们开发了CRUSH（Controlled Replication Under Scalable Hashing，基于可伸缩哈希的受控多副本策略），它是一个伪随机数据分布算法，它能够高效、健壮地将对象副本在异构、结构化存储集群上分布。CRUSH的实现是一个伪随机、确定性的函数，它将输入值（通常是一个对象或对象组的标识符）映射到一系列存储对象副本的设备上。它与传统方法的不同点在于，它的数据分配不依赖每个文件或每个对象的任何类型的目录——CRUSH仅需要对构成存储集群的设备的紧凑的层次结构的描述和副本分配策略的知识。这种方法有两个关键的好处：第一，它完全是分布式的，大型系统中的任意一方都能单独计算任何对象的位置；第二，所需的元数据很少且几乎是静态的，它仅在有设备加入或移除时变化。 CRUSH是为优化数据分布以利用可用资源、在增加或移除存储设备时高效重新组织数据、和为数据副本分配提供灵活的约束以在意外或相关硬件故障时增加数据安全性设计的。其支持很多种数据安全机制，包括n路副本（镜像）、RAID奇偶校验策略或其它纠删码格式、和混合方法（例如RAID-10）。这些特性让CRUSH可以很好地适用于在可伸缩性、性能、和可靠性很重要的大型存储系统中（数PB）管理对象分布。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:3:0","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"2. 相关工作 对象存储最近作为一种提高存储系统可伸缩性的机制备受关注。许多研究和生产级文件系统都采用了基于对象的方法，这包括影响深远的NASD文件系统[Gobioff et al. 1997]、Panasas文件系统[Nagle et al. 2004]、Lustre[Braam 2004]、和其它等基于对象的文件系统[Rodeh and Teperman 2003; Ghemawat et al. 2003]。其它的基于块的分布式文件系统像GPFS[Schmuck and Haskin 2002]和Federated Array of Bricks（FAB）[Saito et al. 2004]面临着类似的数据分布问题。这些系统使用了半随机（semi-random）或基于启发式（heuristic-based）的方法来将新数据分配到有可用容量的存储设备上，但是很少将数据重新放置以随着时间维护均衡的分布。更重要的是，所有的这些系统都通过某种元数据目录的方式定位数据，相反，CRUSH依赖紧凑集群描述（compact cluster description）和确定性映射函数（deterministic mapping function）。二者在写入数据时的区别最明显，使用CRUSH的系统不需要咨询中央分配器（central allocator）就能计算出任何新数据的存储位置。Sorrento[Tang et al. 2004]存储系统使用的一致性哈希[Karger et al. 1997] 与CRUSH最相似，但是其不支持设备权重控制（controlled weighting of devices）、数据良好均匀分布（well-balanced distribution of data）、和用来提供安全性的故障域（failure domains）。 尽管数据迁移问题已经在有显式分配映射的系统中有大量的研究[Anderson et al. 2001; Anderson et al. 2002]，这种方法对元数据的要求很高，而像CRUSH等方法想避免这一点。Choy等人[1996]描述了再次攀上分布数据的算法，其在磁盘增加时可以移动一定数量的对象以获得优化，但是不支持权重、副本、或磁盘移除。Brinkmann等人[2000]使用了哈希函数来将数据分布到异构但静态的集群中。SCADDAR[Goel et al. 2002]解决了添加或移除存储的问题，但是仅支持副本策略的一个受限的子集。这些方法都没有CRUSH的灵活性或用来提高可靠性的故障域的概念。 CRUSH基于RUSH算法族[Honicky and Miller 2004]且与其最为相似。RUSH是已有的文献中唯一的一个利用映射函数替代显式元数据并支持高效的加权设备添加或移除的算法。尽管RUSH算法有这些特性，但是大量的问题使其无法实际应用。CRUSH完整地包括了RUSHP和RUSHT中好用的元素，同时解决了之前未解决的可靠性和副本问题，并优化了性能和灵活性。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:4:0","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"3. CRUSH算法 CRUSH算法参考每个设备的权重值来将数据对象从概率上近似均匀地分布到存储设备上。该分布受分层的（hierarchical）集群映射（cluster map） 控制，其表示可用的存储资源，由这些资源组成的逻辑元素构成。例如，人们可能通过如下方式描述一个大型设施（installation）：设施由多行服务器的cabinet（机箱架）组成，cabinet中装满了disk of shelf（磁盘柜，译注：下文中省略为shelf），shelf中装满了storage device（存储设备，译注，下文中省略为device）。数据分布策略是根据放置规则（placement rules） 定义的，这些规则指定了有多少目标副本被从集群中选取且副本放置时有哪些限制。例如，用户可能指定三份副本需要被放置在不同cabinet上的物理机上，这样它们就不会共享同一个电路。 给定一个整型输入值$x$，CRUSH会输出一个由$n$个不同的目标存储组成的有序列表$\\overrightarrow{R}$。CRUSH使用了一个强大的多输入整型哈希函数，$x$是其输入之一，该函数可以使映射完全是确定性的，且可以仅通过集群映射、放置规则、和$x$计算。该分布是伪随机的，因为相似数据的输出结果或存储在任何device上的item（项）间没有明显的关联。CRUSH生成的副本分布是分簇（declustered） 的，因为一个item的副本所在的device与所有其它item的副本所在位置看上去也是独立的。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:5:0","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"3.1 分层集群映射 集群映射由device 和bucket（桶） 组成，二者都有相关的数字型标识符和权重。bucket可以包括任意数量的device或其它的bucket，因此bucket可以在存储层次中作为中间节点，而device永远都是叶子节点。管理员为device分配权重，以控制它们负责存储的数据的相对的量。尽管大型系统很可能有不同容量、不同性能特征的device，但在随机的数据分布中，从统计学的角度看，device的利用率和负载相关，因此device的负载与存储的数据量成正比。因此，一维的放置矩阵（即权重）应从device容量推导得出。bucket的权重被定义为它包含的item的权重的和。 bucket可被随意组合来构建表示可用存储的层次结构。例如，用户可以创建如下的集群映射：在最下层使用“shelf（机柜）”来表示被安装的相同的device的集合，然后将多个“shelf”合并为“cabinet（cabinet）”bucket来将安装在同一个rack（机架）上的组合在一起。在更大的系统中，“cabinet”可能被进一步组合成“row（行）”或“room（房间）” bucket。数据通过一个伪随机类哈希函数在层次结构中递归选取嵌套的bucket item来放置。在传统的哈希技术中，对目标容器（device）数量的任何变更都会导致大量的容器内容重新调配；相反，CRUSH基于4种不同的bucket类型，每种类型都有不同的选取算法来解决device增加或移除带来的数据移动问题和整体的计算复杂度。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:5:1","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"3.2 副本放置 CRUSH旨在将数据均匀地分布到加权的device上，以维护存储和device带宽资源的利用率在统计上的均衡。副本在层级结构中的device上的放置方式同样对数据安全性有重要影响。CRUSH可以反映出设施的物理组织方式，并通过它对潜在的“关联（correlated）设备故障源”建模，并解决这些问题。典型的关联设备源包括共享的电源、和共享的网络。通过将这一信息编码到集群映射中，CRUSH放置策略可以在维护所需的分布同时将对象的副本分散到不同的故障域中。例如，为了解决可能发生的关联故障，可能需要确保数据副本分布在不同的cabinet、rack、电源、控制器、和/或物理位置上。 为了适配各种可能使用CRUSH的场景，无论是在数据副本策略还是在底层硬件配置方面，CRUSH为采用的每个副本策或分部策略都定义了放置规则，这让存储系统或管理员可以精确地指定对象副本如何放置。例如，为一条选取了2个目标的规则采用2路镜像、为一条选取了3个在不同数据中心的的目标的规则采用3路镜像、为一条规则在5个device上采用RAID-4，等等注1。 注1：尽管各种各样的数据冗余机制都是可行的，为了简单起见，我们提到的数据对象都被采用副本策略存储。 算法1 CRUSH中对象$x$的放置算法1 CRUSH中对象$x$的放置 \" 算法1 CRUSH中对象$x$的放置 每条规则由一系列应用到层次结构中的简单操作组成，如算法1中的伪代码表示的那样。CRUSH函数的整型输入$x$通常是对象名或其它的标识符，如一组副本被放在同一个device上的对象的标识符。$take(a)$操作会在存储层次结构中选取一个item（通常是一个bucket），并将其分配给向量$\\overrightarrow{i}$，作为后续操作的输入。$select(n,t)$操作遍历每个元素$i$（$i \\in \\overrightarrow{i}$），并在以该点为根的子树中选取$n$个类型为$t$的不同的item。device的类型已知且固定，且系统中的每个bucket都有一个类型字段，以用来区分不同类型的bucket（例如，用来表示“row”的bucket和用来表示“cabinet”的bucket）。对于每个$i$（$i \\in \\overrightarrow{i}$），$select(n,t)$调用会遍历item$r$（$ r \\in 1,…,n $），并递归下降地处理中间的任何bucket，然后在每个bucket中通过函数$c(r,x)$（在章节3.4中为每类bucket定义的）伪随机地选取一个其中的item，直到它找到一个类型为请求的类型$t$的item。其产生的$n| \\overrightarrow{i} |$个不同的item会被放回到输入$ \\overrightarrow{i} $中，它们或作为后续$select(n,t)$的输入，或通过$emit$操作被移动到结果向量中。 表1 在同一列但不同的3个cabinet上分布3份副本的简单规则。表1 在同一列但不同的3个cabinet上分布3份副本的简单规则。 \" 表1 在同一列但不同的3个cabinet上分布3份副本的简单规则。 例如，表1中定义的规则从图1中的层次结构的根开始。第一个$select(1,row)$选取了一个类型为“row”的bucket（其选取了row2）。随后的$select(3,cabinet)$在之前选取的row2内选择了3个不同的cabinet（cab21、cab23、cab24），最后，每个$select(1,disk)$遍历其输入向量中的3个cabinet bucket之一，并选择其下的1个disk。最终结果是分散在3个cabinet中的3个disk，但是都在同一个row中。因此，这种方法让副本能跨副本分布且对容器类型（例如，row、cabinet、shelf）进行约束，这一性质兼顾了可靠性和性能。规则由多个$take$、$emit$块组成，允许从不同的存储池中显式地提取目标存储，就像远程副本场景期望的那样（副本被存储在一个远程站点中）或分层设施场景期望的那样（例如，有快速近线存储和较慢但容量更大的存储阵列）。 图1 一个由磁盘（disk）、磁盘柜（shelf of disk）、cabinet（cabinet）、行（row）组成的4层集群映射的局部视图。粗线条表示放置规则中每个$select$操作选取的item和表1中假设的映射。图1 一个由磁盘（disk）、磁盘柜（shelf of disk）、cabinet（cabinet）、行（row）组成的4层集群映射的局部视图。粗线条表示放置规则中每个$select$操作选取的item和\u003cstrong\u003e表1\u003c/strong\u003e中假设的映射。 \" 图1 一个由磁盘（disk）、磁盘柜（shelf of disk）、cabinet（cabinet）、行（row）组成的4层集群映射的局部视图。粗线条表示放置规则中每个$select$操作选取的item和表1中假设的映射。 3.2.1 碰撞、故障与过载 $select(n,t)$操作可能从它的起始点开始穿过多个存储层级以找到其下的$n$个不同的类型为$t$的item，它是一个以$r$（$r=1,…n$，n是所选的副本编号）作为部分参数的递归过程。在此过程中，CRUSH可能因三个不同原因拒绝（reject）并重新选取（reselect）item并修改输入$r'$：如果item已经被选取到当前的集合中（碰撞——$select(n,t)$的结果必须是不重复的），如果device是故障的（failed），或者如果device是过载的（overloaded）。故障或过载的device会在集群映射中标记出来，但这些device还留在层级结构中，以避免不必要的数据迁移。CRUSH会有选择地迁移一个过载device上的数据，这通过按集群映射中指定的概率伪随机地拒绝（reject）实现——这通常与device报告的过度利用的情况有关。对于故障或过载的device，CRUSH会重新开始$select(n,t)$开始处的递归（见算法1第11行），均匀地将它们的item分布到存储集群中。如果发生碰撞，在递归的内层尝试本地搜索时首先会使用$r'$（见算法1第14行）并避免整体数据分布偏离更可能发生冲突的子树（例如，bucket数小于n的子树）。 3.2.2 副本排名 奇偶校验和纠删码策略与副本策略相比有明显不同的放置需求。在主拷贝副本策略（primary copy replication scheme）中，副本常常希望在之前的目标副本（已经有一份数据拷贝的副本）故障后成为新的主副本。在这种情况下，CRUSH可以用$ r ' = r + f $来重新选取“前n个”合适的目标，其中$f$是当前的$select(n,t)$的失败的放置尝试次数（见算法1第16行）。然而，在奇偶校验或纠删码策略下，CRUSH输出中的device的排名或位置十分重要，因为每个目标保存数据对象的不同的位。特别是，如果一个device故障，CRUSH的输出列表$\\overrightarrow{R}$的适当位置的item需要被替换，这样，列表中的其他device会保持相同的排名（即，$\\overrightarrow{R}$中的位置，见图2）。在这种情况下，CRUSH会使用$ r ' = r + f_{r} n $重新选取，其中$f_{r}$是在item$r$上的失败尝试的次数。因此，这为每个副本的排名定义了一个候选序列，每个候选序列从概率上与其它device的故障无关。相反，RUSH没有特殊处理device故障。像其它已有的哈希分布函数一样，它隐式假设使用“前n个”方法跳过结果中的故障device，这使其也适用于副本策略。 图2 $select(6,disk)$中当device $r=2(b)$被拒绝时的重选取行为，方框中包含CRUSH的输出$\\overrightarrow{R}$，表示按排名编号的$n=6$个device。左图展示了“前n个”方法，其已有设备排名$(c,d,e,f)$可能改变。右图中，每个排名都有一个概率独立的潜在目标序列；图中$f_r = 1$且$ r ' = r + f_r n = 8 $（device h）。图2 $select(6,disk)$中当device $r=2(b)$被拒绝时的重选取行为，方框中包含CRUSH的输出$\\overrightarrow{R}$，表示按排名编号的$n=6$个device。左图展示了“前n个”方法，其已有设备排名$(c,d,e,f)$可能改变。右图中，每个排名都有一个概率独立的潜在目标序列；图中$f_r = 1$且$ r ' = r + f_r n = 8 $（device h）。 \" 图2 $select(6,disk)$中当device $r=2(b)$被拒绝时的重选取行为，方框中包含CRUSH的输出$\\overrightarrow{R}$，表示按排名编号的$n=6$个device。左图展示了“前n个”方法，其已有设备排名$(c,d,e,f)$可能改变。右图中，每个排名都有一个概率独立的潜在目标序列；图中$f_r = 1$且$ r ' = r + f_r n = 8 $（device h）。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:5:2","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"3.3 映射变更和数据移动 大型文件系统中数据分布的一个重要要素是对存储资源增加或移除的响应方式。CRUSH在所有时刻都会维护一个均匀的数据分布和负载，以避免负载不对称和相关可用资源利用不充分。当一个device故障时，CRUSH会标记该device，但仍将其留在层次结构中，这样它会被拒绝且它的内容会通过放置算法（见章节3.2.1）被均匀地重分布。这种集群映射的变化将会让总数据的最优（最小）比例（$ w_{failed} / W $，其中$W$是所有设备的总权重）被重新映射到新的存储目标上，因为仅故障设备上的数据会被移动。 当集群层次结构被改变时（例如增加或移除存储资源），情况会更加复杂。CRUSH的映射过程使用集群映射作为加权分层决策树，在这样的情况下可能造成比理论最优$ \\frac{ \\Delta w}{W} $更多的数据移动。在层次结构中的每一层中，当相对子树的权重变化改变了分布时，一些数据必须从权重减小的子树移动到权重增大的子树上。因为层次结构上的每个节点的伪随机放置决策在统计上是独立的，移动到子树的数据会在该点下均匀地重分布，但是不一定会被重新映射到导致权重变化的item下（译注：因此会造成更多的数据移动）。仅后面（更深）的层级的放置过程（经常是不同的）会移动数据以保持整体相对分布的正确性。图3中二分层级结构中说明了这种一般性的影响。 图3 因节点增加或移除和随后的权重变化导致的数据在二分结构上的移动。图3 因节点增加或移除和随后的权重变化导致的数据在二分结构上的移动。 \" 图3 因节点增加或移除和随后的权重变化导致的数据在二分结构上的移动。 层次结构上的总数据移动量的下界为$ \\frac{ \\Delta w}{W} $，这是将驻留在权重为$ \\Delta w$的新增device上的数据比例。数据移动量随着层次结构的高度$h$增长，其具有保守（conservative）的渐近上限$ h \\frac{ \\Delta w}{W} $。当$ \\Delta w $比$W$小得多时，数据迁移的总量趋近于上界，因为在递归的每一步中移动到子树的数据对象被映射到权值相对小的item上的可能性很小。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:5:3","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"3.4 bucket类型 通俗的说，CRUSH旨在调和两个互相竞争的目标：映射算法的性能和可伸缩性、在集群因增加或移除device变化时减小为恢复分布平衡的数据迁移量。为此，CRUSH定义了4个不同的bucket类型来表示集群层级结构的内部节点（非叶子节点）：uniform bucket、list bucket、tree bucket、straw bucket。每种不同的bucket类型都基于不同的内部数据结构，并在副本放置过程中使用不同的函数$c(r,x)$来伪随机地选择内部item，以表示在计算和重组织性能间不同的平衡点（tradeoff）。uniform bucket包含的item必须全都有相同的权重（很像常规的基于哈希的分布函数），而其它类型的bucket的item的权重可以不同。表2中总结了这些区别。 表2 不同类型的bucket的item被添加到bucket或从bucket移除时映射速度和数据重组织效率的总结。表2 不同类型的bucket的item被添加到bucket或从bucket移除时映射速度和数据重组织效率的总结。 \" 表2 不同类型的bucket的item被添加到bucket或从bucket移除时映射速度和数据重组织效率的总结。 3.4.1 uniform bucket device很少被单独添加到大型系统中。相反，新的存储通常以多个设备组成的一整块部署，例如在服务器rack中添加一个shelf或者可能添加一整个cabinet。达到寿命的device也经常作为一个集合退役（单独的设备故障除外），我们很自然地会视它们为一个整体。CRUSH的uniform bucket被用来表示这种情况下的一个相同的device的集合。其关键的优势在于与它的性能：CRUSH可以在恒定的时间内将副本映射到bucket中。在不适合使用均衡限制的情况中，可以使用其它bucket类型。 给定CRUSH的输入$x$和一个副本编号$r$，我们使用函数$c(r,x)=(hash(x)+rp) \\mod m$从大小为$m$的uniform bucket中选择一个item，其中$p$是随机选择的（但是是确定的）大于$m$的质数。对于任意的$r \\le m $，我们总是能使用一些简单的数论定理注2来选择一个不同的item。对于$r \u003e m$，这一保障不再成立，这意味着两个不同的副本$r$和相同的输入$x$可能解析到同一个item。在实际环境中，这仅意味着碰撞的概率非零，且后续会通过放置算法回溯（见章节3.2.1）。 如果uniform bucket的大小改变，会发生device间的完整的数据重调配，这很像传统的基于哈希的分部策略。 3.4.2 list bucket list bucket结构的内容被组织为一个链表，它能容纳任意权重的item。为了放置一份副本，CRUSH从保存着最近被添加的item的链头开始，将它的权重和其余所有的item的权重作比较。后续操作取决于$hash(x,r,item)$的值，或者会以适当的可能性选取当前的item，或者继续递归处理这个列表。这种源于RUSHp的方法将放置的问题转化为“选择最近被添加的item还是之前的item？”。这对于扩展中的集群来说，是一个很自然且直观的选择：或者按适当的概率将对象重新定位到最新的device上，或者它像以前一样保留在旧的device上。在item被加入到bucket时，数据迁移是最优的。然而，当item从链表的中间或尾部移除，会导致大量的不必要的数据移动。因此list bucket最适合bucket从不（或很少）缩小的情况。 RUSHp算法约等于包含内有许多uniform的单个list bucket的2层CRUSH结构。它因集群表示形式固定而无法使用放置规则或CRUSH中通过控制数据放置而增强可靠性的故障域。 3.4.3 tree bucket 像所有的链表数据结构一样，list bucket对于较小的item的集合效率很高，但是可能不适用于较大的集合，因为其$O(n)$的运行时间可能太大。而源于RUSHT的tree bucket通过将它的item保存在二叉树中解决了这一问题。这将放置时间减小到了$O( \\log n )$，使其适用于管理大得多的device或嵌套的bucket集合。RUSHT等同于一个由包含许多uniform bucket的单个tree bucket的2层CRUSH结构。 tree bucket的结构为一个叶子为item的加权二分搜索树。每个内部节点都知道它左子树的总权值和右子树的总权值，且被打上了一个相应的固定策略的标签（后文中会讲解）。为了在一个bucket中选取一个item，CRUSH从树的根节点开始执行，它根据输入键$x$、副本编号$r$、bucket标识符、和当前树节点的标签（初始为根节点）计算哈希。其结果将与左右子树的权重比进行比较，以决定接下来访问哪个子节点。这一过程会反复执行直到到达叶子节点，该叶子节点相关的bucket中的item会被选择。在定位item时，仅需要$ \\log n $次哈希运算和节点比较。 bucket的二叉树节点被打上了二进制值的标签，这使用了一个简单、固定的策略以避免当树增长或缩小时标签改变。数的最左叶子总被标记为“1”。每当树被扩展时，旧的根节点会变成新的根节点的左孩子，新的根节点被标记为旧的根节点的标签左移一位的值（1,10,100，等等）。树的右侧节点的标签是镜像，除了每个值前都加上了一个“1”译注1。图4中展示了一个有6个叶子节点的带标签的二叉树。这一策略保证了当有新item被加入到bucket时（会从bucket移除时）或树增长时（或缩小时），对于任何已存在的叶子item，其穿过二叉树的路径仅需在开始放置决策树时向根节点添加（或移除）额外的节点。一旦一个对象被放置到了特定的子树时，它最后的映射将仅取决于权重和该子树中的节点标签，且只要子树的item不变就不会改变。尽管该层级决策树在内部item迁移时引入了一些额外的数据，这一策略能保证数据在合理的层级移动，同时即使在bucket非常大时也能提供高效的映射。 译注1：这里值前的“1”不是直接在右子树对应的左子树的节点的标签前添加一个“1”，“1”的位置取决于树扩展时新的根节点左移后的“1”的位置。例如，图4中根“10”扩展时，新的根为“100”，其右子树为“110”，右子树的左孩子叶子节点对应的左子树的标签为“1”，而新的根的“1”出现在第三位，因此第三个叶子节点的标签为“101”，以此类推。其实这只是一个用来维护搜索二叉树的二进制标签成立的简单算法。 图4 tree bucket的二叉树使用的节点标签策略。图4 tree bucket的二叉树使用的节点标签策略。 \" 图4 tree bucket的二叉树使用的节点标签策略。 3.4.4 straw bucket list bucket和tree bucket的结构都需要计算有限数量的哈希值并与权重对比以选择bucket的一个item。为了提高副本放置的性能，它们都采用了某种分治方法，要么让某些item优先（例如，哪些在列表开始位置的item），要么根本不需要考虑item的整个子树。但是，在提高副本放置过程性能的同时，但也会在bucket的内容变更时（item的添加、移除、或修改item的权重）引入次优的重组织行为。 straw bucket在副本放置时能通过一种模拟抽签的方法让所有的item公平地“竞争”。在放置副本时，会向bucket中的每个item分配一个长度随机的签。签最长的item会胜利。每个签的长度最初是一个固定范围中的一个值，其基于CRUSH的输入$x$、副本编号$r$、和bucket item $i$的哈希得到。每个签的长度可通过一个基于其item的权重的因子$f( w_i )$伸缩注3，因此权重更重的item更有可能赢得抽签，即$c(r,x)= \\max _i ( f( w_i ) hash(x,r,i) )$。尽管这一过程几乎比list bucket慢两倍甚至比tree bucket（按对数伸缩）还要慢，但是straw bucekt能在变更时得到最优的内部item间的数据移动。 注3：尽管$f( w_i )$的简单解析解未知，但是程序化地计算权重因子是相对容易的（有源码）。该计算仅需要在每次bucket被修改时执行。 bucket的类型可根据期望的集群增长模式选择，在适当情况下可以用映射函数计算效率还换取数据移动效率。当期望bucket是固定的时（例如一个有相同磁盘的shelf），uniform bucket是最快的。如果bucket预期仅能够扩展，list bucket能在新item被加入到链头时提供最优的数据移动。这让CRUSH能恰好将适当的数据转移到新的device上，而不需要在其它的bucket item间调配数据。其缺点是映射速度为$O(n)$且当旧的item被移除或修改权重时会造成额外的数据移动。在可能存在item移除或重组效率很重要时（例如，接近存储层次结构的根处），straw bucket能在子树间提供最优的迁移行为。tree bucket是在各种方面折衷的方案，它能提供出色的性能和良好的重组效率。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:5:4","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"4. 评估 CRUSH的设计基于很多种目标：均衡、在异构存储是被间加权分布、减小存储增加或移除时的数据移动（包括单个磁盘故障的情况）、通过跨多个故障域放置副本来提高系统可靠性、和迎来描述可用存储与数据分布的灵活地描述与规则系统。我们在相应的CRUSH配置下评估了这些行为，我们模拟了对象到device的分配并检验了分布结果，并与RUSHP和RUSHT风格的集群进行了对比。我们分别通过有1个包含了许多uniform bucket的list bucket或tree bucket的二层结构生成了RUSHP和RUSHT。尽管RUSH的固定的集群表示方式无法使用放置规则或跨故障域放置副本（CRUSH用此来提高数据安全性），但我们还是考虑了其性能和数据迁移行为。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:6:0","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"4.1 数据分布 CRUSH的数据分布看上去应该是随机的（与对象标识符$x$或存储目标无关），且应在权重相同的device间形成均衡地分布。我们凭经验各种bucket类型的device间的对象分布，并比较了设备利用率与二项分布间的差异（二项分布是我们在理论上从完全均匀随机过程中得出的期望表现）。当我们分布$n$个对象且将每个对象放置在给定设备$i$上的概率是$ p = \\frac{ w_i }{ W } $时，按相应的二项分布$b(n,p)$得到的设备利用率期望$ \\mu = n p $，标准差$ \\sigma = \\sqrt{np(1-p)} $。在有许多device的大型系统中，我们可以近似$ 1-p \\backsimeq 1 $，这样标准差$ \\sigma \\backsimeq \\sqrt{ \\mu } $——即使当数据对象的数量很多时，利用率也很高注4:。正如期望的那样，我们发现对于由相同device组成的集群和不同权重的device组成的集群中，CRUSH的分布的均值与标准差始终与二项分布的相匹配。 注4：在有很多对象时（即当n很大时），二项分布近似于高斯分布。 4.1.1 过载保护 尽管CRUSH在有大量对象的情况实现了良好的均衡（设备利用率的方差小），但和任何随机过程一样，向任意的某个利用率明显大于均值的device分配对象的概率仍是非零的。不像已有的按概率的映射算法（包括RUSH），CRUSH有一个校正每个device复杂的机制，它可以重新分布一个device上任意比例的数据。这可以在device有过度使用的危险时，按比例缩小对该device的分配，来选择性地拉平过载device的负载。当向已用99%的容量的1000个device组成的集群分布数据时，我们发现尽管这导致了在47%的device上的负载调整，但是CRUSH映射执行时间的增长量却少于20%，且方差减小了4倍（正如预期的那样）。 4.1.2 差异和部分故障 之前的研究[Santos et al. 2000]表明随机的数据分布能让现实中的系统性能与精心的数据分条的性能相媲美（但稍慢一点）。在我们的自己的性能测试中，我们使用CRUSH作为分布式文件存储系统的一部分，我们发现随机对象放置会因OSD工作负载的差异而导致5%的写入性能下降，这与OSD利用率的差异程度有关。然而在实际环境下，这种差异大部分只会在精心设计的分条策略生效的同类工作负载下（通常是写入负载）才会出现。更常见的情况是，工作负载是混合的，且当它们到达磁盘时已经几乎是随机的了（或者至少与磁盘的布局无关），因此随机化测流和分条的策略在device负载和性能方面相似（尽管在精心设计的布局下也是如此），且都降低了差不多的整体吞吐量。我们发现在任何潜在的负载下，与CRUSH在元数据在分配的健壮性方面的缺失相比，其在少量工作负载下较少的性能损失就没那么重要了。 这一分析假设了随着时间变化device的能力几乎是静态的。然而，在真实系统中的经验表明，分布式存储系统的性能常常会被少量的缓慢的、过载的、碎片化的或性能差的device拖慢。传统的显式分配策略可以手动避免这类有问题的device，然而类哈希分布函数通常无法避免。CRUSH可以通过已有的过载校正机制来将这些表现不佳的device视为”部分故障“，从这些device上转移适量的数据和负载以避免这种性能瓶颈，并随着时间推移校正不均衡的负载。 正如DSPTF算法[Lumb et al. 2004]证明的那样，存储系统的细粒度的负载均衡能通过将读负载分摊到多个数据副本上来进一步减轻设备负载的差异。尽管这种方法与CRUSH互补，但是其不在CRUSH映射函数和本文的讨论范围内。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:6:1","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"4.2 重组织和数据移动 我们在由7290个device的集群上对使用CRUSH和RUSH的系统在因新增或移除设备时导致的数据移动进行了评估。CRUSH集群深度有4层：9个row，每个row有9个cabinet，每个cabinet有9个shelf，每个shelf有10个device，总计7290个device。RUSHP和RUSHT等于一个2层CRUSH映射，它们分别包含单个tree bucket或list bucket，其下有729个uniform bucket，每个uniform bucket有10个device。其测量结果将与理论最优的数据迁移量$ m_{optimal} = \\frac{ \\Delta w }{ W } $进行对比，其中$ \\Delta w $新增或移除的device的总权重，$W$是系统的总权重。例如，当系统容量加倍时，最优的重组织策略需要将半数的已有的数据迁移到新device上。 图5 CRUSH集群架构在4层架构的第2层中增加或移除device后的重组织效率与RUSH\u003csub\u003eP\u003c/sub\u003e和RUSH\u003csub\u003eT\u003c/sub\u003e的对比。1是最优的。图5 CRUSH集群架构在4层架构的第2层中增加或移除device后的重组织效率与RUSH\u003csub\u003eP\u003c/sub\u003e和RUSH\u003csub\u003eT\u003c/sub\u003e的对比。1是最优的。 \" 图5 CRUSH集群架构在4层架构的第2层中增加或移除device后的重组织效率与RUSHP和RUSHT的对比。1是最优的。 图5以移动因子$ m_{actual} / m_{optimal} $的性质展示了相对的重组织效率，其中1表示最优的对象数，越大的值意味着需要越多的移动。X轴是增加或移除的OSD的双，Y轴是移动因子的对数。在所有的情况下，更大的权重变换（与整个系统相比）会导致更高效的重组织。RUSHP（单个大型list bucket）在极端情况下遥遥领先，即添加存储带来的移动最少（最优），而移除存储带来的移动最多（此时会有严重的性能下降，见章节4.3）。CRUSH中由list bucket（仅在只有存储添加的情况）或straw bucket组成的多层结构有次少的数据移动。使用tree bucket的CRUSH的效率稍微低一下，但是比朴素的RUSHT好了几乎25%（由每个tree bucket中的9个item带来的轻微的的不均衡导致）。而正如预期的那样（见章节3.3），在由list bucket构建的CRUSH结构中移除存储的效率很差。 图6 在不同类型的bucket中添加或移除item后的重组织效率。1是最优的。尽管从list bucket的尾部移除item时会导致最坏的情况，straw bucket和list bucket通常仍是最佳选择。tree bucket的性能受bucket大小的对数限制。图6 在不同类型的bucket中添加或移除item后的重组织效率。1是最优的。尽管从list bucket的尾部移除item时会导致最坏的情况，straw bucket和list bucket通常仍是最佳选择。tree bucket的性能受bucket大小的对数限制。 \" 图6 在不同类型的bucket中添加或移除item后的重组织效率。1是最优的。尽管从list bucket的尾部移除item时会导致最坏的情况，straw bucket和list bucket通常仍是最佳选择。tree bucket的性能受bucket大小的对数限制。 图6展示了在向不同类型的bucekt添加或移除内部item时的重组织效率。tree bucket变更的移动因子受二叉树的深度$ \\log n $限制。向straw bucket和list bucket添加item几乎是最右的。uniform bucket的变更会导致整体数据的重调配。对链表尾部处的修改（例如移除最老的device）几乎会造成与bucket大小所占的比例相似的数据移动。尽管受这些限制，在整个存储结构中很少移除device时且要减小扩展带来的性能影响时，list bucket可能是合适的。结合了uniform bucket、list bucket、tree bucket和straw bucket的融合方法能在大多数常见的重组织场景下减小数据移动量，且仍能够维护良好的映射性能。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:6:2","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"4.3 算法性能 CRUSH映射的计算是为了快速而设计的，它能以$O( \\log n )$计算n个OSD组成的集群的映射——因此devoce能够快速定位任何对象，或在集群映射变更后快速地重新评估它们已经保存了的对象的适当的存储目标。我们通过在不同大小的集群上的100万次映射检验了CRUSH的性能，并将其与RUSHP和RUSHT进行对比。图7展示了将一个副本集合映射到完全由8个item组成的tree bucket和uniform bucket的CRUSH集群（结构深度不同）的平均时间（ms），并与由固定的2层结构的RUSH进行对比。X轴是系统中device的数量，其坐标是对数的，因此它对应着存储结构的深度。CRUSH的性能与device的数量呈对数变化。RUSHT的性能比CRUSH好一些，因为其稍稍简化了代码的复杂度。性能紧随其后的是list bucket和straw bucket。在本测试中RUSHP是线性变化的（在32768个divice上消耗的时间比CRUSH长25倍），而在现实场景中，新部署的磁盘大小是随时间指数增长的，这会对其线性变化稍有改善 [Honicky and Miller 2004]。这些测试是在2.8GHz Pentium 4上进行的，总映射时间为数十微妙。 图7 随着层次结构大小变化，CRUSH和RUSH\u003csub\u003eT\u003c/sub\u003e的计算时间按对数变化，而RUSH\u003csub\u003eP\u003c/sub\u003e线性变化。图7 随着层次结构大小变化，CRUSH和RUSH\u003csub\u003eT\u003c/sub\u003e的计算时间按对数变化，而RUSH\u003csub\u003eP\u003c/sub\u003e线性变化。 \" 图7 随着层次结构大小变化，CRUSH和RUSHT的计算时间按对数变化，而RUSHP线性变化。 CRUSH的效率取决于存储结构的深度和其bucket的类型。图8比较了$c(r,x)$从每种类型的bucket中选取一个副本的时间（Y轴）作为bucket大小（X轴）的函数。在上层中，CRUSH按$O( \\log n )$变化（即随结构深度线性变化），其每个bucket在不超过固定的最大大小下可能是$O(n)$的（list bucket和straw bucket线性变化）。不同的bucket类型在何时何处使用应取决于期望的新增、移除、权重修改的数量。list bucket提供了比straw bucket稍好一点的性能，然而在移除时可能导致过多的数据重调配。tree bucket在计算和重组织开销很客观的非常大或经常被修改的bucket中是一种很好的选择。 图8 将副本映射到每个CRUSH bucket时下层速度与bucket大小的关系。uniform bucket需要常数时间，tree bucket需要对数时间，list bucket和straw bucket需要线性时间。图8 将副本映射到每个CRUSH bucket时下层速度与bucket大小的关系。uniform bucket需要常数时间，tree bucket需要对数时间，list bucket和straw bucket需要线性时间。 \" 图8 将副本映射到每个CRUSH bucket时下层速度与bucket大小的关系。uniform bucket需要常数时间，tree bucket需要对数时间，list bucket和straw bucket需要线性时间。 无论是执行时间还是结果的质量，CRUSH性能的核心来自于对整型哈希函数的使用。伪随机值是通过基于Jenkin的32位哈希混合[Jenkins 1997]的多输入整型哈希函数计算的。在目前的形式中，CRUSH映射函数的45%的时间都花费在了哈希值计算上，因此哈希是整体性能和分布质量的关键，它也是优化的重要目标。 4.3.1 被忽略的老化问题 CRUSH保留了故障的device在存储结构中的位置，因为通常故障时暂时的（故障设备通常会被替换）且这能够避免无效的数据重组织。如果忽略了存储系统的老化问题，发生故障但没被替换的deivce的数量可能会变得很大。尽管CRUSH会将数据重新分布到没有故障的device上，因为在放置算法中发生回溯的可能性变高了，它会带来少量的性能损失。我们在被标记为故障的device比例不同的1000个device组成的集群上评估了映射速度。在半数的设备故障的相对极端的场景下，映射计算时间增加了71%。（因为其现象是每个device的负载翻倍导致I/O性能的严重下降，所以这种情况很可能被这一现象掩盖。） ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:6:3","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"4.4 可靠性 对于大型存储系统来说，数据安全性是很重要的，因为设备数量大导致硬件故障的发生是很正常的。像CRUSH这种分簇分布的随机分布策略备受关注，因为它们扩大了与任意给定device共享数据的对等节点的数量。（通常来说，）有两个相互竞争且影响相反的因素。第一，因为副本数据更少的位能分布在更大的对等节点集合上，因此故障后的恢复能够并行执行，这减少了恢复时间且将系统的脆弱点的范围缩小到了其它故障上。第二，更大的对等节点的组意味着因同时发生了第二个故障而导致流丢失共享数据的可能性更高。在使用2路镜像时，这两个因素相互抵消；而在超过两份副本的情况下，数据的安全性随着分簇程度提高 [Xin et al. 2004]。 然而，多故障（multiple failure）情况的关键问题是：通常，故障可能不是独立的——在许多情况下，像电源故障或物理干扰这样的单个事件会影响到多个device，且采用分簇副本策略的更大型的对等节点组会增大数据丢失的可能性。CRUSH可将副本分散到用户定义的不同故障域中（在RUSH或已有的基于哈希的策略中都没有这一功能），这是专门为了防止并发的关联故障导致的数据丢失设计的。尽管这可以明显降低风险，但是缺少用来研究的特定的存储集群配置和相关的历史故障数据时，很难量化整个系统可靠性提高的程度。我们希望在以后进行这样的研究，但这超出了本文讨论的范围。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:6:4","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"5. 展望 CRUSH正在作为Ceph的一部分进行开发，Ceph是一个数PB级的文件系统。目前的研究包括一个主要基于CRUSH的独有特性的智能、可靠的分布式对象存储。目前CRUSH使用的原始规则结构的复杂度刚好足以支持我们目前设想的数据分布策略。一些系统的特殊需求需要更强大的规则结构来满足。 尽管对意外故障发生时的安全性考虑是驱动CRUSH设计的主要目标，但是在可以用马尔可夫或其它定量模型评估其对系统平均数据丢失时间（MTTDL）的精确的影响前，还需要研究实际系统中的故障，以确定故障的特征与频率。 CRUSH的性能高度依赖于较为强大的多输入整型哈希函数。因为它同时影响着算法的正确性（分布的质量）与速度，因此，有必要对足以在CRUSH中使用的更快的哈希技术进行研究。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:7:0","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"6. 结论 分布式存储系统对数据放置提出了一系列的独特的可扩展性的挑战。CRUSH通过将数据放置问题转化为伪随机映射函数解决了这些挑战，它消除了常见的元数据分配需求，转为通过基于用来描述可用存储的加权层次结构的方式分布数据。集群映射的层次接口能够反映出下层物理设备的组织情况和设施的基础架构情况，例如，在数据中心中，device被组成shelf、cabinet、和row，从而使自定义的放置规则可以定义各种策略以能够将对象的不同副本分布到不同的用户定义的故障域中（如不同的电力和网络基础设施中）。这样，CRUSH能够减轻通常在已有的采用分簇副本策略的伪随机系统中的关联设备故障问题带来的问题。CRUSH还通过选择性地将数据以最小计算代价地从装的太多的device迁出，解决了随机化的方法固有的设备中数据过量装载的风险。 CRUSH以极为高效的方式完成了所有这些任务，在计算效率和所需的元数据方面都是如此。映射计算的运行时间为$O( \\log n )$，在数千台device的情况下执行时仅需几十毫秒。CRUSH集高效、可靠、灵活与一身，使其成为大型分布式存储系统的理想选择。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:8:0","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"7. 致谢 R. J. Honicky’s excellent work on RUSH inspired the development of CRUSH. Discussions with Richard Golding, Theodore Wong, and the students and faculty of the Storage Systems Research Center were most helpful in motivating and refining the algorithm. This work was supported in part by Lawrence Livermore National Laboratory, Los Alamos National Laboratory, and Sandia National Laboratory under contract B520714. Sage Weil was supported in part by a fellowship from Lawrence Livermore National Laboratory. We would also like to thank the industrial sponsors of the SSRC, including Hewlett Packard Laboratories, IBM, Intel, Microsoft Research, Network Appliance, Onstor, Rocksoft, Symantec, and Yahoo. ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:9:0","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"8. 更多 CRUSH的源码采用LGPL协议，可通过https://users.soe.ucsc.edu/~sage/crush访问。 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:10:0","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"参考文献 ANDERSON, E., HALL, J., HARTLINE, J., HOBBS, M., KARLIN, A. R., SAIA, J., SWAMINATHAN, R., AND WILKES, J. 2001. An experimental study of data migration algorithms. In Proceedings of the 5th International Workshop on Algorithm Engineering, SpringerVerlag, London, UK, 145–158. ANDERSON, E., HOBBS, M., KEETON, K., SPENCE, S., UYSAL, M., AND VEITCH, A. 2002. Hippodrome: running circles around storage administration. In Proceedings of the 2002 Conference on File and Storage Technologies(FAST). AZAGURY, A., DREIZIN, V., FACTOR, M., HENIS, E., NAOR, D., RINETZKY, N., RODEH, O., SATRAN, J., TAVORY, A., AND YERUSHALMI, L. 2003. Towards an object store. In Proceedings of the 20th IEEE / 11th NASA Goddard Conference on Mass Storage Systems and Technologies, 165–176. BRAAM, P. J. 2004. The Lustre storage architecture. http://www.lustre.org/documentation.html, Cluster File Systems, Inc., Aug. BRINKMANN, A., SALZWEDEL, K., AND SCHEIDELER, C. 2000. Efficient, distributed data placement strategies for storage area networks. In Proceedings of the 12th ACM Symposium on Parallel Algorithms and Architectures (SPAA), ACM Press, 119–128. Extended Abstract. CHOY, D. M., FAGIN, R., AND STOCKMEYER, L. 1996. Efficiently extendible mappings for balanced data distribution. Algorithmica 16, 215–232. GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. 2003. The Google file system. In Proceedings of the 19th ACM Symposium on Operating Systems Principles (SOSP ’03), ACM. GOBIOFF, H., GIBSON, G., AND TYGAR, D. 1997. Security for network attached storage devices. Tech. Rep. TR CMU-CS-97-185, Carniege Mellon, Oct. GOEL, A., SHAHABI, C., YAO, D. S.-Y., AND ZIMMERMAN, R. 2002. SCADDAR: An efficient randomized technique to reorganize continuous media blocks. In Proceedings of the 18th International Conference on Data Engineering (ICDE ’02), 473–482. GRANVILLE, A. 1993. On elementary proofs of the Prime Number Theorem for Arithmetic Progressions, without characters. In Proceedings of the 1993 Amalfi Conference on Analytic Number Theory, 157–194. HONICKY, R. J., AND MILLER, E. L. 2004. Replication under scalable hashing: A family of algorithms for scalable decentralized data distribution. In Proceedings of the 18th International Parallel \u0026 Distributed Processing Symposium (IPDPS 2004), IEEE. JENKINS, R. J., 1997. Hash functions for hash table lookup. http://burtleburtle.net/bob/hash/evahash.html. KARGER, D., LEHMAN, E., LEIGHTON, T., LEVINE, M., LEWIN, D., AND PANIGRAHY, R. 1997. Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the World Wide Web. In ACM Symposium on Theory of Computing, 654–663. LUMB, C. R., GANGER, G. R., AND GOLDING, R. 2004. D-SPTF: Decentralized request distribution in brick-based storage systems. In Proceedings of the 11th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 37–47. NAGLE, D., SERENYI, D., AND MATTHEWS, A. 2004. The Panasas ActiveScale storage cluster—delivering scalable high bandwidth storage. In Proceedings of the 2004 ACM/IEEE Conference on Supercomputing (SC ’04). RODEH, O., AND TEPERMAN, A. 2003. zFS—a scalable distributed file system using object disks. In Proceedings of the 20th IEEE / 11th NASA Goddard Conference on Mass Storage Systems and Technologies, 207–218. SAITO, Y., FRØLUND, S., VEITCH, A., MERCHANT, A., AND SPENCE, S. 2004. FAB: Building distributed enterprise disk arrays from commodity components. In Proceedings of the 11th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 48–58. SANTOS, J. R., MUNTZ, R. R., AND RIBEIRO-NETO, B. 2000. Comparing random data allocation and data striping in multimedia servers. In Proceedings of the 2000 SIGMETRICS Conference on Measurement and Modeling of Computer Systems, ACM Press, Santa Clara, CA, 44–55. SCHMUCK, F., AND HASKIN, R. 2002. GPFS: A shareddisk file system for large computing clusters. In Proceedings of the 2002 ","date":"2020-10-01","objectID":"/posts/paper-reading/weil-crush-sc06/:11:0","tags":["CRUSH","Translation"],"title":"《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译","uri":"/posts/paper-reading/weil-crush-sc06/"},{"categories":["Paper Reading"],"content":"本篇文章是对论文In Search of an Understandable Consensus Algorithm (Extended Version)的原创翻译，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:0:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"作者 Diego Ongaro and John Ousterhout Stanford University ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:1:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"摘要 Raft是一个用来管理多副本日志（replicated log）的共识算法。其作用与（multi-）Paxos相同、效率与Paxos想用，但结构与Paxos不同；这让Raft比Paxos更容易理解，且Raft为构建实用的系统提供了更扎实的基础。为了提高可理解性，Raft将共识的关键元素分离为：领导选举、日志复制、和安全性；且其增强了连贯性（coherency）译注1，以减少必须考虑的状态数。用户学习结果表明，对于学生来说，Raft比Paxos更容易学习。Raft还包括一个用于变更集群成员的新机制，其使用重叠的大多数来保证安全性。 译注1：本文的连贯性指coherency，在很多翻译中将其翻译成了一致性，这样容易与consistency混淆，二者间存在一定差异。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:2:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"1. 引言 共识算法让一组机器能像能容忍一些成员故障的一个连贯组一样工作。因为这一点，它们在构建可靠的大规模软件系统中扮演者关键角色。Paxos[15, 16]在过去的十年中主导了共识算法的讨论：大多数共识的实现都基于Paxos或受其影响，且Paxos成为了用来教授学生有关共识知识的主要工具。 不幸的是，Paxos相当难以理解，尽管有很多使其更易接受的尝试。另外，其架构需要复杂的修改以支持实用的系统。其结果是，系统构建者和学生都很受Paxos困扰。 在我们自己饱受Paxos困扰后，我们开始寻找一个能够为系统构建和教育提供更好的基础的新的共识算法。我们的方法不太寻常，因为我们的主要目标是可理解性：我们能否定义一个为实用系统设计的共识算法，并用一个比Paxos更容易学习的方式描述它？此外，我们希望算法能够让开发更加直观，这对系统构建者来说是很重要的。重要的不光是算法，还有为什么算法能工作。 这项工作的成果是一个被称为Raft的共识算法。在设计Raft时，我们使用了特殊的方法来提高可理解性，包括算法分解（Raft将领导选举、日志复制、和安全性分离开来）和减少状态空间（与Paxos相比，Raft减少了不确定性，且该方法允许服务器相互不一致）。对两个大学的43个学生的研究表明，Raft比Paxos更好理解得多：在学习这两种算法后，这些学生中的33名能够更好得回答有关Raft的问题。 Raft与现有的共识算法在很多方面都很相似（最明显的时候，Oki和Liskov的Viewstamped Replication[29, 22]），但Raft有很多新特性： 强leader： 与其它共识算法相比，Raft使用了更强的领导权形式。例如，日志条目(log entry)仅从leader流向其它服务器。这简化了对多副本日志的管理，并使Raft更容易理解。 领导选举： Raft使用随机计时器来选举leader。这仅在任何共识算法都需要的心跳机制上增加了很小的机制，但能够简单又快速地解决冲突。 成员变更： Raft用来变更集群中服务器集合的机制使用了一个新的*联合共识（joint consensus）*方法，其两个不同配置中的大多数服务器会在切换间有重叠。这让集群能够在配置变更时正常地继续操作。 我们认为，无论为了教育目的还是作为实现的基础，Raft都比Paxos和其它共识算法更优秀；Raft的描述足够完整，能够满足使用系统的需求；Raft有很多开源实现并已经被一些公司使用；Raft的安全性性质已经被形式化定义并证明；Raft的效率与其它算法相似。 本文的剩余部分介绍了多副本状态机问题（第二章），讨论了Paxos的优势与劣势（第三章），描述了我们为了可理解性使用的通用方法（第四章），给出了Raft共识算法（第5~8章），评估了Raft（第九章），并讨论了相关工作（第十章）。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:3:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"2. 多副本状态机 共识算法通常在*多副本状态机问题（replicated state machine problem）*[37]的上下文中出现。通过这种方法，在一系列服务器上的状态机会计算相同状态的相同副本，且即使在一些服务器宕机是也可以继续操作。多副本状态机被用来解决分布式系统中各式各样的容错问题。例如，有单集群leader的大型系统（如GFS[8]、HDFS[38]、和RAMCloud[33]）通常使用独立的多副本状态机来管理领导选举并存储必须能在leader崩溃时幸存的配置信息。多副本状态机的例子还包括Chubby[2]和ZooKeeper[11]。 多副本状态机通常使用多副本日志实现，如图1所示。每个服务器存储一个包含一系列指令的日志，状态机会按照顺序执行日志。每个日志包含相同顺序的相同指令，因此每个状态机会处理相同的指令序列。因为状态机是确定的，每个状态机都会计算出相同的状态并得出相同的输出序列。 图1 多副本状态机架构。共识算法管理由来自不同客户端的状态及指令组成的多副本日志。状态机按照日志处理相同的指令序列，因此它们会产生相同的输出。图1 多副本状态机架构。共识算法管理由来自不同客户端的状态及指令组成的多副本日志。状态机按照日志处理相同的指令序列，因此它们会产生相同的输出。 \" 图1 多副本状态机架构。共识算法管理由来自不同客户端的状态及指令组成的多副本日志。状态机按照日志处理相同的指令序列，因此它们会产生相同的输出。 保持多副本日志的一致性是共识算法的任务。服务器上的共识模块会接收来自客户端的指令，并将其添加到它的日志中。它与其它服务器上的共识模块通信来确保每个日志最终包含相同顺序的相同请求，即使一些服务器故障也是如此。一旦指令被恰当地多副本化，每个服务器的状态机就可以按日志顺序处理它们，并将输出返回给客户端。这样，所有服务器对外会表现为单个高可靠性的状态机。 为实用系统设计的共识算法通常有如下属性： 它们确保所有非拜占庭条件下的安全性（永远不会返回错误结果），需要处理的问题包括网络延迟、分区、丢包、重复、和乱序。 只要大多数服务器可以操作那么其所有功能都可用，且能够与相遇通信或与客户端通信。因此，通常使用的由5个服务器组成的集群能够容忍任意2个服务器故障。服务器被假设可能宕机停止；它们也可能在随后从稳定存储中恢复并重新加入集群。 它们不依赖定时来保证日志的一致性：在最坏的情况下，时钟故障和极端的消息延迟会导致可用性问题。 在通常情况下，一条指令能在集群的大多数响应一轮远程过程调用（RPC）后完成；少数的较慢的服务器不会影响整个系统的性能。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:4:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"3. Paxos有什么问题？ 在过去十年中，Leslie Lamport的Paxos协议[15]几乎和共识成了同义词：Paxos是在课程中最常被教授的协议，也是大多数共识实现的起点。Poxos首先定义了一个能够对单个决策达成一致的协议，如单个多副本日志条目。我们称这个子集为单决策Paxos（single-decree Paxos）。Paxos接着将该协议的多个实例结合，以实现一系列的决策，例如一个日志（multi-Paxos）。Paxos同时确保了安全性和活性（liveness），且它支持集群中成员的变更。它的正确性已经被证明，且Paxos在一般场景下很高效。 不幸的是，Paxos有两个显著的劣势。第一个劣势是Paxos非常难以理解。众所周知，Paxos的完整解释[15]非常隐晦；只有很少的人在付出很大努力后才能成功理解它。因此，出现了很多试图通过更简单的方式解释Paxos的尝试[16, 20, 21]。这些解释着手于单决策Paxos这一子集，尽管这仍很有挑战。在对NSDI2012出席者的非正式调查中，我们发现尽管在经验丰富的研究者中，也几乎没有人觉得Paxos容易。我们自己就受Paxos困扰，直到阅读了一些简化的解释后我们才理解了完整的协议，所以我们设计了自己的替代的协议，这一过程花了差不多一年时间。 我们假设Paxos的隐晦性来自于其选择了单决策子集作为其基础。单决策Paxos很冗杂且隐晦：单决策Paxos被分为两个阶段，其没有简单的直观解释也不能被单独理解。因此，人们很难对单决策协议为什么可行建立一个直观认识。Multi-Paxos的规则由增加了很大的额外的复杂性和隐晦性。我们认为达到多决策共识（例如，一个日志而不是单个日志条目）的整个问题可被分解为更直观更显然的其他方式。 Paxos的第二个问题是它没有为构建实用的实现提供良好的基础。其原因之一是人们对multi-Paxos算法没有广泛的一致意见。Lamport的描述几乎都关于单决策Paxos；他概括了multi-Paxos的可能的方法，但缺少许多细节。后来出现了很多试图具体化并优化Paxos的尝试，如[26, 39, 13]，但这些方法互相之间都不一样且与Lamport的蓝图也不通。像Chubby[4]这样的系统实现了类Paxos算法，但在大多数条件下的细节都没有发表。 此外，Paxos架构对构建实用系统来说很弱；这时将算法分解为单决策的另一个后果。例如，单独选取一组日志中的每个条目并将它们合并为一个顺序日志并没有什么还出；这样做只会增加复杂性。设计一个围绕系统的日志更加简单且高效，其中新日志条目可以按照受约束的顺序被依次添加。另一个问题是，Paxos的核心使用了一个对称的（symmetric）对等(peer-to-peer)方法（尽管其最后提出了一个弱领导权方法作为性能优化）。这在仅需要做一个决策的简单的世界中是有意义的，但是对于使用这种方法的实用系统来说意义不大。如果必须做出一系列决策，那么先选举出一个leader更简单却更快，随后让该leader协调决策。 因此，使用系统与Paxos相似之处很少。每个实用系统的实现首先都会从Paxos开始，然后发现很难实现它，接着开发了一个非常复杂的架构。这很消耗时间且容易出错，且Paxos难以理解加剧了这一问题。Paxos的表达形式可能对于证明其理论正确性来说很好，但是因为其真实实现与Paxos太过不同，这样理论证明就失去了价值。来自Chubby的实现者的评论尤为典型： 引用 现实系统的需求的与Paxos算法的描述之间有很大的隔阂。为了构建现实的系统，专家需要使用分散在各种文献中的许多思想，并作出一些较小的协议扩展。这些不断累积的扩展会非常多，最后系统会基于一个未被证明的协议。[4] 因为这些问题，我们总结出Paxos没有为系统构建和教学提供良好的基础。考虑到共识对大型软件系统的重要性，我们决定尝试我们能否构建出一个能替代Paxos的且比Paxos的属性更好的共识算法。Raft就是这一实验的成果。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:5:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"4. 为可理解性做出的设计 我们在设计Raft时有许多目标：它必须为系统构建提供完整且实用的基础，这样就能大量减少开发者所需的设计工作；它必须在所有条件下都安全，在典型的操作条件下可用；它必须能在通用操作中保持高效。但我们最重要的目标是可理解性，这也是最难的挑战。它必须能被大量读者容易地理解。另外，它必须能够建立对算法的直观直觉，这样系统构建者可以对其进行扩展，这在真实实现中是不可避免的。 在我们设计Raft时，有很多要点不得不选择替代方法。在这些场景中，我们基于可理解性对这些替代方法进行了评估：解释每个替代方法有多难？（例如：其状态空间多复杂？其有没有难以捉摸的实现？）读者完整理解该方法并实现它有多简单？ 我们意识到这样的分析有很大的主观性；尽管如此，我们还是使用了两种能使其更容易被大家接受的方法。第一个方法是众做周知的问题分解方法：我们尽可能地将问题划分为能被解决、解释并理解的相对独立的子问题。例如，在Raft中，我们将其分为领导选举、日志复制、安全性和成员变更。 我们的第二个方法是通过减少需要考虑的状态数来简化状态空间，使系统鞥具有连贯性并尽可能消除不确定性。特别是，日志不允许有“洞”（译注：这里的“洞”指日志间的空隙，见Ceph的论文。），且Raft限制了日志变得与其它不一致的方式。尽管在大多数情况下，我们试图消除不确定性，但是有些情况下不确定性实际上可以提高可理解性。在实践中，随机化的方法引入了不确定性，但是它们通常会通过用相同的方法解决所有可能的选择，因此减少了状态空间。我们使用的随机化的方法简化了Raft的领导选举算法。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:6:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"5. Raft共识算法 Raft是一种用来管理第二章中描述的形式的多副本日志的算法。图2以浓缩的形式总结了算法以供参考，图3列出可算法的关键性质；这些图中元素将在本章剩下的部分分条讨论。 图2 对Raft共识算法的浓缩总结（不包括成员变更和日志压缩）。服务器的行为在左上角的的格子中被作为一系列独立且可重复的触发器规则描述。像“§5.2”这样的章节号表示某个特征将在哪一节中讨论。正式定义\u003csup\u003e[31]\u003c/sup\u003e将会更精确地描述算法。图2 对Raft共识算法的浓缩总结（不包括成员变更和日志压缩）。服务器的行为在左上角的的格子中被作为一系列独立且可重复的触发器规则描述。像“§5.2”这样的章节号表示某个特征将在哪一节中讨论。正式定义\u003csup\u003e[31]\u003c/sup\u003e将会更精确地描述算法。 \" 图2 对Raft共识算法的浓缩总结（不包括成员变更和日志压缩）。服务器的行为在左上角的的格子中被作为一系列独立且可重复的触发器规则描述。像“§5.2”这样的章节号表示某个特征将在哪一节中讨论。正式定义[31]将会更精确地描述算法。 图3 Raft在所有时刻都保证这些性质的每一条都成立。章节号表示每条性质将在哪一节中讨论。图3 Raft在所有时刻都保证这些性质的每一条都成立。章节号表示每条性质将在哪一节中讨论。 \" 图3 Raft在所有时刻都保证这些性质的每一条都成立。章节号表示每条性质将在哪一节中讨论。 Raft通过先选举一个高级leader然后给予该leader管理分布式日志的所有责任的方式实现共识。该leader接收来自客户端的日志条目，将它们复制到其它服务器上，然后告诉服务器什么时候可以安全地将这些日志条目应用到它们的状态机中。使用leader可以简化对分布式日志的管理。例如，leader可以在不询问其它服务器的情况下决定将日志的新条目放在哪儿，且数据流仅简单地从leader流向其它服务器。leader可能发生故障也可能在其它服务器中失去对其的连接，在这种情况下，会有新的leader被选举出来。 考虑leader的方法，Raft将共识问题分解成三个相对独立的子问题，接下来的小节会对这些问题进行讨论： 领导选举： 当已有的leader故障时必须有新的leader被选举出来（章节5.2）。 日志复制： leader必须接收来自客户端的日志条目，并将它们复制到集群中，强制其它日志对它自己的日志达成一致（章节5.3）。 安全性： Raft中关键的安全性是图3中的状态机安全性（State Machine Safety Property）：如果任意服务器将一个特定的日志条目应用到了其状态机中，那么不会有应用了有相同index（索引）的其他指令的日志条目的服务器。章节5.4描述了Raft如何确保这一性质；其解决方案包括对章节5.2中描述的选举机制的一个额外的约束。 在给出共识算法后，本章讨论了可用性问题和定时在本系统中的角色。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:7:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"5.1 Raft基础 一个Raft集群包括多个服务器；通常数量是5，这可以让系统能够容忍2个服务器故障。在给定时间内，每个服务器处于以下三个状态之一：leader、follower、或candidate。在正常的操作中，会有恰好一个leader，所有其它服务器都是follower。follower是被动的：它们不会自己提出请求，而仅响应来自leader和candidate的请求。leader处理所有的客户端请求（如果客户端联系了一个follower，该follower会将其重定向到leader）。第三个状态candidate，在章节5.2中描述的选举新leader时使用。图4展示了状态和状态间的转移；状态转移将在后文中讨论。 图4 服务器状态。follower仅响应来自其它服务器的请求。如果follower没有收到通信，那么它会变为candidate并开始一次选举。收到了来自整个集群中大多数节点投票的candidate会成为新的leader。leader通常会持续到其故障。图4 服务器状态。follower仅响应来自其它服务器的请求。如果follower没有收到通信，那么它会变为candidate并开始一次选举。收到了来自整个集群中大多数节点投票的candidate会成为新的leader。leader通常会持续到其故障。 \" 图4 服务器状态。follower仅响应来自其它服务器的请求。如果follower没有收到通信，那么它会变为candidate并开始一次选举。收到了来自整个集群中大多数节点投票的candidate会成为新的leader。leader通常会持续到其故障。 Raft将时间划分为任意长度的term（任期），如图5所示。term被编号为连续的整数。每个term从选举（election）开始，在选举中一个或多个candidate会试图成为leader，就像章节5.2中描述的那样。如果candidate赢得选举，那么它将在该term余下的时间了作为leader提供服务。在某些情况下，一次选举可能导致投票决裂，此时该term最终可能没有leader，那么很快会开始一个新的term（伴随一次新的选举）。Raft确保一个给定的term中最多只会有一个leader。 图5 时间被划分为term，每个term从选举开始。在一次成功选举后，单个leader会管理集群，直到该term结束。有些选举会失败，在这种情况下，term会不选择leader就结束。term之间的转换可以在不同服务器上的不同时间被观测到。图5 时间被划分为term，每个term从选举开始。在一次成功选举后，单个leader会管理集群，直到该term结束。有些选举会失败，在这种情况下，term会不选择leader就结束。term之间的转换可以在不同服务器上的不同时间被观测到。 \" 图5 时间被划分为term，每个term从选举开始。在一次成功选举后，单个leader会管理集群，直到该term结束。有些选举会失败，在这种情况下，term会不选择leader就结束。term之间的转换可以在不同服务器上的不同时间被观测到。 不同的服务器可能在不同时间观测到term的装换，且在一些情况下，服务器可能没有观测到选举甚至没观测到整个term。term在Raft扮演逻辑时钟[14]的角色，且term能让服务器检测到过时的（obsolete）信息，如陈旧的（stale）leader。每个服务器会存储当前的term号，其随时间单调递增。当前的term号在任何服务器通信时都会被交换，日过服务器当前的term小于其它服务器的，那么它会更新其term号到到较大值。如果candidate或leader发现它的term过期了，它会立刻转到follower状态。如果服务器收到了有陈旧的term号的请求，它会拒绝该请求。 Raft服务器使用远程过程调用（remote procedure call，RPC）通信，且基本的共识算法仅需要两种RPC。RequestVote RPC在选举时由candidate发起（章节5.2），而AppendEntries RPC被leader发起，用来复制日志条目和提供心跳（章节5.3）。第七章加入了第三个RPC，用来在服务器间传输快照。如果服务器没有及时收到响应，它们会重试RPC，且它们会并行地发起RPC以获得最佳性能。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:7:1","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"5.2 领导选举 Raft使用心跳机制来触发领导选举。当服务器启动时，它们按照如下方式开始。只要服务器能收到来自leader或candidate的合法的RPC，它就会保持follower状态。leader会定期发送心跳（不携带任何日志条目的AppendEntries RPC）给所有follower，以维护其权威。如果follower在超过一段时间后（这段时间被称为选举超时时间，election timeout）仍没受到通信，那么它会假设当前没有可行的leader，并开始一次选举以选择一个新leader。 为了开始一次选举，follower会增大其当前的term，并转换到candidate状态。其随后为自己投票，并并行地给集群中每个其它的服务器发起RequestVote RPC。candidate会保持其状态，知道以下三件事情之一发生：（a）它赢得了选举；（b）另一个服务器成为了leader；（c）一段时就过后仍没有胜者。这些后果将在后文中分别讨论。 如果candidate收到了整个集群中大多数相同term服务器的投票，那么它会赢得选举。在一个给定的term中，每个服务器会按先到先得（first-come-first-served）的方式给最多一个candidate投票（注意：章节5.4对投票增加了一个额外约束）。“大多数”规则确保了在特定的term中最多只有一个candidate能赢得选举（选举安全性如图3所示）。一旦candidate赢得选举，它会变成leader。随后它会向所有其他服务器发送心跳消息以建立起权威，并防止新选举发生。 在等待投票时，candidate可能收到来自其它服务器声明自己是leader的AppendEntries RPC。如果leader的term（包括在其RPC中）至少与该candidate当前的term一样大，那么这个candidate会视其为合法的leader，并返回到follower状态。如果RPC中的term比该candidate当前的term小，那么该candidate会拒绝RPC并继续以candidate状态运行。 第三种可能的结果是，candidate既没有赢得选举也没有输：如果许多follower在同时变成了candidate，投票可能决裂，这样可能没有candidate获取大多数的投票。当这种情况发生时，每个candidate都将会超时并通过增大其term和开始另一轮RequestVote RPC来开始新的一轮选举。然而，如果不采取额外措施，投票决裂可能会无限反复。 Raft使用了随机额选举超时时间以确保投票决裂很少发生，且投票决裂可以被快速解决。为了在初次选举时防止投票决裂，选举超时时间会从一个固定的时间段（例如，150~300ms）中随机选取。这会在所有服务器上实现，这样大多数情况下只有单个服务器会超时，它会在任何其它服务器超时前赢得选举并发送心跳。同样的机制还在处理投票决裂时使用（译注：之前介绍的是防止投票决裂，接下来是处理投票决裂）。每个candidate在开始选举时重置其随机选举超时时间的计时器，且它会在下一次选举开始前等待该超时时间流逝，这减少了新的选举中再次发生投票决裂的可能性。章节9.3说明这种方法能快速地选出leader。 选举是可理解性如何指导我们在备选设计中做出选择的一个实例。最初我们计划使用一个排名（ranking）系统：假设每个candidate有一个唯一的排名（rank），这个排名会在选取竞争中的candidate时使用。如果candidate发现了有更高排名的另一个candidate，它会返回follower状态，这样有更高排名的candidate能够更容易地赢得下一次选举。我们发现这种方法制造了有关可用性的隐晦的问题（如果排名较高的服务器故障，排名较低的服务器可能需要等待超时才能再次成为candidate，但是如果它再次成为candidate过早，它可能会重置领导选举的进度）。我们对该算法做了很多次调整，但是在每次调整后都会出现新的小问题。最终我们总结出，随机重试的方法更显而易见且易于理解。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:7:2","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"5.3 日志复制 一旦leader被选举出来，它会开始为客户端的请求提供服务。每个客户端请求包含一条需要被多副本状态机执行的指令。leader将该指令作为新的条目追加到其日志中，随后它会并行地向每台其他的服务器发起AppendEntries RPC复制该条目。当该条目被安全地复制时（正如后文描述的那样），leader会将该条目应用到其状态机中，并将执行的结果返回给客户端。如果follower崩溃或运行缓慢，或者如果网络包丢失，leader会无限重试AppendEntries RPC（即使它已经响应了客户端），知道所有的follower最终存储了所有的日志条目。 图6 日志由条目组成，日志条目被顺序编号。每个日志条目都包含它被创建时的term号（每个方框中的数字）和一条给状态机的指令。如果一个日志条目能被安全地应用到状态机中，它会被视为committed。图6 日志由条目组成，日志条目被顺序编号。每个日志条目都包含它被创建时的term号（每个方框中的数字）和一条给状态机的指令。如果一个日志条目能被安全地应用到状态机中，它会被视为committed。 \" 图6 日志由条目组成，日志条目被顺序编号。每个日志条目都包含它被创建时的term号（每个方框中的数字）和一条给状态机的指令。如果一个日志条目能被安全地应用到状态机中，它会被视为committed。 日志被按照如图6的方式组织。每条日志都保存了一条状态机指令和leader收到该条目时的term号。日志条目中的term号被用作检测日志间的不一致性并确保图3中的一些性质。每个日志条目还有一个标识它在日志中的位置的整数index（索引）。 leader会决定什么时候能够安全地将日志条目应用到状态机，这种条目被称为committed（已提交）的。Raft保证committed的条目时持久性的，且最终将会被所有可用的状态机执行。一旦撞见了日志条目的leader将其如知道了大多数服务器上，那么该条目会变成committed的（例如，图6中的条目7）。这还会提交在leader的日志中所有之前的条目。章节5.4讨论了在leader变更后应用这一规则的一些微妙的情况，其还展示了“提交是安全的”的定义。leader会跟踪它知道的被提交的最高的index，且它会在之后的AppendEntries RPC中包含这个index，这样其它server最终会发现它。一旦follower得知一个日志条目被提交，它会将该条目（按日志顺序）应用到它本地的状态机中。 我们设计的Raft日志机制能维护不同服务器间高级别的连贯性。这不但简化了系统的行为，还使系统更可预测，这时确保安全性的重要部分。Raft维护的如下的性质共同构成了图3中的“日志匹配性质（Log Matching Property）”： 如果不同日志的两个条目有相同的index和term，那么它们存储了相同的指令。 如果不同日志的两个条目有相同的index和term，那么日志中之前的所有条目都是相同的。 第一条性质基于“leader在给定term期间只会创建一条有给定index的日志条目，且日志条目在日志中的位置永远不会改变”的事实。第二条性质由AppendEntries提供的一个简单的一致性校验保证。当发送AppendEntries RPC时，leader会包含其日志中紧随新条目之前的index和term。如果follower在其日志中没有找到有相同index和term的条目，那么它会拒绝该新条目。一致性检验会以归纳的步骤执行：日志最初的空状态满足“日志匹配性质”，且每当日志被扩展时一致性检验会保证“日志匹配性质”。因此，每当AppendEntries成功返回，leader会得知follower的日志和它自己的日志直到新条目的位置都是相同的。 在正常的操作中，leader的日志会和follower的日志保持一致，所以AppendEntries的一致性检验永远不会失败。然而，leader崩溃会使日志不一致（旧的leader可能没有完全复制它的日志中的所有条目）。这些不一致会随着一系列leader和follower的故障加剧。图7给出了follower可能与新leader不同的原因。follower可能丢失了leader有的一些条目，follower可能多出leader没有的一些条目，或者二者都有。日志中丢失或多出的条目可能垮了多个term。 图7 当最上面的leader开始当权时，a~f中的任意情况都可能在follower中发生。每个方框表示一个日志条目，方框中的条目是它的term。follower可能丢失条目（a~b），可能有额外的未被提交的条目（c~d），或者二者都有（e~f）。例如，在情况f中，该服务器是term 2的leader，其将一些条目添加到了它的日志中，然后它在将任意这些条目提交前崩溃了；该服务器快速地重启了并成了了term 3的leader，并又将更多条目添加到了它的日志中；在这些term 2和term 3的条目被提交前，该服务器再次崩溃，并在几个term期间保持离线状态。图7 当最上面的leader开始当权时，a~f中的任意情况都可能在follower中发生。每个方框表示一个日志条目，方框中的条目是它的term。follower可能丢失条目（a~b），可能有额外的未被提交的条目（c~d），或者二者都有（e~f）。例如，在情况f中，该服务器是term 2的leader，其将一些条目添加到了它的日志中，然后它在将任意这些条目提交前崩溃了；该服务器快速地重启了并成了了term 3的leader，并又将更多条目添加到了它的日志中；在这些term 2和term 3的条目被提交前，该服务器再次崩溃，并在几个term期间保持离线状态。 \" 图7 当最上面的leader开始当权时，a~f中的任意情况都可能在follower中发生。每个方框表示一个日志条目，方框中的条目是它的term。follower可能丢失条目（a~b），可能有额外的未被提交的条目（c~d），或者二者都有（e~f）。例如，在情况f中，该服务器是term 2的leader，其将一些条目添加到了它的日志中，然后它在将任意这些条目提交前崩溃了；该服务器快速地重启了并成了了term 3的leader，并又将更多条目添加到了它的日志中；在这些term 2和term 3的条目被提交前，该服务器再次崩溃，并在几个term期间保持离线状态。 在Raft中，leader会通过强制follower复制其日志的方式处理不一致。这意味着follower日志中不一致的条目会被覆盖为leader日志中的条目。章节5.4将展示当结合一个额外的约束后，这会是安全的。 为了使follower的日志和leader自己的日志一致，leader必须找到二者的日志中最新的一致的日志条目，删除follower日志中该点后的所有条目，并将leader日志中该点后的所有条目发送给follower。所有这些操作都在响应AppendEntries RPC的一致性检验时发生。leader会为每个follower维护一个nextIndex，它是leader将发送给该follower的下一条日志的index。当leader首次掌权时，它会将所有的nextIndex值初始化为其日志的最后一个条目的下一个index（在图7中该值为11）。如果follower的日志与leader的不一致，下一次AppendEntries RPC中的一致性检验会失败。当RPC被拒绝后，leader会减小该nextIndex并重试AppendEntries RPC。最终，nextIndex会达到leader和follower的日志匹配的点。这时，AppendEntries会成功，它会删除follower日志中任何冲突的条目，并将日志条目从leader的日志中追加到follower的日志中（如果有的话）。一旦AppendEntries成功，follower的日志就与leader的一致，并在该term的余下的时间里保持这样。 如果需要的话，协议可被优化以减少拒绝AppendEntries RPC的次数。例如，当follower拒绝AppendEntries请求时，它可以在响应中带上冲突条目的term和它存储的该term下第一个条目的index。有了这一信息，leader可以减少相应的nextIndex来绕过该term中所有冲突的条目，这样每个term的日志条目仅需要一次AppendEntries RPC，而不是每个条目需要一次RPC。在实际中，我们对这一优化是否必要持怀疑态度，因为故障不会频繁发生，且不太可能出现许多不一致的条目。 通过这种机制，leader在掌权时不需要采取特殊的行为来恢复日志一致性。它只需要开始正常的操作，然后日志会在响应AppendEntries一致性检验故障时自动恢复。leader永远都不会覆写或删除它自己的日志条目（图3中的领导只增性质，Leader Append-Only Property）。 这种日志复制机制体现了第二章描述的理想的共识性质：只要大多数服务器在线，Raft就可以接受、复制、并应用新日志条目；在正常情况下新日志条目只需要一轮RPC就可以被复制到集群中大多数节点上；单个缓慢的follower不会影响性能。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:7:3","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"5.4 安全性 前面的章节描述了Raft如何选举领导和复制日志。然而，这些讨论过的机制目前还不能充分确保每个状态机会精确地按照相同的顺序执行相同的指令。例如，follower可能在leader提交一些日志条目时不可用，然后它可能被选举为leader并讲这些条目覆盖为新的条目；这样，不同的状态机可能执行不同的指令序列。 本章通过加入了一个对哪些服务器可以被选举为leader的约束完善了Raft算法。这一约束确保了在任意给定的term中，leader包含所有之前term中提交了的日志条目（图3中的领导完整性性质，Leader Completeness Property）。通过选举约束，我们可以让提交的规则更加精确。最后，我们给出了领导完整性性质的简要证明，并展示了它如何让多副本状态机的行为正确。 5.4.1 选举约束 在任何基于leader的共识算法中，leaer最终都必须保存所有已提交的日志条目。在一些共识算法中（像Viewstamped Replication[22]），即使一个服务器最初没有还包含所有已提交的日志条目，它也可以被选举为leader。这些算法有额外的机制来识别确实的日志条目，并在选举时或选举完成后的很短的时间里，将它们传输给新的leader。不幸的是，这需要引入相当多的额外的机制，大大增加了复杂性。Raft使用了一种更简单的方法，该方法会保证在过去的term中已被提交的条目在选举之初就在每个新的leader上，而不需要将这些条目再传输给leader。这意味着日志条目仅单向流动：从leader到follower，且leader永远不会覆写它的日志中已存在的条目。 Raft通过投票的过程组织没包含所有已提交日志条目的candidate赢得选举。由于candidate必须联系集群中大多数的才有可能被选举，这意味着每个被提交过的条目一定出现在这些服务器中的至少一个上。如果该candidate的日志至少与这大多数的服务器的日志一样“新（up-to-date）”（up-to-date在后文中有精确的定义），那么它就持有所有已提交的日志条目。RequestVote RPC实现了这一约束：RPC包括有关candidate的日志的信息，且如果投票者（voter）自己的日志比该candidate的日志更新，那么该投票者会拒绝给这个candidate投票。 Raft通过比较日志最后一个条目的index和term来确定哪个日志更新。如果日志最后个条目的term不同，那么有更新的term的日志更新。如果两个日志最后的term相同，那么更长的日志更新。 5.4.2 提交之前term的日志条目 正如章节5.3中描述的那样，一条当前term的日志，一旦它被存储到大多数的服务器上，那么leader会得知这个条目被提交了。如果leader在提交一个条目时崩溃，最后的leader会试图完成这个条目的复制。然而，当之前term的条目被存储到大多数服务器上时，leader无法立刻断定它被提交了。图8阐述了一种情况，在该情况下，就的日志条目被存储到了大多数服务器中，但仍会被之后的leader覆盖。 图8 展示为什么leader不能通过之前term的日志条目判断提交的时间序列。在（a）中，S1是leader，它部分复制了index为2的日志条目。在（b）中，S1崩溃，S5收到来自S3、S4和它自己的投票而被选举为term 3的leader，并接受了不同的日志条目作为index 2的条目。在（c）中，S5崩溃，S1重启且被选举为leader，并继续复制。此时，term 2的日志已经被复制到了大多数服务器上，但是没有被提交。如果此时S1像（d）中的情况一样发生崩溃，那么S5将会被选举为leader（S2、S3和S4会为其投票），且会用它自己的term 3的日志条目覆盖index 2中的条目。然而，如果S1在崩溃前复制了其当前term的条目到大多数服务器上，正如（e）中所示，那么这个条目会被提交（S5无法赢得选举）。此时，日志中所有之前的条目都同样被提交了。图8 展示为什么leader不能通过之前term的日志条目判断提交的时间序列。在（a）中，S1是leader，它部分复制了index为2的日志条目。在（b）中，S1崩溃，S5收到来自S3、S4和它自己的投票而被选举为term 3的leader，并接受了不同的日志条目作为index 2的条目。在（c）中，S5崩溃，S1重启且被选举为leader，并继续复制。此时，term 2的日志已经被复制到了大多数服务器上，但是没有被提交。如果此时S1像（d）中的情况一样发生崩溃，那么S5将会被选举为leader（S2、S3和S4会为其投票），且会用它自己的term 3的日志条目覆盖index 2中的条目。然而，如果S1在崩溃前复制了其当前term的条目到大多数服务器上，正如（e）中所示，那么这个条目会被提交（S5无法赢得选举）。此时，日志中所有之前的条目都同样被提交了。 \" 图8 展示为什么leader不能通过之前term的日志条目判断提交的时间序列。在（a）中，S1是leader，它部分复制了index为2的日志条目。在（b）中，S1崩溃，S5收到来自S3、S4和它自己的投票而被选举为term 3的leader，并接受了不同的日志条目作为index 2的条目。在（c）中，S5崩溃，S1重启且被选举为leader，并继续复制。此时，term 2的日志已经被复制到了大多数服务器上，但是没有被提交。如果此时S1像（d）中的情况一样发生崩溃，那么S5将会被选举为leader（S2、S3和S4会为其投票），且会用它自己的term 3的日志条目覆盖index 2中的条目。然而，如果S1在崩溃前复制了其当前term的条目到大多数服务器上，正如（e）中所示，那么这个条目会被提交（S5无法赢得选举）。此时，日志中所有之前的条目都同样被提交了。 为了消除图8中的这样的问题，Raft永远不会通过对副本数计数的方式提交之前term的条目。只有leader当前term的日志条目才能通过对副本数计数的方式被提交。而一旦当前term的日志被以这种方式提交后，所有在其之前的日志条目会因“日志匹配性质”被间接地提交。虽然在某些情况下，leader可以安全断定之前的日志条目被提交（例如，该条目被存储到了每台服务器上），但是Raft为了简单起见而采取了更保守的方法。 因为当leader复制来自之前term的日志条目时，日志条目会保持其原来的term号，所以Raft在提交规则中引入了额外的复杂性。在其它共识算法中，如果新的eader要重新复制来自更高“term”的条目，它必须用它的新“term号”来做。Raft的方法会使日志条目的推导更简单，因为无论何时它们都能在日志间维护相同的term号。另外，Raft中的新的leader发送的来自之前term的日志条目比其它算+法更少（其它算法必须发送冗余日志条目，对其重新编号，然后才能提交）。 5.4.3 安全性参数 有了完整的Raft算法，现在我们可以更精确地证明“领导完整性性质”成立（该证明基于安全性的证明，见章节9.2）。我们假设“领导完整性性质”不成立，那么我们会得出一个矛盾。假设term $T$的leader（$leader_T$）提交了一个该term的日志条目，但是该日志条目没被之后的某个term的leader保存。考虑该leader没有保存该条目的最小term $U$（$U\u003eT$）。 被提交的条目在$leader_U$被选举时必须不在其日志中（leader从未删除或覆写日志条目）。 $leader_T$将该条目复制到了集群中大多数服务器上，且$leader_U$收到了来自集群大多数的投票。因此，至少一个服务器（“投票者”）既从$leader_T$接受了该条目，又为$leader_U$投了票，如图9所示。该投票者是达成矛盾的关键。 投票者必须在为$leader_U$投票前接受来自$leader_T$的已提交的条目；否则，它会拒绝来自$leader_T$的AppendEntries请求（它当前的term将会比T高）。 投票者在为$leader_U$投票时仍保存着该条目，因为每个中间leader都包括该条目（基于假设），leader永远不会删除条目，且follower仅在条目与leader冲突时才会删除它们。 投票者将它的选票投给了$leader_U$，因此$leader_U$的日志必须至少于该投票者一样新。这导致两个矛盾之一。 首先，如果该投票者和$leader_U$的最后一条日志条目的term相同，那么$leader_U$的日志必须至少与投票者一样长，所以它的日志包括投票者日志中的每一个条目。这有一个矛盾，因为投票者包含了该已提交的条目而我们假设了$leader_U$没有包括该条目。 否则，$leader_U$的最后一条日志条目的term必须比投票者大。此外，该term还大于$T$，因为投票者的最后一条日志的term至少为$T$（它包括来自term $T$的被提交的日志条目）。创建了$leader_U$的最后一个日志条目的之前的leader的日志必须包含了该被提交的条目（基于假设）。那么，根据“日志匹配性质”，$leader_U$的日志必须同样要包含该被提交的日志，这是一个矛盾。 这样，就产生了完整的矛盾。因此，所有term大于$T$的leader必须包括所有在term $T$中提交的条目。 “日志匹配性质”简介地确保了之后的leader也包含了被提交的条目，如**图8（d）**中的index 2一样。 图9 如果S1（term ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:7:4","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"5.5 follower和candidate崩溃 目前我们都专注于leader故障的情况。处理follwer和candidate崩溃比处理leader崩溃简单得多，且这二者的故障可以用相同的方式处理。如果一个follower或candidate故障，那么之后发送到它的RequestVote和AppendEntries RPC会失败。Raft通过无限重试来处理这些故障；如果崩溃的服务器重启，那么RPC将会成功完成。如果服务器在完成RPC后但在响应前故障，那么在它重启后会收到相同的RPC。Raft的RPC是幂等的，所以这不会造成影响。例如，如果一个follower收到了一个包含了已经在它日志中的条目的AppendEntries，它会忽略新请求中的那些条目。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:7:5","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"5.6 定时和可用性 我们对Raft的要求之一是安全性决不能依赖定时：系统绝不能因某个时间发生的比预期的更快或更慢而产生错误的结果。然而，可用性（系统能够及时响应客户端的能力）必须不可避免地依赖定时。例如，如果崩溃服务器间的消息交换消耗了比通常更长的时间，candidate将不会保持足够的时间以赢得选举。没有稳定的leader，Raft不能进展（make progress，译注：本文中指可以继续提交新值）。 领导选举是Raft中定时最重要的一个方面。只要系统满足如下的定时要求（timing requirement），那么Raft就能够选举并维护一个稳定的leader： $$ broadcastTime \\ll electionTimeout \\ll MTBF $$ 在这个不等式中，$broadcatTime$是服务器将RPC并行地发送给集群中的每个服务器并收到它们的响应的平均时间，$electionTimeout$是章节5.2中描述的选举超时，$MTBF$是单个服务器发生两次故障间的平均时间。$broadcastTime$应该比$electionTimeout$小一个数量级，这样leader可以可靠的发送心跳消息以阻止选举发生；因为$exectionTimeout$采用了随机方法，这一不等性也让投票决裂不太可能发生。$electionTimeout$应该比$MTBF$小几个数量级，这样系统能够取得稳定的进展。当leader崩溃时，系统将会在大概$electionTime$的时间内不可用，我们想让这一时间仅占总时间的很小的比例。 $broadcastTime$和$MTBF$是下层系统的属性，而$electionTimeout$是必须由我们选取的。Raft的RPC通常需要收件人将信息持久化到稳定存储中，所以$broadcastTime$可能在0.5ms到20ms不等，这取决于存储技术。因此，$electionTimeout$可能在10ms到500ms之间。通常服务器的MTBF为几个月或更长时间，这可以轻松满足定时要求。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:7:6","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"6. 集群成员变更 到目前为止，我们都假设集群配置（configuration）（参与共识算法的服务器集合）是固定的。在实践中，偶尔变更配置是必要的，例如当服务器故障时替换它们，或修改副本数。尽管这可以通过该将整个集群下线、更新配置文件并重启集群来实现，但是这会让集群在变更期间不可用。另外，任何人工参与的步骤都有操作错误的风险。为了避免这些问题，我们决定将配置变更自动化，并将它整合到Raft共识算法中。 为了使配置变更机制是安全的，在切换期间决不能有同时出现两个被选举出的leader的情况。不幸的是，任何让服务器直接从就配置转换到新配置的方法都是不安全的。立刻对所有服务器进行切换是不可能的，因此集群在切换中可能被分为两个相互独立的“大多数”（见图10）。 图10 直接从一个配置切换到另一个配置是不安全的，因为不同的服务器会在不同时间切换。在本例中，集群从3个服务器增长到5个服务器。不幸的是，有一个时间点可能出现在同一个term中有两个被选举出来的leade的情况，其中一个leader有旧配置（$C_{old}$）的大多数，另一个有新配置（$C_{new}$）的大多数。图10 直接从一个配置切换到另一个配置是不安全的，因为不同的服务器会在不同时间切换。在本例中，集群从3个服务器增长到5个服务器。不幸的是，有一个时间点可能出现在同一个term中有两个被选举出来的leade的情况，其中一个leader有旧配置（$C_{old}$）的大多数，另一个有新配置（$C_{new}$）的大多数。 \" 图10 直接从一个配置切换到另一个配置是不安全的，因为不同的服务器会在不同时间切换。在本例中，集群从3个服务器增长到5个服务器。不幸的是，有一个时间点可能出现在同一个term中有两个被选举出来的leade的情况，其中一个leader有旧配置（$C_{old}$）的大多数，另一个有新配置（$C_{new}$）的大多数。 为了确保安全，配置变更必须使用两阶段的方法。实现这两个阶段有很多方式。例如，有些系统（例如[22]）在第一阶段禁用旧配置，这样它就不能处理客户端请求，然后在第二阶段应用新配置。在Raft中，集群首先会切换到过渡配置，我们称之为联合共识（joint consensus）；一旦联合共识被提交，接着系统会切换到新配置。联合共识结合了旧配置和新配置： 日志条目会被复制到两个配置的所有服务器上。 任一个配置的任何服务器都可能成为leader。 一致（用来选举和日志条目提交）需要在旧配置和新配置中的大多数分别达成。 联合共识让每个服务器能在不同时间切换配置而不需要做出安全性妥协。另外，联合共识让集群能够在配置变更时继续为客户端请求提供服务。 图11 配置变更的时间线。虚线表示已被创建但还没被提交的配置条目，实现表示最新的已被提交的配置条目。leader首先在它的日志中创建了配置条目$C_{old,new}$，并将其提交到了$C_{old,new}$中（$C_{old}$中的大多数和$C_{new}$中的大多数）。然后它创建$C_{new}$条目并将其提交到$C_{new}$的大多数。不存在$C_{old}$和$C_{new}$能独立作出决策的时间。图11 配置变更的时间线。虚线表示已被创建但还没被提交的配置条目，实现表示最新的已被提交的配置条目。leader首先在它的日志中创建了配置条目$C_{old,new}$，并将其提交到了$C_{old,new}$中（$C_{old}$中的大多数和$C_{new}$中的大多数）。然后它创建$C_{new}$条目并将其提交到$C_{new}$的大多数。不存在$C_{old}$和$C_{new}$能独立作出决策的时间。 \" 图11 配置变更的时间线。虚线表示已被创建但还没被提交的配置条目，实现表示最新的已被提交的配置条目。leader首先在它的日志中创建了配置条目$C_{old,new}$，并将其提交到了$C_{old,new}$中（$C_{old}$中的大多数和$C_{new}$中的大多数）。然后它创建$C_{new}$条目并将其提交到$C_{new}$的大多数。不存在$C_{old}$和$C_{new}$能独立作出决策的时间。 集群配置的存储和通信使用了多副本日志中的特殊条目。图11阐释了配置变更的过程。当leader收到将配置从$C_{old}$变更为$C_{new}$的请求时，它会将该配置的联合共识（图中的$C_{old,new}$）作为一个日志条目存储，并将该条目使用之前描述的机制复制。一旦某个服务器将新配置的条目加入到了它的日志中，它会在所有之后的决策中应用该配置（服务器总是使用它的日志中的最新的配置，无论该条目是否被提交了）。这意味着leader会使用$C_{old,new}$的规则来决定什么时候$C_{old,new}$的日志条目被提交。如果leader崩溃，新的leader可能在$C_{old}$或$C_{old,new}$下被选举出，这取决于赢得选举的candidate有没有收到$C_{old,new}$。这期间的任何情况下，$C_{new}$都不能单独做决策。 一旦$C_{old,new}$已经被提交，无论$C_{old}$还是$C_{new}$都不能在没有对方认同的情况下做决策，而“领导完整性特性”确保了只有$C_{old,new}$的日志条目的服务器才可能被选举为leader。现在leader可以安全地创建描述$C_{new}$的日志条目并将其复制到集群中。第二次也是一样，配置一被服务器看到就会生效。当新的配置已在$C_{new}$的规则下被提交时，旧的配置就无关紧要了，且不在新配置下的服务器可以被关机。如**图11**所示，在任何时间$C_{old}$和$C_{new}$都不能单独做决策，这确保了安全性。 重配置有另外三个问题待解决。第一个问题是，新的服务器最初可能没有存储任何日志条目。如果它们在这种状态下加入集群，它可能需要很长时间来追赶，这段时间内它可能不能提交新的日志条目。为了避免可用性有间隔，Raft在配置变更前引入了一个额外的阶段，在这一阶段中，新的服务器作为不投票的成员加入集群（leader会将日志条目复制给它们，但它们不在关于“大多数”的考虑范围内）。一旦新服务器追赶上了集群中其余的服务器，重配置可按之前描述的那样继续。 第二个问题是，集群的leader可能不是新的配置的一部分。在这种情况下，一旦leader提交了$C_{new}$的日志条目，它就会下台（返回为follower状态）。这意味着，在一段时间内（当leader提交$C_{new}$时），leader会管理不包括它自己在内的集群。它将日志复制到不包括自己在内的大多数中。leader在$C_{new}$被提交时发生切换，因为这是新配置可以单独操作的第一个时间点（它将总是可以从$C_{new}$中选择一个新leader）。在这个时间点之前，可能处于只有$C_{old}$中的服务器才能被选举为leader的情况下。 第三个问题是，被移除的服务器（那些不在$C_{new}$中的服务器）可能会扰乱集群。这些服务器将不会收到心跳，所以它们会超时并开始新的选举。接着，他们会使用新的term号发送RequestVote RPC，这会导致当前的leader转换成follower状态。最终，新的leader会被选举出来，但是被移除的服务器将再次超时，这一过程会重复，这会导致很弱的可用性。 为了防止这一问题，当服务器认为当前有leader存在时，它们会忽略RequestVote RPC。具体地说，如果一个服务器在当前leader的最小选举超时内收到了RequestVote RPC，它不会更新其term或投票。这不会影响正常的选举，在正常的选举中，每个服务器都在开始选举前等待了最小选举超时时间。这可以帮助免受被移除的服务器的打扰：如果leader能够给其集群发送心跳，那么它不会被更大的term号废除。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:8:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"7. 日志压缩 Raft的日志会随着正常的操作增长，以合并更多的客户端请求。但是在实用系统中，它不能不受限地增长。随着日志越来越长，它会占用越来越的的空间，并需要更长的时间来重放。最终，如果没有机制来丢弃日志中积累的过时的信息，这会导致可用性问题。 快照策略是最简单的压缩方式。在快照策略中，当前的整个系统状态会被写入到稳定存储上的一个快照中，然后直到这一点的所有日志条目会被丢弃。快照策略被使用在Chubby和Zookeeper中，本章其余部分会描述Raft中的快照策略。 用来压缩的增量方法（如日志清理（log cleaning）[36]和日志结构合并树[30， 5]）也是可行的。这些操作每次会处理一定比例的数据，所以它们能够更均匀地分摊压缩的负载。首先，它们会选取一个已经积累了许多被删除或被覆写的对象的数据区域；然后，它们将这个区域中存活的对象更紧凑地重新，并释放该区域。这与快照策略相比，需要很多额外机制与复杂性，快照策略通过总是对整个数据集操作的方式简化了这一问题。日志清理还需要对Raft进行修改，而状态机能使用与快照策略相同的接口实现LSM树。 图12 服务器使用新的快照取代它日志中已提交的条目（index 1~5），快照只保存了当前状态（在本例中为变量$x$和$y$）。该快照包括的最后的index和term用来安置紧随其后的日志条目6图12 服务器使用新的快照取代它日志中已提交的条目（index 1~5），快照只保存了当前状态（在本例中为变量$x$和$y$）。该快照包括的最后的index和term用来安置紧随其后的日志条目6。 \" 图12 服务器使用新的快照取代它日志中已提交的条目（index 1~5），快照只保存了当前状态（在本例中为变量$x$和$y$）。该快照包括的最后的index和term用来安置紧随其后的日志条目6 图12展示了Raft的快照的基本想法。每个服务器独立地创建快照，快照仅覆盖服务器日志中已提交的条目。大部分的工作由状态机将其当前状态写入到快照组成。Raft还在快照中包括了少量的元数据：last included index是快照替换的最后一个条目（状态机应用的最后一个条目）的index，last included term是这一条目的term。这些被保存的信息用来支持对紧随快照后面的第一个条目的AppendEntries的一致性检验，因为该条目需要前一条日志的index和term。为了启用集群成员变更（见第六章），快照还包括日志中直到last included index的最新的配置。一旦服务器完成了快照写入，它会删除直到last included index的所有日志条目和任何之前的快照。 尽管正常情况下服务器独立地创建快照，leader偶尔必须将快照发送给落后的follower。这会在leader已经丢弃了它需要发送给follower的下一条日志条目是发生。幸运的是，这种情况不太可能在正常操作中出现：在正常情况下，跟上了leader的follower中已经有了这一条目。然而，异常缓慢的follower或新加入集群中的服务器（见第六章）中没有这一条目。对leader来说，让这样的follower追上新状态的方法就是通过网络向其发送快照。 图13 InstallSnapshot RPC总结。快照被分割成块来传输，这可以通过给follower发送每个块时告知其存活，这样follower可以重设选举定时器。图13 InstallSnapshot RPC总结。快照被分割成块来传输，这可以通过给follower发送每个块时告知其存活，这样follower可以重设选举定时器。 \" 图13 InstallSnapshot RPC总结。快照被分割成块来传输，这可以通过给follower发送每个块时告知其存活，这样follower可以重设选举定时器。 leader使用一种被称为InstallSnapshot的新的RPC像落后太多的follower发送快照，如图13所示。当follower通过这个RPC收到一个快照时，它必须决定如何处理它已有的日志条目。通常，快照会包含在接收者的日志中还没有的新信息。在这种情况下，follower会丢弃它全部日志，因为这些日志都可以被快照取代，且日志中可能含有与快照冲突的未提交的日志条目。相反，如果follower收到的快照描述的是它日志的前面一部分（由于重传或错误），那么被快照覆盖的日志条目会被删除，但是快照之后的条目仍有效且必须被保留。 这种快照策略与Raft的强领导原则相驳，因为follower可以不通过leader的知识来创建快照。然而，我们认为这一偏差是合理的。尽管通过leader可以避免达成共识时的决策冲突，但是由于共识已经在快照中被达成过，所以不会有决策冲突。数据仍仅从leader流向follower，只是现在只有follower认识它们的数据。 我们考虑了另一种基于leader的方法，在这个方法中只哟iuileader会创建一个快照，然后它会将快照发给它的每个follower。然而，这有两个缺点。首先，向每个follower发送快照会浪费网络带宽，且会让快照创建的进程变慢。每个follower已经有了创建它自己的快照所需的信息，且通常服务器根据其本地状态创建快照比通过网络发送和接受快照开销低很多。第二，leader的实现会变得更复杂。例如，leader会需要并行地执行向follower发送快照的操作和给它们复制日志条目的操作，这样才能不阻塞新的客户端请求。 影响快照性能的问题还有另外两个。第一，服务器必须决定什么时候创建快照。如果服务器创建快照太过频繁，它会浪费磁盘带宽和能量；如果创建快照太少，它会面临好近其存储容量的风险，且会增加重启时重放日志的时间。一个简单的策略时当日志大小达到固定字节数时创建快照。如果设置的大小明显比期望的快照大小大很多，那么创建快照时的额外磁盘带宽开销会很小。 第二个性能问题是写入快照可能需要特别多的时间，且我们不想让这耽搁正常的操作。其解决方案是使用写入时复制（copu-on-write）技术，这样新的更新可以在不影响快照写入的情况下被接受。例如，由功能性数据结构构造的状态机本身就支持这一点。另外，操作系统的写入时复制支持（例如Linux的fork）可被用作对整个状态机创建内存式快照（我们的实现使用了这一方法）。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:9:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"8. 客户端交互 本章描述了客户端如何与Raft交互，包括客户端如何找到集群leader和Raft如何支持线性语义[10]。这一问题存在于所有的基于共识的系统，且Raft的解决方案与其它系统类似。 Raft的客户端会将所有的请求发送给leader。当客户端首次启动时，它会随机选择一个服务器连接。如果客户端首次选择的服务器不是leader，该服务器会拒绝该客户端的请求，并为其提供它所知道的最新的leader的相关信息（AppendEntries请求需要leader的网络地址）。如果leader崩溃，客户端的请求会超时，客户端会再次随机挑选一个服务器重试。 我们对Raft的目标是实现线性语义（每个操作看上去是在它的调用和响应期间的某一时刻被瞬时（instantaneously）、至少一次执行（exactly once）的）。然而，目前我们描述的Raft算法可能多次执行统一指令：例如，如果leader在提交日志条目但是没有响应客户端的时候崩溃，客户端会向新的leader重试该指令，这导致该指令会被二次执行。其解决方案是，让客户端为每个指令分配一个唯一的序列号。然后，状态机会记录每个客户端执行过的最新的序列号和相应的响应。如果它收到的请求的序列号已经被执行过，那么它不会执行请求，并立即发出响应。 处理只读操作不需要向日志写入任何东西。然而，如果不引入额外的方法，其会有返回陈旧数据的风险，因为响应该请求的leader可能已经被一个它未知的新leader取代。线性读取严禁返回陈旧的数据，Raft需要两个不使用日志的额外的预防措施来确保这一点。第一，leader必须有关于哪些条目被提交了的最新信息。“领导完整性性质”确保了leader有所有已提交的条目，但是在它的term开始时，它可能不知道哪些条目是被提交了的。为了得知哪些条目已被提交，它必须提交一个来自于它的term的条目。Raft通过让每个leader在它的term开始时向日志提交一个空的no-op条目的方式处理这一问题。第二，leader必须在处理只读请求前检查其是否被废除（如果有更新的leader被选举出来，之前的leader的信息可能是陈旧的）。Raft通过让leader在响应只读请求前与集群的大多数节点交换心跳消息来处理这一问题。另外，leader可以依赖心跳机制来提供租约（lease）[9]，但这需要依赖定时来保证安全性（其假时钟偏斜（clock skew）是有界的）。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:10:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"9. 实现和评估 我们已经实现了Raft算法，其作为RAMCloudRAMCloud中保存配置信息的多副本状态机的一部分，并为RAMCloud协调器的故障转移提供帮助。该Rafte实现包含了约2000行C++代码，但不包括测试、注释、或空白行。该源码可以随意访问[23]。此外，还有约25个Raft的第三方开源实现[34]被用在不同的开发领域，它们基于本论文的草稿。另外，很多公司都部署了基于Raft的系统[34]。 本章其余部分将从三个角度来评估Raft：可理解性、正确性、和性能。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:11:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"9.1 可理解性 为了衡量Raft的可理解性并与Paxos对比，我们在Stanford University的Advanced Operating System课程和U.C.Berkeley的Distributed Computing课程的高年级本科生和研究生中开展了一项实验。我们分别录制了有关Raft和Paxos的课程，并编写了相关的小测验。Raft的课程覆盖了本文除了日志复制的内容；Paxos覆盖了能创建等效的多副本状态机的内容，包括单决策Paxos、多决策Paxos、重配置、和实践中需要的较少的一些优化（例如领导选举）。小测验检测了对算法的基本理解，且要求学生推理极端情况。每个学生都先看一个视频，做相关的测验，然后看第二个视频，再做第二个测验。为了避免学习的先后顺序对表现和经验的影响，大概半数的参与者先做了Paxos的部分，另一半先做了Raft的部分。我们通过比较参与者在两个测验中的分数来判断参与者是否对Raft有更好的理解。 我们尽可能公平地在Paxos和Raft间挫比较。实验对Paxos有两方面偏向：43名参与中的15个称他们之前对Paxos有一定经验，且Paxos的视频比Raft的视频长了14%。如表1所示，我们采取了一些措施来减轻潜在的偏见。我们所有的材料都可供审查[28, 31]。 表1 学习中可能对Paxos存在的偏见、为每种情况采取的措施、可用的其他材料。表1 学习中可能对Paxos存在的偏见、为每种情况采取的措施、可用的其他材料。 \" 表1 学习中可能对Paxos存在的偏见、为每种情况采取的措施、可用的其他材料。 平均来看，参与者在Raft的测验中比Paxos的测验高了4.9分（总分为60分，Raft的平均分是25.7，Paxos的平均分是20.8）。图14展示了他们每个人的分数。$t$-检验表明，在95%的置信度下，Raft分数的真实分布比Paxos的至少高2.5分。 图14 比较43名参与者在Raft和Paxos的测验中的表现的散点图。斜线以上的的点（33个）表示Raft分更高的参与者。图14 比较43名参与者在Raft和Paxos的测验中的表现的散点图。斜线以上的的点（33个）表示Raft分更高的参与者。 \" 图14 比较43名参与者在Raft和Paxos的测验中的表现的散点图。斜线以上的的点（33个）表示Raft分更高的参与者。 我们还基于三个因素创建了一个用来预测新学生的测验分数的线性回归模型，这三个因素是：他们做了哪个测验、他们之前对Paxos的理解程度、和他们学习算法的顺序。根据该模型的预测，测验的选择偏向Raft 12.5分。这比实际看到的4.9分的差异高很多，因为许多学生实际上之前都有Paxos的经验，这对理解Paxos很有帮助，而对理解Raft的帮助相对较少。奇怪的是，该模型还预测先参加过Paxos测验的参与者比先参加Raft测验的参与者低6.3分，虽然我们不知道为什么，但这在统计上确实很重要。 我们在参与者做完测验后，我们还对他们进行的调研，以看他们觉得那种算法更容易实现或解释，这一结果如图15所示。超过大多数的参与者报告称Raft更容易实现与解释（对每个问题都有41人中的33个这样表示）。然而，这些自己报告的感觉可能没有参与者的测试分数那么可靠，且参与者可能因为我们对Raft的“更易理解”的假设产生偏见。 关于Raft在用户学习方面的讨论可见[31]。 图15 参与者通过5个程度衡量哪个算法更容易在有价值、正确、高效的系统中实现（左）和哪个对CS的研究生来说更容易解释（右）。图15 参与者通过5个程度衡量哪个算法更容易在有价值、正确、高效的系统中实现（左）和哪个对CS的研究生来说更容易解释（右）。 \" 图15 参与者通过5个程度衡量哪个算法更容易在有价值、正确、高效的系统中实现（左）和哪个对CS的研究生来说更容易解释（右）。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:11:1","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"9.2 正确性 我们已经为第五章描述的共识机制进行了形式化规范与证明。形式化的规范[31]使用了TLA+规范语言[17]能让图2中总结的信息完全准确。其有大概400行长，并可以作为证明。它对任何实现Raft的人来说也很有用。我们通过TLA证明系统[7]机械化地证明了“日志完整性性质”。然而，该证明依赖了还没被机械化检验的不变式（日历，我们还没有证明规范的类型安全性）。此外，我们为“状态机安全性性质”编写了完整（它仅依赖规范本身）且相对准确的非形式化的证明[31]（其大概有3500个词那么长）。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:11:2","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"9.3 性能 Raft的性能与其他共识算法（如Paxos）相似。对性能来说，最重要是一个已建立的leader复制新日志条目的时候。Raft使用了最少的消息数量来实现这一点（从leader到集群中的半数只需要一轮）。Raft的性能还可以进一步提升。例如，Raft可以轻松支持分批（batching）和流水线处理（pipelining）请求，以获得高吞吐量和低延迟。在其它算法的文献中，已经有各种优化方法被提出。这些优化中，许多都可以被用在Raft中，但我们将此留给了后续工作。 我们使用我们的Raft实现测量了Raft领导选举算法的性能，并回答了两个问题。第一，领导选举过程能快速收敛吗？第二，领导崩溃后可实现的最小停机时间是多少？ 图16 检测与替换崩溃的leader的时间。上图中设置了不同的electionTimeout随机范围，下图中设置了不同的最小electionTimeout。每条线代表每个不同的electionTime的范围的1000次考察（除了“150-150ms”，其只有100次）；例如“150-150ms”表示对electionTimeout在150ms到155ms间随机且均匀分布的实验的考察。这些测量值是在5个服务器组成的集群得出的，该集群广播时间大概在15ms左右。9个服务器组成的集群实验结果相似。图16 检测与替换崩溃的leader的时间。上图中设置了不同的electionTimeout随机范围，下图中设置了不同的最小electionTimeout。每条线代表每个不同的electionTime的范围的1000次考察（除了“150-150ms”，其只有100次）；例如“150-150ms”表示对electionTime在150ms到155ms间随机且均匀分布的实验的考察。这些测量值是在5个服务器组成的集群得出的，该集群广播时间大概在15ms左右。9个服务器组成的集群实验结果相似。 \" 图16 检测与替换崩溃的leader的时间。上图中设置了不同的electionTimeout随机范围，下图中设置了不同的最小electionTimeout。每条线代表每个不同的electionTime的范围的1000次考察（除了“150-150ms”，其只有100次）；例如“150-150ms”表示对electionTimeout在150ms到155ms间随机且均匀分布的实验的考察。这些测量值是在5个服务器组成的集群得出的，该集群广播时间大概在15ms左右。9个服务器组成的集群实验结果相似。 为了测量领导选举，我们反复地让5个服务器组成的leader崩溃，并测量集群多长时间能检测到崩溃并选举出新的leader（见图16）。为了保证最坏情况，每次考察中的日志长度都不同，所以一些candidate没有成为leader的资格。另外，为了提高投票决裂的产生，我们的测试脚本会在杀死leader的进程前触发一个同步的心跳RPC广播（这近似于leader在复制了新条目后崩溃的情况）。leader会在心跳时间内均匀且随机地崩溃，对于所有测试，心跳时间是最小electionTimeout的一半。因此，可能存在的最小停机事件大概是electionTimeout的一半。 图16的上图表明，electionTimeout只要小范围的随机化就足以在选举中避免投票决裂。在我们的测试中，如果没有随机化，领导选举会持续超过10秒，这是因为产生了许多的投票决裂。仅增加5ms的随机化也能显著改善这一问题，其平均停机时间为287ms。使用更大的随机范围能够改善最坏情况的表现：（在超过1000次实验中）50ms的随机范围的最坏完成时间为513ms。 图16的下图表明减小electionTimeout可以减少停机时间。当electionTimeout为12~24ms时，选举leader的平均时间仅为35ms（最长的一次为152ms）。然而，降低该超时时间可能会违背Raft的定时要求：leader在其它服务器开始新的选举前很难将心跳广播出去。这可能导致不必要的领导变更并降低整个系统的可用性。我们推荐使用保守的electionTimeout，如150~300ms；这样的超时不太可能造成不必要的领导变更，且仍能提供良好的可用性。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:11:3","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"10. 相关工作 共识算法相关的出版物有很多，这些出版物大多都属于如下类别之一： Lamport对Paxos的原始描述[15]，和将其解释地更清晰的尝试[16, 20, 21]。 Paxos的详述，其填补了算法的缺失，并对算法做出修改以为实现提供更好的基础[26, 39, 13]。 实现了共识算法的系统，如Chubby[2, 4]、ZooKeeper[11, 12]，和Spanner[6]。Chubby和Spanner的算法没有详细地公开发表，但二者都声称它们基于Paxos。Zookeeper的算法发表得更详细一些，但它与Paxos很不同。 能应用到Paxos的性能优化[18, 19, 3, 25, 1, 27]。 Oki和Liskov的Viewstamped Replication（VR），这是另一个实现共识的方法，它和Paxos大概在相同的时间被开发出。它的初始描述[29]非常复杂，其使用了一个分布式事务协议，但是核心共识协议在最近的更新[22]中被分离了出来。VR使用了基于leader的方法，它与Raft有很多相似之处。 Raft和Paxos最大的不同在于Raft的强领导权：Raft的领导选举是共识协议必要的部分，且Raft尽可能多地将功能集中到了leader中。这使算法更简单，且更容易理解。例如，在Paxos中，领导选举与基本的共识协议是独立的：它仅作为性能优化，而实现共识并不需要它。然而，这导致需要额外的机制：Paxos基本共识包括两段协议和用来领导选举的独立的机制。相反，Raft直接讲领导选举合并到了算法中，并用它作为两段共识的第一段。这使Raft的机制比Paxos更少。 像Raft一样，VR和Zookeeper也基于leader，因此它们和Paxos相比，有很多与Raft相同的优势。然而，Raft比VR或Zookeeper的机制更少，因为它减少了非leader的功能。例如，Raft中的日志条目仅单向流动：通过AppendEntries RPC从leader外流。在VR中，日志条目有两个流动方向（leader可以在选举过程中接收日志条目），这需要额外的机制和复杂性。Zookeeper发表的描述中，日志条目同样能传输给leader也能从leader传输，但是它的实现似乎更像Raft[35]。 Raft为达成共识所需的消息类型比其他我们知道的基于日志复制的算法更少。例如，我们数了VR和Zookeeper用作基本共识和成员变更的消息类型数（不包括日志复制和成员交互，因为它们几乎与算法相互独立）。VR和Zookeeper都定义了10中不同的消息类型，而Raft仅有4中消息类型（两种RPC请求和它们的响应）。Raft的消息比其它算法的更浓缩一点，但是总体上更简单。另外，在VR和Zookeeper的描述中，它们在leader变更时会传输整个日志，在实际情况下，将需要额外的消息类型来优化这些机制。 Raft的强领导权方法简化了算法，但它也妨碍了一些性能优化。例如，Egalitarian Paxos（ePaxos）可以在一些条件下通过无leader的方法实现更好的性能[27]。EPaxos利用了状态机指令的可交换性。任何服务器，在有其它指令被提议并与它发生指令交换时，可以仅用一轮通信就能提交一个指令。然而，如果被并发提议的指令没有互相发生交换，EPaxos就需要额外一轮通信。因为任何服务器都可能提交指令，EPaxos在服务器间的负载均衡更好，且在广域网配置下能实现比Raft更低的延迟。然而，它比Paxos又增添了相当的复杂性。 在其它工作中，有很多不同的用作集群成员变更的方法被提出或实现，包括Lamport最初的提议[15]、VR[22]、和SMART[24]。我们为Raft选择了联合共识的方法，因为它利用了其余的共识协议，因此只需要很少的额外机制来实现成员变更。Raft没有选择Lamport的$\\alpha$方法，因为它假设共识的达成不需要leader。相比VR和SMART，Raft的重配置算法的优势是，成员变更时不需要限制对正常请求的处理；相反，VR在配置变更期间会停止所有正常的处理，SMART对未完成的请求数量有类$\\alpha$的限制。Raft的方法还比VR或SMART加入了更少的额外机制。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:12:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"11. 结论 算法通常是以正确、效率、和/或简洁为主要目标设计的。尽管这些都是很有价值的目标，但我们认为可理解性和这些目标一样重要。在开发者将算法转化为实用的实现之前，其它目标都无法实现，且实用的实现不可避免地会脱离或扩展算法发表的形式。除非开发者对算法有很深的理解并能建立有关该算法的直觉，否则他们很难在实现过程中保留算法期望的性质。 本文中，我们解决了分布式共识的问题。该领域有一个被广泛接受但是难以理解的算法Paxos，它已经挑战了学生和开发者很多年。我们开发了一个新算法Raft，我们已经表明它比Paxos更好理解。我们还认为Raft为系统构建提供了更好的基础。把可理解性作为设计的主要目标改变我我们设计Raft的方法。在设计的过程中，我们发现我们反复地使用了一些技术，如分解问题和简化状态空间。这些技术不但提高了Raft的可理解性，还让我们能更好地理解它的正确性。 ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:13:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"12. 致谢 The user study would not have been possible without the support of Ali Ghodsi, David Mazières, and the students of CS 294-91 at Berkeley and CS 240 at Stanford. Scott Klemmer helped us design the user study, and Nelson Ray advised us on statistical analysis. The Paxos slides for the user study borrowed heavily from a slide deck originally created by Lorenzo Alvisi. Special thanks go to David Mazières and Ezra Hoch for finding subtle bugs in Raft. Many people provided helpful feedback on the paper and user study materials, including Ed Bugnion, Michael Chan, Hugues Evrard, Daniel Giffin, Arjun Gopalan, Jon Howell, Vimalkumar Jeyakumar, Ankita Kejriwal, Aleksandar Kracun, Amit Levy, Joel Martin, Satoshi Matsushita, Oleg Pesok, David Ramos, Robbert van Renesse, Mendel Rosenblum, Nicolas Schiper, Deian Stefan, Andrew Stone, Ryan Stutsman, David Terei, Stephen Yang, Matei Zaharia, 24 anonymous conference reviewers (with duplicates), and especially our shepherd Eddie Kohler. Werner Vogels tweeted a link to an earlier draft, which gave Raft significant exposure. This work was supported by the Gigascale Systems Research Center and the Multiscale Systems Center, two of six research centers funded under the Focus Center Research Program, a Semiconductor Research Corporation program, by STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA, by the National Science Foundation under Grant No. 0963859, and by grants from Facebook, Google, Mellanox, NEC, NetApp, SAP, and Samsung. Diego Ongaro is supported by The Junglee Corporation Stanford Graduate Fellowship. ","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:14:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"参考文献 [1] BOLOSKY, W. J., BRADSHAW, D., HAAGENS, R. B., KUSTERS, N. P., AND LI, P. Paxos replicated state machines as the basis of a high-performance data store. In Proc. NSDI’11, USENIX Conference on Networked Systems Design and Implementation (2011), USENIX, pp. 141–154. [2] BURROWS, M. The Chubby lock service for looselycoupled distributed systems. In Proc. OSDI’06, Symposium on Operating Systems Design and Implementation (2006), USENIX, pp. 335–350. [3] CAMARGOS, L. J., SCHMIDT, R. M., AND PEDONE, F. Multicoordinated Paxos. In Proc. PODC’07, ACM Symposium on Principles of Distributed Computing (2007), ACM, pp. 316–317. [4] CHANDRA, T. D., GRIESEMER, R., AND REDSTONE, J. Paxos made live: an engineering perspective. In Proc. PODC’07, ACM Symposium on Principles of Distributed Computing (2007), ACM, pp. 398–407. [5] CHANG, F., DEAN, J., GHEMAWAT, S., HSIEH, W. C., WALLACH, D. A., BURROWS, M., CHANDRA, T., FIKES, A., AND GRUBER, R. E. Bigtable: a distributed storage system for structured data. In Proc. OSDI’06, USENIX Symposium on Operating Systems Design and Implementation (2006), USENIX, pp. 205–218. [6] CORBETT, J. C., DEAN, J., EPSTEIN, M., FIKES, A., FROST, C., FURMAN, J. J., GHEMAWAT, S., GUBAREV, A., HEISER, C., HOCHSCHILD, P., HSIEH, W., KANTHAK, S., KOGAN, E., LI, H., LLOYD, A., MELNIK, S., MWAURA, D., NAGLE, D., QUINLAN, S., RAO, R., ROLIG, L., SAITO, Y., SZYMANIAK, M., TAYLOR, C., WANG, R., AND WOODFORD, D. Spanner: Google’s globally-distributed database. In Proc. OSDI’12, USENIX Conference on Operating Systems Design and Implementation (2012), USENIX, pp. 251–264. [7] COUSINEAU, D., DOLIGEZ, D., LAMPORT, L., MERZ, S., RICKETTS, D., AND VANZETTO, H. TLA+ proofs. In Proc. FM’12, Symposium on Formal Methods (2012), D. Giannakopoulou and D. M´ery, Eds., vol. 7436 of Lecture Notes in Computer Science, Springer, pp. 147–154. [8] GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In Proc. SOSP’03, ACM Symposium on Operating Systems Principles (2003), ACM, pp. 29–43. [9] GRAY, C., AND CHERITON, D. Leases: An efficient faulttolerant mechanism for distributed file cache consistency. In Proceedings of the 12th ACM Ssymposium on Operating Systems Principles (1989), pp. 202–210. [10] HERLIHY, M. P., AND WING, J. M. Linearizability: a correctness condition for concurrent objects. ACM Transactions on Programming Languages and Systems 12 (July 1990), 463–492. [11] HUNT, P., KONAR, M., JUNQUEIRA, F. P., AND REED, B. ZooKeeper: wait-free coordination for internet-scale systems. In Proc ATC’10, USENIX Annual Technical Conference (2010), USENIX, pp. 145–158. [12] JUNQUEIRA, F. P., REED, B. C., AND SERAFINI, M. Zab: High-performance broadcast for primary-backup systems. In Proc. DSN’11, IEEE/IFIP Int’l Conf. on Dependable Systems \u0026 Networks (2011), IEEE Computer Society, pp. 245–256. [13] KIRSCH, J., AND AMIR, Y. Paxos for system builders. Tech. Rep. CNDS-2008-2, Johns Hopkins University, 2008. [14] LAMPORT, L. Time, clocks, and the ordering of events in a distributed system. Commununications of the ACM 21, 7 (July 1978), 558–565. [15] LAMPORT, L. The part-time parliament. ACM Transactions on Computer Systems 16, 2 (May 1998), 133–169. [16] LAMPORT, L. Paxos made simple. ACM SIGACT News 32, 4 (Dec. 2001), 18–25. [17] LAMPORT, L. Specifying Systems, The TLA+ Language and Tools for Hardware and Software Engineers. AddisonWesley, 2002. [18] LAMPORT, L. Generalized consensus and Paxos. Tech. Rep. MSR-TR-2005-33, Microsoft Research, 2005. [19] LAMPORT, L. Fast paxos. Distributed Computing 19, 2(2006), 79–103. [20] LAMPSON, B. W. How to build a highly available system using consensus. In Distributed Algorithms, O. Baboaglu and K. Marzullo, Eds. Springer-Verlag, 1996, pp. 1–17. [21] LAMPSON, B. W. The ABCD’s of Paxos. In Proc. PODC’01, ACM Symposium on Principles of Distributed Computing (2001), ACM, pp. 13–13. [22] LISKOV, B., AND COWLING, J. Viewstamped replication revisited. Tech. Rep. MIT-CSAIL-TR-2012-021, MIT, July 2012.","date":"2020-09-27","objectID":"/posts/paper-reading/raft-extended/:15:0","tags":["Raft","Translation"],"title":"《In Search of an Understandable Consensus Algorithm (Extended Version)》论文翻译","uri":"/posts/paper-reading/raft-extended/"},{"categories":["Paper Reading"],"content":"本篇文章是对论文Paxos Made Live - An Engineering Perspective的原创翻译，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:0:0","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"作者 Tushar Chandra, Robert Griesemer, and Joshua Redstone Google Inc. ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:1:0","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"摘要 我们描述了我们在使用Paxos共识算法构建一个容错的数据库的经历。尽管在该领域已经有文献，但是事实上构建这样一个数据库并非易事。我们描述了选取算法和遇到的工程问题，及我们对这些问题的解决方案。我们的度量表明我们构建了一个很有竞争力的系统。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:2:0","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"1. 引言 众所周知，在商用硬件上的容错可通过副本实现[17, 18]。一个更通用的方法是使用共识算法[7]来确保所有副本相互一致[8, 14, 17]。通过对输入值的序列反复应用这样的算法，使为每个副本构建相同的值的日志成为可能。如果值是对某个数据结构的操作，那么建立在所有副本的相同日志上的应用程序可被用作实现所有副本中相互一致的数据结构。例如，如果日志由数据库操作序列组成，且对每个副本上的（本地）数据库应用了相同的操作序列，那么最终所有副本会有相同的数据库内容（前提是它们都从相同的初始数据库状态开始）。 这种通用的方法可被用作实现很多种容错基本组件，可容错数据库仅是一个例子。因此，在过去20年中，共识问题被大量研究。其中有几个众所周知的共识算法，它们可以在多种设置下执行，并能够容忍各种故障。Paxos算法[8]已经被从理论[16]和应用[10, 11, 12]的社区中被讨论了超过10年。 我们使用Paxos算法（Paxos）作为实现了容错日志的框架的基础。接着，我们依赖这个框架构建了一个容错数据库。尽管在这个方向已经有文献，但是构建一个生产级的系统并非易事，其原因有如下几点： 尽管Paxos能被一页伪代码描述出来，但是我们的完整实现却有几千行C++代码。代码量的爆炸并非由于我们使用了C++而不是伪代码，也不是由于我们的代码风格很啰嗦。将算法转化为实用的、可用于生产的系统，需要实现很多特性和优化——其中一些在文献中发表过，而一些却没有。 容错算法社区习惯于证明短算法的正确性（一页伪代码）。但是这种方法不适用于证明由几千行代码组成的系统的正确性。为了在真实系统中获取对“正确性”的自信，我们使用了很多不同的方法。 容错算法能够容忍一系列被小心选取的故障。然而，真实世界中的软件面对着各种各样的故障模式，包括算法中的错误、实现中的bug、和操作错误。我们必须设计软件并设计操作程序，以更健壮地处理更大范围的故障模式。 一个真是的系统很少能够精确地定义。甚至更糟的是，定义可能在实现阶段变化。因此，系统的实现应该是易修改的。最后，系统构建可能因为在其定义阶段的误解而失败。 本文挑选了我们在将Paxos从理论搬到实践过程中遇到的一些算法和工程挑战。这一工作比直接将伪代码翻译为C++多出很多研究与开发上的努力。 本文剩下的部分按照如下方式组织。接下来两章将展开讲解本项目的动机并描述我们构建的系统所处的一般环境。接着，我们重新回顾一下Paxos。我们将我们的经验分为三类并一次讨论：文献中算法的漏洞、软件工程上的挑战、和没有预期到的故障。最后我们通过度量我们的系统来进行总结，并对我们的领域的技术状况进行了更广泛的观察。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:3:0","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"2. 背景 Chubby[1]是Google的容错系统，其提供了分布式锁机制并存储小文件。通常，每个数据中心有一个Chubby实例，或成为“cell”。一些Google的系统，如Google File System（GFS）[4]和Bigtable[2]，使用Chubby进行分布式协作并存储少量元数据。 Chubby通过副本的方式来实现容错。通常，一个Chubby cell有5个运行相同代码的副本组成，每个副本都运行在一个专用的机器上。每个Chubby对象（例如Chubby锁、或Chubby文件）被作为数据库的一个条目存储。因此其实这些就是数据库的副本。在任意时间，这些副本之一会作为“master”。 Chubby的客户端（例如GFS和Bigtable）通过与Chubby cell通信来获取服务。master副本为所有Chubby请求提供服务。如果Chubby通信的副本不是master，那么该副本会回复master副本的网络地址。随后Chubby可以联系master副本。如果master宕机，那么新的master会被自动选举，随后新的master副本会基于其本地的数据库副本继续提供服务。因此，数据库的副本保证了在master故障转移过程中Chubby状态的连续性。 Chubby的第一个版本基于第三方商业容错数据库，我们在本文的余下部分称这个数据库为“3DB”。这个数据库有与副本相关的bug历史。事实上，据我们所知，其副本机制没有基于被证明过的算法，且我们不知道其是否正确。考虑到这个产品的历史原因和Chubby的重要性，我们最终决定使用我们自己的基于Paxos算法的方案替换3DB。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:4:0","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"3. 架构概要 图1阐述了单Chubby副本的架构。基于Paxos算法的容错副本日志坐落在协议栈的底层。每个副本维护一个本地的日志拷贝。Paxos算法按照需求反复运行以确保所有副本在它们的本地日志中要相同的条目序列。副本间互相通过Paxos专用的协议通信。 图1 单Chubby副本图1 单Chubby副本 \" 图1 单Chubby副本 下一层是一个多副本容错数据库，其每个副本都有一个数据库的本地拷贝。数据库由一个本地的snapshot（快照）和replay（重放日志，数据库操作的日志）组成。新的数据库操作会被提交到多副本的日志中。当数据库操作出现在副本中时，其会被应用到副本的本地数据库拷贝中。 最后，Chubby使用了容错数据库来存储其状态。Chubby的客户端通过Chubby专用协议与单个Chubby副本通信。 我们致力于设计将Paxos框架、数据库、和Chubby分离的清晰的接口。这部分是为了系统开发的清晰性，也同样为了能在其它应用程序中复用多副本日志层。我们预计Google之后的系统会通过副本的方式实现容错。我们相信容错日志对于构建这样的系统来说是一个强大的组件。 图2 容错日志的API图2 容错日志的API \" 图2 容错日志的API 我们的容错日志的API如图2所示。其包括一个用来将新的值提交（submit）到日志中的调用。一旦被提交的值进入容错日志，我们的系统会调用每个客户端应用程序中的回调，并传递被提交的值。 我们的系统是多线程的，多个值可在不同的线程中被并发提交。多副本的日志不会创建其自己的线程，但是可在任意数量的线程中被并发调用。这种线程化的系统有助于我们测试系统，我们将在后文中详细介绍。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:5:0","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"4. 回顾Paxos 本章中我们将介绍基本Paxos算法的概要，并概括地介绍我们如何将运行的多个Paxos联系到一起（Multi-Paxos）。想要了解更多形式化的描述和正确性证明的读者可以参考文献[8, 9, 16]。熟悉Paxos的读者可以跳过这一章。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:6:0","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"4.1 Paxos基础 Paxos是一种共识算法，由一个进程（被称为副本，replica）集合执行，用于在有故障的情况下就单个值达成一致。副本可能崩溃也可能随后恢复。网络可能丢失信息也可能导致信息重复。副本可以访问持久化存储，其可在崩溃时幸存。一些副本可以提交（submit）值以达成共识。如果最终大部分副本运行了足够长的时间而没有崩溃且没有故障，那么所有运行中的副本会对被提交的值之一达成一致（agree）。在我们的系统中，要达成一致的值是（多副本）日志的下一个条目，正如引言中描述的那样。 该算法由3个阶段组成，每个阶段都有可能重复（因为失败）： 选举一个副本作为coordinator（协调者）。 coordinator选取一个值，并通过被称为“accept消息”的消息将其广播给所有的副本。其他副本或者“acknowledge（确认）”该消息，或者“reject（拒绝）”该消息。 当大部分副本确认了该coordinator后，共识就会被达成，coordinator会广播一条“commit消息”来通知所有副本。 为了直观地了解该算法是如何工作的，首先考虑仅有一个coordinator且没有故障的情况。一旦大部分的副本收到了来自coordinator的accept消息并确认它后，就会达成共识。接下来，如果任意的少部分副本故障，我们仍能够保证至少一个收到了共识值的副本存活。 在现实中，coordinator可能故障。Paxos不需要在同一时间只有一个副本作为coordinator。在任何时间，都可以有多个副本可能决定变为coordinator并执行算法。通常，系统会被设计为限制coordinator的更替，因为其会推迟共识的达成。 这种宽松的选取策略意味着同时可能有多个认为自己是coordinator的副本。而且，这些coordinator可能选取了不同的值。Paxos通过两种额外的机制确保仅有一个值会达成共识（该值可能来自任一coordinator），这两个机制为：（1）对连续的coordinator排序（2）限制每个coordinator的选择中只能选取一个值。 对coordinator排序让每个副本都能区分当前的coordinator和过去的coordinator。通过这种方式，副本可以拒绝来自旧coordinator的消息，并防止这些消息破坏已达成的共识。Paxos通过给coordinator分配递增的序列号的方式对它们进行排序。每个副本追踪其上一次见到的序列号。当副本想要变为coordinator时，它会生成一个唯一的注1比其之前见过的更高的序列号，并将其在proposer消息中广播给所有副本。如果大部分副本作出回复并标明它们没有见过更大的序列号，那么该副本就会作为coordinator。这些回复被称为promise消息，因为副本承诺从此以后拒绝来自旧的coordinator的消息。proposer/promise消息交换构成了上面列出的步骤1。 注1：例如，在有$n$个副本的系统中，为每个副本$r$分配一个$0$到$n-1$的唯一的id $i_r$。副本$r$选取比起见过的序列号$s$要大的最小序列号，且$s \\mod n = i_r$。 一旦对一个值的共识达成，Paxos必须强制后面的coordinator选取与其相同的值，以确保持续的一致。为了确保这一点，来自副本的promise消息包含它们上一次听说的值（如果存在）和它们听说的值来自的coordinator的序列号。新的coordinator选取最近的coordinator的值。如果promise消息都没包含值，那么coordinator可以自由地选取提交的值。 算法能工作的原理有些微妙，但大致如下。新的coordinator需要来自大多数副本的对proposer消息的响应。因此，如果之前的coordinator达成了一个共识，那么可以保证新的coordinator能够至少从一个副本听到决定的值。通过归纳，该值将会有所有收到的响应中最大的序列号，所以其将会被选取为新的coordinator。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:6:1","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"4.2 Multi-Paxos 使用的系统使用Paxos作为构建单元来实现值序列的共识，如多副本日志。实现它的简单方式是反复执行Paxos算法。我们把每次执行称为Paxos算法的一个实例（instance）。“像Paxos提交（submit）一个值”表示“执行一个Paxos的实例同时提交该值”。 在Multi-Paxos中，一些缓慢（slow，lagging）的副本可能不会参与最近的Paxos实例。我们使用了*追赶（catch-up）*机制来使缓慢的副本能够追赶上领先的副本。 每个副本维护了一个本地的持久化日志来记录Paxos的行为。当副本崩溃并随后恢复后，它会重放持久化日志来重构其崩溃前的状态。副本还会在帮助落后的副本追赶时使用这个日志。目前为止，我们描述的Paxos算法要求所有消息的发送者在发送消息前记录它们的状态——因此，该算法的关键路径上会对磁盘进行5次写入（每次proposer、promise、accept、acknowledgement、commit消息会写入一次）。需要注意的是，所有的写入在系统可以继续进行任何操作之前必须立即刷盘。在副本在网络中邻近的系统中，刷盘时间可能会主导该实现的整体延迟。 一个用来减少消息数的常用优化是将多个Paxos实例连接到一起[9]。如果coordinator身份不会在实例间发生变化，那么propsoer消息可以被省略。因为任何副本在任何时间仍然可以通过广播有更高序列号的proposer消息来试图变为coordinator，所以Paxos的性质不会受影响。为了利用这一优化，Multi-Paxos算法会被设计为选取一个coordinator并长时间保持，尽量不要使coordinator改变。我们称这样的coordinator为master。通过这种优化，Paxos算法的每个副本的每个实例仅需要单次磁盘写入，且可以与其他实例并行执行。master在发送accept消息后立即落盘，其他副本在它们发送acknowledge消息前落盘。 为了在并发系统中获得额外的吞吐量，可以将不同应用程序线程中提交的值分批到单个Paxos实例中。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:6:2","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"5. 算法上的挑战 虽然Paxos核心算法被描述得很好，但是实现一个基于Paxos算法的容错日志并非易事。现实中的不完美之处增加了一些复杂性（例如硬盘故障或资源有限），额外的需求又带来了一些复杂性（如“master租约”）。许多这些挑战的算法上的挑战都与Paxos核心算法密切相关。接下来，我们我们将描述一些我们引入的机制。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:7:0","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"5.1 硬盘损坏 副本有时可能会遇到磁盘损坏的情况。磁盘损坏可能由于媒介故障或操作错误（操作员可能意外地删除了关键数据）造成。当副本的磁盘损坏且丢失了持久化状态时，其可能违背它之前对其它副本做出的承诺。这会违背Paxos算法的一个关键的假设。我们通过如下机制来解决这一问题[14]。 磁盘损坏有两种表现形式。或者文件内容可能改变，或者文件可能变得无法访问。为了检测前者，我们在一个文件中存储了每个文件的内容的校验和注2。后者可能与有空白磁盘的新副本的情况无法区分——我们通过让新副本在启动后在GFS中留下一个标记的方式检测这种情况。如果副本重启且棋盘为空，它将发现GFS中的标记，并意识到其磁盘发生了损坏。 注2：该机制不会检测被回滚到旧状态的文件。我们认为这种情况发生的可能性很小，所以我们选择不去显式地处理它。稍后，我们将描述可以检测这类问题的校验和机制。 磁盘损坏的副本会按照如下方式重建其状态。其作为一个不投票的成员参与到Paxos中，这意味着它使用追赶机制来追赶日志，但是不会响应promise或acknowledge消息。该副本会一直保持这一状态，直到它观察到了在该副本开始重构它的状态后，有一个完整的Paxos实例启动了。通过等待额外的Paxos实例，我们可以确保该副本不会违背之前的承诺。 这种机制让如下改进系统延迟的机制成为可能。因为系统现在可以处理偶然的磁盘损坏，在一些情况下，可以接受不将写入的内容立即落盘注3。虽然我们考虑了利用这一性质的策略，但是我们还没有实现它们。 注3：例如，如果每个副本的操作系统和底层硬件极少故障，且不同副本间的故障相互独立，那么可以修改我们的系统，使其不需要将写入落盘。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:7:1","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"5.2 master租约 当使用基本Paxos算法来实现多副本数据结构时，对数据结构的读取需要执行一个Paxos实例。这会串行化与更新相关的读取操作，并确保读取的内容是当前状态。另外，读操作不能由master的数据结构副本提供，因为有可能其它副本已经选举了另一个master，修改了数据结构，且没有通知旧的master。在这种情况下，由master提供的读操作有返回陈旧数据的风险。因为读操作通常在所有操作中占很大比例，所以通过Paxos的串行读取开销很高。 其解决方案是实现带有以下语义的*master 租约（master leases）*[5]：一旦master持有租约，它将确保其它副本不能成功地向Paxos提交值。因此，持有租约的master的本地数据结构有最新的信息，其可被用作直接通过本地提供读操作。通过让master在租约过期前试图刷新租约，我们可以确保master在大部分时间都持有租约。在我们的系统中，master可以一次性保持租约长达几天。 在我们的实现中，所有副本隐式地向之前的Paxos实例的master颁发租约，并在租约期间拒绝处理来自任何其他副本的Paxos消息。master会维护比副本更短的租约超时时间——这可以避免系统时钟漂移。master定期向Paxos提交一个虚拟的心跳值以刷新租约。 当存在间歇性网络中断时，Multi-Paxos优化有如下稳定性问题。当master的连接临时中断时，Paxos会选举一个新的master。新的master会维护一个跨Paxos实例的固定的序列号。同时，当连接中断的旧master试图运行Paxos算法时，如果它成功地连接到另一个副本时，它会增大它的序列号。当旧master重新连接时，它可能有比新的master更高的序列号，且有可能取到新的master。稍后它可能再次失去连接，重复这个循环。 这样的行为是不可取的，因为Chubby的master变化会对其部分用户有负面影响。另外，这种行为在连接情况较差的网络中可能会更糟，变为master反复快速变更。在我们的实现中，master会通过一轮Paxos算法（包括发送propsoer消息注4）周期性地增大自己的序列号。在大多数情况下，通过以正确的频率增大序列号可以避免这种master频繁变更的情况。 注4：在有负载的系统中，仅有低于1%的实例运行了一整轮Paxos算法。（译注：由于存在分批提交等优化。） 注意，足月的概念可以扩展到所有副本中。可将让任意持有曲乐的副本能够从它的本地数据结构中为读请求提供服务。当读取流量显著超过写入流量时，这种扩展的租约机制非常有用。我们已经研究了副本租约的算法，但是目前还没有实现它们。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:7:2","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"5.3 epoch号 （通过Chubby客户端）提交到Chubby cell的请求会被定向到Chubby当前的master副本中。从master副本收到请求到请求引起底层数据库更新的这段时间内，该副本可能会丢失其master状态，甚至可能在丢失master状态后又重新获得了master状态。如果在处理请求期间，master的所有权丢失和（或）重新获得了master所有权，Chubby需要中断将要到来请求。我们需要一种机制来可靠地检测master的转移并在必要时中断操作。 我们通过引入一种有如下语义的全局epoch号的方式解决了这一问题。若master副本收到了两个获取epoch号的请求，那么当且仅当该副本在这两个请求间一直是master时，这两个请求会收到相同的值。epoch号会被存储为数据库条目，且所有数据库操作都以epoch号的值为条件。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:7:3","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"5.4 组成员 实用的系统必须能够处理副本集合的变化。在文献[3]中，这被称为“组成员”问题。一些Paxos的论文指出，组成员可通过Paxos算法本身实现[3]。虽然实用Paxos核心算法实现组成员很简单，但当我们引入了Multi-Paxos、磁盘损坏等优化时，其具体细节就不再简单。不幸的是，文献中没有详细说明这一点，也没有对使用Paxos实现组成员变更的算法正确性进行证明。我们必须填补这些空白，以使组成员能够在我们的系统中工作。尽管其实现细节相对较小，但是仍然很微妙，这超出了本文讨论的范围。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:7:4","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"5.5 快照 如目前描述的那样，这种反复应用共识算法来创建多副本日志的方式会引起日志持续增长。这会有两个问题：这需要无限大的磁盘空间；更糟的是，因为恢复副本必须在其追赶上其他副本前重放一个可能非常长的日志，这会导致恢复时间无限长。日志通常是一个要被应用到某个数据结构的操作序列，因此，日志（通过重放）可以隐式表示数据结构的持久化的形式。问题在于寻找一种为其中的数据结构找到一种替代的持久化表示方法。一种显而易见的机制是直接持久化该数据结构或对其做快照（snapshot），这样就不再需要引导数据结构到当前状态的日志了。例如，如果数据结构在内存中，我们通过将其序列化到磁盘上的方式为它做快照。如果数据结构保存在磁盘上，快照可以是其在磁盘上的一个副本。 Paxos框架仅通过自身不能得知我们想要备份的数据结构，它仅关注多副本日志的一致性。使用Paxos框架的应用程序才有所有有关多副本数据结构的信息。因此，应用程序必须负责制作快照。我们的框架提供了一个让客户端应用程序（如我们的容错数据库）通知框架快照创建完成的机制。客户端应用程序可以在任意时间制作快照。当Paxos被通知有快照时，它会通过删除在快照前的日志的方式裁剪日志。如果副本故障，在随后的恢复过程中，它简单地安装最新的快照并重放裁剪后的日志来重构其状态即可。快照不会跨副本同步，每个副本单独决定其什么时候创建快照。 起初，这一机制看上去很简单，且文献[8]中有提到过它。然而，它为系统引入了相当多的复杂性：现在副本的持久化状态包括日志和快照，它们都必须保持一致。日志完全受框架的控制，而快照格式是由应用程序指定的。快照机制的某些方面很令人关注： 快照和日志需要相互一致。每个快照需要有与其内容对应的容错日志相关的信息。为此，我们的框架引入了*快照句柄（snapshot handle）*的概念。快照句柄包含与特定快照相关的所有Paxos指定的信息。当创建快照时（其受应用程序控制），相应的快照句柄（由框架提供）同样需要被应用程序存储。当恢复快照时，应用程序必须将快照句柄返回给框架，随后框架会使用句柄中的信息来协调快照与日志。 需要注意的是，句柄其实是对Paxos状态本身的快照。在我们的系统中，其包含与该（日志）快照相关的Paxos实例号和当前的组成员。 制作快照需要一定时间，且在一些情况下，我们无法承担制作快照时冻结（freeze）副本日志的代价。在我们的框架中，制作副本被分为三个阶段。首先，当客户端程序决定制作快照时，它会请求一个快照句柄。接着，该客户端程序会制作其快照。制作快照期间可能阻塞系统，更可能的情况是，创建一个在副本继续参与Paxos的同时制作快照的线程。快照必须对应于日志中客户端获取句柄时的状态。因此，如果副本在制作快照时继续参与Paxos，则必须采取特殊的预防措施，来在客户端的数据结构被更新时更新快照注5。最后，当快照被创建完成时，客户端程序告知框架该快照，并传递相应的快照句柄。随后框架会适当地裁剪日志。 注5：我们最初实现的容错数据库会在对（小型）数据库做内存拷贝时短暂地阻塞系统。随后它会将拷贝的数据通过另一个线程落盘。后来，我们实现了虚拟的无暂停（pause-less）快照。现在我们使用一个“影子（shadow）”数据结构来在下层数据库被序列化到磁盘时跟踪数据结构的更新。 创建快照可能失败。我们的框架仅在其被通知快照已经创建完且接收完相应的快照句柄时才会裁剪日志。因此，只要客户端程序没有通知框架，从框架的视角来看，就没有创建快照。这让客户端程序可以校验快照的完整性，并在必要时丢弃快照。如果快照存在问题，那么客户端不会让框架裁剪日志。通过这个机制，客户端程序甚至可以试图同时创建多个快照。 在“追赶”时，副本可能试图获取丢失的日志记录。如果副本不能获取它们（因为没有副本有足够老且可用的日志条目），该副本会被告知从另外一个副本获取快照。这个快照的句柄包含直到它捕获到的状态的Paxos实例的相关信息。在大多数情况下，一旦快照被接收并安装，落后的副本将会接近领先的副本。为了完全赶上，落后的副本会向领先的副本请求并接收剩余的日志记录。 注意，领先的副本可能在落后的副本正在安装较旧的快照时创建了一个新的快照——在容错系统中这是不可避免的。在这种情况下，落后的副本可能无法获得任何剩余日志记录，因为快照的提供者（和所有其它副本）可能已经将它们删除了。落后的副本将需要获取一个更近的快照。 此外，领先的副本可能在发送其快照后故障。追赶机制必须能够通过让落灰的副本联系另一个副本来从这样的问题中恢复。 我们需要一种定位最近快照的机制。一些应用程序可能选择直接在领先的副本和落后的副本间传输快照，而其它的应用程序可能让落后的副本在GFS上查找快照。我们实现了一种通用的机制，让应用程序在领先的副本和落后的副本间传递快照位置信息。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:7:5","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"5.6 数据库事务 Chubby对数据库的需求非常简单：数据库需要存储键值对（键和值可以使任意字符串），并支持常用的操作，如：insert、delete、lookup、原子性的compare and swap（cas）、和遍历所有条目。我们使用对整个数据库的快照实现了一个日志结构设计的数据库，每条数据库操作日志可被应用到快照上。操作日志是Paxos日志。该实现定期创建数据库状态的快照，并裁剪相应的日志。 相对于其它数据库操作，cas操作（可能由不同的副本提出）需要是原子性的。这可以通过将所有cas相关数据提交到Paxos的一个“值”中实现。我们意识到，我们可以扩展这种机制，在不需要实现真正的数据库事务的情况下提供像事务一样的支持。我们描述了我们的解决方案中的更多细节，因为我们认为这可能在其他场景中也很有用。 我们的实现围绕一个我们称为MultiOp的强大的原语。除了遍历的所有其他数据库操作都通过一次MultiOp调用实现。MultiOp会被原子性地应用，它由3个组件组成： 一组被称为guard的校验（test）。guard中的每个校验会检查数据库中的一个条目。它会检查值是否存在，或与给定值比较。guard中的两个不同的校验可能应用到相同或不同额数据库条目。guard中所有的校验都会被应用，且MultiOp会返回结果。如果所有的校验都是ture，MultiOp会执行t_op（见第2点），否则其会执行f_op（见第3点）。 一组被称为t_op的数据库操作。其中每个操作可能是一个insert、delete、或lookup操作，它们会应用到数据库的一个条目上。一组中两个不同的操作可能会被应用到数据库中相同或不同的条目上。这些操作会在guard的值为true时执行注6。 一组被称为f_op的数据库操作。类似t_op，但是在guard的值为false时执行。 注6：与其它操作不同，每个MultiOp操作会原子性地串行执行。一组中的各个操作会在数据库上按顺序执行。 在我们的后期开发中（在我们已经实现了数据库和MultiOp后），我们意识到我们还需要使用epoch号来实现Chubby的数据操作。因为这个额外需求，所有的Chubby操作都变得与epoch有关，且当Paxos的epoch变化时需要执行失败。MultiOp在适应这个新需求时被证实很有帮助。在我们把Paxos的epoch作为数据库的一个条目后，我们可以改变所有之前的对数据库的调用，来引入用来检查epoch号的额外的guard。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:7:6","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"6. 软件工程上的挑战 人们期望容错系统能够连续运行很长时间。用户对容错系统的bug容忍度比其他系统要低得多。例如，文档编辑器的布局bug可能会让用户很烦，但这个问题是可以“通融”的，尽管事实上这是软件核心功能的bug。而在容错系统中，相同分量的bug可能会使系统不可用。 我们采用了多种软件工程方法来容我们对我们的实现的健壮性有信心。本章中，我们描述了一些我们是用了的方法。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:8:0","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"6.1 算法的有效表达 众所周知，容错算法很难正确表达，即使是伪代码也是如此。当这些算法与其他一起构建完整的系统时，这一问题可能会变得更糟。当出现bug时，核心代码难以分辨、推导、或调试。这还会使需求变化时难以修改核心代码。 我们通过将核心算法编写成两个显式的状态机的方式解决这一问题。为此，我们设计了一个简答的状态机专用语言，并构建了一个将该语言翻译为C++的编译器。该语言被设计得非常简洁，因此整个算法可在一屏中显示。它的另一个好处是，状态机编译器还能自动生成代码来记录状态转移并测量代码覆盖率，以帮助调试和测试。 我们认为选择专用语言比混在系统其它部分中的显式代码实现更容易推断和修改我们的状态机。我们通过如下的经历阐述了这一点。在我们开发容错日志的最后阶段，我们不得不对我们的组成员算法做根本性的修改。在修改前，我们的系统大致会经历三个状态。最初系统等待加入组，然后系统加入到组中，最后系统离开组。一旦一个副本离开了组，它就不再被允许重新加入该组。因为间歇性故障的副本可能无法加入组且会使组长时间混乱，所以我们认为这种方法是最好的。然而，因为正常的副本有时也会间歇故障，间歇性故障比我们最初预期的更常见。因此，我们需要将算法改为有两个状态。即副本在组中或副本不在组中。在系统的生命周期中，副本可以在这两种状态间频繁切换。做出这些修改花了我们一个小时的时间，修改相关的测试花了我们三天时间。如果我们将状态机和系统其它部分混在一起编写，那么这些修改将会更难实现。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:8:1","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"6.2 运行时一致性检查 不一致发生的可能性会随着代码库大小、项目持续时间、和同时编写同一处代码的人的增加而增加。我们使用了各种主动自我检查机制，如使用了很多assert（断言）以及使用测试数据结构一致性的显式验证代码。 例如，我们使用了如下的数据库一致性检查。master定期向数据库日志提交checksum（校验和）请求。收到该请求后，每个副本会计算其本地数据库的校验和注7。因为Paxos日志对所有副本的所有操作进行了相同的串行化，我们期望所有副本会计算出相同的校验和。在master完成校验和计算后，它会把它的校验和发送给所有副本，副本会将master的校验和与它们计算出的校验和进行比较。 注7：我们使用了影子数据结构来在处理数据库操作的同时并发处理校验和操作。 到目前为止，我们发生了三种数据库不一致事故： 第一个事故原因是一名操作员错误。 我们没有发现第二个事故的原因。在重放故障副本的日志时，我们发现它与其它副本是一致的。因此这可能是由随即发生的硬件内存损坏造成的。 我们换衣第三次事故是由我们使用的代码库（其大小相当可观）的不正常的非法内存访问导致的。为了防止未来再次发生，我们维护了第二个校验和数据库，并通过校验和对每次数据库访问进行双重校验。 在这所有三种情况下，我们在Chubby发生问题前就通过人工解决了这些问题。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:8:2","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"6.3 测试 鉴于目前的技术水平，想要证明像我们的系统一样的真实的系统的正确性是不现实的。为了实现健壮性，除了精细的软件工程外，最佳的方式是对系统进行彻底的测试。我们的系统从一开始就被设计为可测试的，目前其包含一套范围很广的测试。本节中，我们将描述两种测试，它们会让系统经历较长的一系列随机故障，并验证其行为是否符合预期。两个测试都可在以下两种模式下运行： 安全模式： 在这个模式下，测试会验证系统是否一致的。系统不需要能够取得进展（译注：即Paxos算法不断执行）。例如，可以接受一个让系统故障、或执行完成、或报告系统不可用的操作。 存活模式： 在这个模式下，测试会验证系统是否一致，且系统是否取得了进展。所有操作都被期望会完成，且系统需要保持一致。 我们的测试从安全模式开始，并向系统中注入了随机的故障。在运行了预定的一段时间后，我们停止向系统注入故障，并给系统时间使其完全恢复。然后我们将测试转为存活模式。存活模式的目的是验证系统在一系列故障后没有发生死锁。 我们的测试之一会验证容错日志。该测试会模拟一个由随机数量的副本组成的分布式系统，并让容错日志经历随机的网络中断、消息延迟、超时、进程崩溃与恢复、文件损坏、交叉编排（schedule interleaving）等故障序列。我们希望这个测试可以重复进行，以助于调试。为此，我们通过随机数生成器来确定故障的编排。随机数生成器的种子在测试运行开始时给出。我们通过在单线程中运行测试，来消除我们不想在多线程中看到的比确定性，以确保两个有相同随机数种子的测试会按同样的方式运行。正是因为容错日志不会创建其自己的线程，且可在单线程的环境中运行（即使通常它会运行在多线程环境下），所以这样做可以的。 每个测试执行后会报告其成功或失败。如果测试失败，我们会返回该测试的随机数种子和调试器中的详细日志，以确定哪里出了问题。正是因为测试是可重复的，所以可以这样做。 实践证明，这个测试对发现各种微妙的协议错误有很大帮助，这些错误有我们的组成员实现错误和我们为了应对磁盘损坏而做出的修改错误。为了衡量该测试的强度，我们在系统中留下了一些审查代码和设计时发现的协议bug。在修复了一些bug后，该测试变得非常稳定。为了让它能发现更多bug，我们开始同时在数百台Google的机器上运行这一测试。这样，我们发现了其它bug，其中一些bug需要花数周的时间（并以极高的故障率）模拟执行才能发现。 另一侧测试会验证新的Chubby系统面对下层系统和硬件故障时的健壮性。我们在容错日志中实现了多个钩子（hook）以注入故障。该测试将随机地调用这写钩子并验证上层系统能否处理。这些钩子可以导致副本崩溃、在一定时间内中断副本与其他副本的连接、或强制副本假装其不再是master。该测试在头两周找到了Chubby中与master故障转移的5个微妙的bug。我们以同样的方式构建了一个带有钩子的文件系统，以通过编程对其注入故障，并用它来测试我们处理文件系统故障的能力。 最后，我们指出一个我们在测试系统时面对的挑战，对此我们没有系统的解决方案。容错系统本质上是要试图掩盖问题。因此，它们会在掩盖bug或配置问题时，会难以察觉地地降低了自身的容错能力。例如，我们观察到了如下的情况。有一次我们启动了一个有5个副本的系统，但是我们在初始化组的时候拼错了一个副本的名字。因为四个配置正确的副本能够继续执行，所以系统看上去似乎是正常运行的。另外，第五个副本一直在以追赶模式注8运行，因此其似乎也在正确运行。然而，在该配置下，系统只能容忍一个副本故障，而不是预期的两个副本故障。现在，我们有了检测这种特定类型故障的流程。我们无法得知是否存在其他的被容错掩盖了的bug或配置问题。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:8:3","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"6.4 并发 在开始项目的时候，我们考虑了测试并发容错代码的问题。特别是，我们希望我们的测试是可重复的。就像之前描述的一样，我们的容错日志没有任何自己的线程（尽管它可以在不同线程上处理并发的请求）。线程是在代码的边缘引入的——即我们接受来自网络层的调用。通过将我们的测试编写成可重复的，我们可以在测试期间找出许多模糊的协议错误。 随着项目进行，我们不得不使几个子系统变得比我们预期的更具有并发性，且不得不牺牲可重复性。Chubby的核心是多线程的，因此我们不能在整个系统运行可重复的测试。接着，我们不得不将我们的数据库多线程化，这样它可以创建快照、计算校验和、并在为数据库请求提供服务的同时处理遍历请求。最后，我们被迫将处理日志本地副本的代码也并行化（其原因超出了本文探讨的范围）。 总之，我们认为我们为了执行的可重复性而约束并发性是正确的的。但不幸的是，随着产品需求增长，我们无法坚持这些目标。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:8:4","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"7. 意外地故障 目前，我们的系统已经在生产环境中良好地记录了超过100“机器年”的日志。在这段时间中，我们目睹了如下的意外故障： 我们为第一个版本提供了原来的Chubby10倍的工作线程数量。我们希望通过这一改变可以让我们处理更多请求。不幸的是，在负载下，工作线程最终耗尽了其他关键线程，并导致了系统频繁超时。这引起了快速的master故障转移，随后大量客户端一起迁移到新的master，这又导致了新的master过载，随后又会发生master故障转移，如此往复。 当问题首次出现时，我们还不知道其确切的原因，我们必须保证不受系统中潜在的危险的bug的影响。我们觉得谨慎行事，将我们其中一个数据中心的Chubby会滚到旧版本（基于3DB的版本）。那时，回滚机制没有适当的文档（因为我们从没预期到会使用它），其使用很不直观，执行回滚的操作员对此没有经验，且当回滚执行时，没有开发团队的成员在场。这导致回滚中意外地使用了旧快照。当我们发现在这个错误的时候，已经丢失了15个小时的数据，一些关键数据集必须被重建。 当我们在几个月后再次试图升级Chubby cell的时候，因为我们忽略了删除上次升级生成的文件，我们的升级脚本发生了故障。最后在我们发现问题前，Chubby cell运行了几分钟的几个月前的快照。这导致我们丢失了30分钟的数据。幸运的是，Chubby的所有客户端都从这次故障中恢复了过来。 在我们首次发行的几个月后，我们意识到我们的数据库提供的语义与Chubby期望的不同。如果Chubby向数据库提交了一个操作，且数据库失去了master状态，Chubby期望该操作将失败。而在我们的系统中，在数据库操作的时间内，副本可能重新作为master，这样操作可能成功。这个修复需要对Chubby和我们的框架间的集成层进行大量重做（我们需要实现epoch号）。事实证明，MultiOp对解决该意外问题提供了很大帮助——这表明MultiOp是一个强大的原语。 正如之前提到的，我们三次发现Chubby中数据库的其中一个副本与其它的不同。因为我们的系统定期对所有副本计算校验和并对比它们，我们才找到了这个问题。 我们负责将cell从3DB的Chubby迁移到Paxos版本的升级脚本因各种问题发生了几次故障。例如，它曾因基本Google程序没被安装到我们的cell之一而发生过故障。 我们遇到过因底层操作系统导致的bug。例如，在Linux 2.4内核中，当我们试图将小文件落盘时，如果缓冲区中有很多其它文件的写入，该调用会被挂起很长时间。这会在我们将数据库快照写入到磁盘时立刻发生。 在这种情况下，我们观察到，内核需要花数秒的时间将不相关的小写入冲刷到Paxos日志中。我们的解决方案是对所有大文件的小块写入，在每个小块被写入后冲刷到磁盘。尽管这会稍微影响写入性能，但这可以避免更重要的日志写入不会有意外的延迟。 对于大多数生产系统来说，在100机器年中仅发生很少几次故障是非常好的。然而，我们认为目前你的故障率对于Chubby来说还是过高，我们决定我们需要进一步降低故障率。 其中有三次故障发生在升级时（或回滚时）。每次在升级过程中遇到问题时，我们会相应地更新升级脚本。一旦cell升级完成，这类故障会消失。 其中有两次故障来自我们已经修复的bug。为了减少发生其它bug的可能性，我们会继续改进并运行之前描述的Chubby验证测试。 我们的两个意外问题与新版本发布期间操作员的错误有关，其造成了数据丢失。在Google，系统的日常监控与管理由系统操作员完成。尽管他们做的很棒，但是由于他们通常不是构建系统的开发团队，因此不熟悉系统中复杂的细节。这可能会导致在不可预见的情况下偶尔发生的错误。现在，我们依赖小心地编写代码并使用良好测试过的脚本来自动化部署并减少操作员的参与。这样，我们最近能够在提供服务的同时无故障地在几百台机器上无故障地发行主要的Chubby版本。 其中的一次故障由于内存损坏。因为我们的系统是日志结构的，且维护了数日的日志和快照，因此可以重放数据库直到故障发生的具体位置。我们验证了我们的日志是正确的，并得出内存损坏是由于不正常的软件或硬件问题的结论。我们添加了额外的校验和数据来在以后检测这种问题，并在检测到这种问题时使副本崩溃。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:9:0","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"8. 评估 我们系统最初的目标是用我们自己的数据库替代3DB。因此，我们的系统必须有与3DB等同或更好的性能。我们测量了使用了我们的容错多副本数据库的完整的Chubby系统（客户端、服务器、及网络延迟）的性能。我们还将我们的系统与机遇3DB的系统进行了benchmark测试（见表1）。在我们的测试中，我们在相同的5个服务器（奔腾级的机器）上运行了两个Chubby的副本。Chubby的其中一份副本使用了我们的数据库，另一份副本使用了3DB。我们在工作站上运行Chubby客户端，以生成服务器的负载。在我们的测试中，我们测量了这个那个系统的吞吐量。每次调用包括Chubby客户端、网络、Chubby服务器、和我们的容错数据库。虽然这些测试会低估我们数据库的性能，但它提供了对基于Paxos系统的整个系统吞吐量的感知。 尽管在实际情况下，Chubby的读请求占大多数，我们仍将测试编写为写密集型。因为读请求完全由master处理，其通常持有租约，不会执行Paxos算法。 在我们的测试中，每个worker会在Chubby中反复地创建文件并等待Chubby返回。因此，每个操作都会对底层数据库进行一次写入调用。如果文件内容很小且只有一个worker，测试会测量系统延迟。日过文件内容很大，测试会测量以MB/s为单位测量系统吞吐量。通过使用多个并发的worker，我们还能测量系统在不同submissions/s下的吞吐量。 所有使用超过1个worker的测试展示了将提交值分批的影响。通过将一些数据库事务中的更新打包应该可以对3DB有一些提速。最后两个吞吐量测试展示了创建快照的影响。该系统被配置为只要副本日志超过了100MB就创建快照。在这两个测试中，提供大概每100秒创建一个快照。在创建快照时，系统会为数据库创建另一个副本并将其落盘。这样，其性能会暂时下降。 我们的系统没有性能上的优化，我们相信它有很大的提速空间。然而，既然其性能已经超过了3DB，进一步的优化目前不是优先考虑的。 表1 我们的系统与3DB的对比（数值越高越好）。表1 我们的系统与3DB的对比（数值越高越好）。 \" 表1 我们的系统与3DB的对比（数值越高越好）。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:10:0","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"9. 总结与未解决的问题 我们描述了我们的基于Paxos共识算法的容错数据库的实现。尽管该领域有大量论文、算法可以追溯到15年前、我们的团队有相关经验（我们团队中有一个人过去设计过类似的系统，其他人过去构建过其他类型的复杂系统），构建该系统仍比我们最初预期的要困难得多。我们将其归咎于一下几点： 现实系统的需求的与Paxos算法的描述之间有很大的隔阂。为了构建现实的系统，专家需要使用分散在各种文献中的许多思想，并作出一些较小的协议扩展。这些不断累积的扩展会非常多，最后系统会基于一个未被证明的协议。 容错计算社区还没有开发能使实现它们的算法变得简单的工具。 容错计算社区对测试没有给予足够的关注，这是构建容错系统中关键的部分。 因此，核心算法工作仍是相对理论性的，且在更大的计算社区中可能无法使用。我们认为，为了使其能有更大的影响，该领域的研究人员应该专注于解决这些缺陷。 相反，在编译器构造领域，尽管该领域的理论很复杂，但是它们能被广泛地接受。在解析的理论被充分理解后不久，就出现了像yacc[6]这样的工业级解析工具。且现在不光有像ANTLR[15]或CoCo/R[13]这样的前端工具，还有能够帮助优化、指令选择的树重写工具（tree-rewriting tool），和帮助生成二进制代码的汇编器，等等。因此，在软件工程的这个领域中，有一整套工具出现，这大大地简化了编译器的构造，或者至少减少了出错的可能性。编译器构造领域中，像解析这样的问题曾经处于研究的前言，现在已经被认为是“已经解决了”，并在许多学校的本科阶段中都有常规的教学。 容错分布式计算社区似乎没有像编译器社区那样，开发出能够弥补理论和实践之间差距的工具和技术。我们的经验表明，这些差距并不是微不足道的，值得研究团体的关注。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:11:0","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"10. 致谢 Google中的许多人为这个项目提供了帮助。实现了Chubby的Mike Burrows建议我们用基于Paxos的系统替换3DB。它和Sharon Perl审查了我们的设计并提供了非常棒的反馈。他们向我们介绍了处理磁盘损坏的机制并建议我们实现master租约。Michal Cierniak将最初的状态机编译器从Perl前移到了C++，并做了后续的修改（现在它也再Google的地方中被使用）。Vadim Furman帮助我们编写了Chubby验证测试。Salim Virji和他的团队负责将我们的系统在Google的数据中心上运行。 Mike Burrows、Bill Coughran、Gregory Eitzman、Peter Mckenzie、Sharon Perl、Rob Pike、David Presotto、Sean Quinlan、和Salim Virji审查了本文的早期版本并提供了有价值的反馈。 ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:12:0","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"11. 参考文献 [1] Burrows, M. The Chubby lock service for loosely-coupled distributed systems. In Proceedings of the 7th USENIX Symposium on Operating Systems Design and Implementation, pp. 335-350 [2] Chang, F., Dean, J., Ghemawat, S., Hsieh, W. C., Wallach, D. A., Burrows, M., Chandra, T., Fikes, A., and Gruber, R. E. Bigtable: A distributed storage system for structured data. In Proceedings of the 7th USENIX Symposium on Operating Systems Design and Implementation, pp. 205-218 [3] Cristian, F. Reaching agreement on processor-group membership in synchronous distributed systems. Distributed Computing 4, 4 (1991), 175–188. [4] Ghemawat, S., Gobioff, H., and Leung, S.-T. The Google file system. In Proceedings of the 19th ACM Symposium on Operating Systems Principles (Dec. 2003), pp. 29–43. [5] Gray, C., Cheriton, D. Leases: An efficient fault-tolerant mechanism for distributed file cache consistency. In Proceedings of the 12th ACM Symposium on Operating Systems Principles (1989), pp. 202–210. [6] Johnson, S. C. Yacc: Yet another compiler-compiler. [7] Lamport, Shostak, and Pease. The byzantine generals problem. In Advances in Ultra-Dependable Distributed Systems, N. Suri, C. J. Walter, and M. M. Hugue (Eds.), IEEE Computer Society Press. 1995. [8] Lamport, L. The part-time parliament. ACM Transactions on Computer Systems 16, 2 (1998), 133–169. [9] Lamport, L. Paxos made simple. ACM SIGACT News 32, 4 (Dec. 2001), 18–25. [10] Lampson, B. W. How to build a highly available system using consensus. In 10th International Workshop on Distributed Algorithms (WDAG 96) (1996), Babaoglu and Marzullo, Eds., vol. 1151, Springer-Verlag, Berlin Germany, pp. 1–17. [11] Lee, E. K., and Thekkath, C. A. Petal: Distributed virtual disks. In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems (Cambridge, MA, 1996), pp. 84–92. [12] MacCormick, J., Murphy, N., Najork, M., Thekkath, C. A., and Zhou, L. Boxwood: Abstractions as the foundation for storage infrastructure. In Proceedings of the 6th Symposium on Operating Systems Design and Implementation (2004), pp. 105–120. [13] Moessenboeck, H. A generator for production quality compilers. In Proceedings of the 3rd International Workshop on Compiler Compilers - Lecture Notes in Computer Science 477 (Berlin, Heidelberg, New York, Tokyo, 1990), Springer-Verlag, pp. 42–55. [14] Oki, Brian M., and Liskov, Barbara H. Viewstamped Replication: A New Primary Copy Method to Support Highly-Available Distributed Systems. In Proceedings of the 7th annual ACM Symposium on Principles of Distributed Computing (1988), pp. 8–17. [15] Parr, T. J., and QUONG, R. W. Antlr: A predicated-ll(k) parser generator. Software–Practice and Experience 25, 7 (JULY 1995), 789–810. [16] Prisco, R. D., Lampson, B. W., and Lynch, N. A. Revisiting the paxos algorithm. In 11th International Workshop on Distributed Algorithms (WDAG 96) (1997), pp. 111–125. [17] Schneider, F. B. Implementing fault-tolerant services using the state machine approach: A tutorial. ACM Computing Surveys 22, 4 (1990), 299–319. [18] von Neumann, J. Probabilistic logics and synthesis of reliable organisms from unreliable components. Automata Studies (1956), 43–98. ","date":"2020-09-24","objectID":"/posts/paper-reading/paxos-made-live/:13:0","tags":["Paxos","Translation"],"title":"《Paxos Made Live - An Engineering Perspective》论文翻译","uri":"/posts/paper-reading/paxos-made-live/"},{"categories":["Paper Reading"],"content":"本篇文章是对论文Paxos Made Simple的原创翻译，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-09-21","objectID":"/posts/paper-reading/paxos-made-simple/:0:0","tags":["Paxos","Translation"],"title":"《Paxos Made Simple》论文翻译","uri":"/posts/paper-reading/paxos-made-simple/"},{"categories":["Paper Reading"],"content":"作者 Leslie Lamport ","date":"2020-09-21","objectID":"/posts/paper-reading/paxos-made-simple/:1:0","tags":["Paxos","Translation"],"title":"《Paxos Made Simple》论文翻译","uri":"/posts/paper-reading/paxos-made-simple/"},{"categories":["Paper Reading"],"content":"摘要 Paxos算法，用直白的话描述的时候，真的很简单。 ","date":"2020-09-21","objectID":"/posts/paper-reading/paxos-made-simple/:2:0","tags":["Paxos","Translation"],"title":"《Paxos Made Simple》论文翻译","uri":"/posts/paper-reading/paxos-made-simple/"},{"categories":["Paper Reading"],"content":"1. 引言 Paxos算法是用来实现容错的分布式系统的算法，一直以来都被认为难以理解，这可能是因为最初的论文中的描述方式对很多读者来说太迷惑了[5]注1。事实上，Paxos可能是分布式算法中最简单且最显而易见的。其核心是共识算法——[5]中的“教会”算法。下一章中说明了该共识算法遵循了几乎所有我们希望满足的不可避免的属性。最后一章解释了完整的Paxos算法，该章通过简单直接的共识程序实现了构建分布式系统用的状态机——通过状态机实现分布式系统的方式非常有名，因为这可能是有关分布式系统理论中最常被引用的文章的主题。[4] 注1：原文为“ the original presentation was Greek to many readers”。这个paper通篇都能感受到Leslie Lamport对我等凡人的嘲讽hhh。 ","date":"2020-09-21","objectID":"/posts/paper-reading/paxos-made-simple/:3:0","tags":["Paxos","Translation"],"title":"《Paxos Made Simple》论文翻译","uri":"/posts/paper-reading/paxos-made-simple/"},{"categories":["Paper Reading"],"content":"2. 共识算法 ","date":"2020-09-21","objectID":"/posts/paper-reading/paxos-made-simple/:4:0","tags":["Paxos","Translation"],"title":"《Paxos Made Simple》论文翻译","uri":"/posts/paper-reading/paxos-made-simple/"},{"categories":["Paper Reading"],"content":"2.1 问题描述 假设有一系列能够提议（propose）值的进程。共识算法保证在这些被提议的值中有单个值被选中。如果没有提议的值，那么应该不会有值被选中。如果一个值已经被选中，那么进程应该能够获悉被选中的值。共识的安全性需求有： 只有被提议的值才可能被选中； 仅一个值被选中； 除非值被选中，否则进程永远不会获悉该值。 我们不会试图明确这些需求。然而，这些需求的目标是确保某个被提议的值最终会被选中，且如果一个值被选中，那么进程最终会获悉该值。 我们让共识算法中的三种角色由三类agent执行：proposer、acceptor、和learner。在一个共识算法的实现中，单个进程可能作为不止一个agent，但是这里我们不需要关心agent到进程的映射。 假设agent可以通过发送消息的方式与另一个agent通信。我们使用了自定义的异步、非拜占庭（non-Byzantine）模型： agent以任意速度执行，可能宕机停止，也可能宕机重启。因为所有的agent可能在值被选取（choose）后故障并随后重启，所以除非一些信息可以在agent故障和重启后仍能被agent记得，否则不可能有解决方案。 消息分发可以消耗任意长的时间，可以重复也可以丢失，但是消息不会损坏。 ","date":"2020-09-21","objectID":"/posts/paper-reading/paxos-made-simple/:4:1","tags":["Paxos","Translation"],"title":"《Paxos Made Simple》论文翻译","uri":"/posts/paper-reading/paxos-made-simple/"},{"categories":["Paper Reading"],"content":"2.2 值的选取 选取（choose）值的最简单的方式是只有一个acceptor agent。一个proposer向该acceptor发送一个提议（proposal），acceptor选取其收到的第一个提议值。尽管这种方式很简单，但是其不符合需求，因为acceptor的故障会让之后的过程无法继续。 因此，我们尝试另一种选取值的方式。与其使用单个acceptor，让我们采用多个acceptor agent。一个proposer将提议值发送给一系列acceptor。acceptor可能接受（accept） 该提议值。当值被足够大的acceptor的集合接受时，值会被选取。那么多大才是“足够大”呢？为了保证仅有一个值被选取，我们让足够大的集合中包括任意的大部分的agent。因为任意两个“大多数的agent”的交集中会有一个共有的agent，所以如果一个acceptor最多只能接受一个值，那么这种方法就是可行的。（这是很多论文中得出的对“大多数”的推论，这一推论最早可能来自[3]。） 在没有故障和信息丢失的情况下，我们希望选取一个值，尽管只有一个proposer且仅提议了一个值，这需要： P1： acceptor必须接受其收到的第一个提议。 但是这会引起一个问题。在大概相同的时间可能存在来自多个不同的porposer提议的多个值，这会导致虽然每个acceptor都接受了一个值，但是没有同一个提议值被大多数的acceptor接受。甚至在仅有两个提议值的情况下，如果每个值被大概半数的acceptor接受，单个acceptor的故障可能会导致无法得知被选取的是哪个值。 在P1之前提到的需求中，一个值只有被大多数的acceptor接受时，该值才会被选取，这意味着acceptor必须能够接受超过一个提议。我们通过为每个提议分配一个（自然数）编号的方式来追踪一个acceptor可能接受的不同的提议，这样，一个提议由一个提议号和一个值组成。为了避免冲突，我们需要让不同的提议有不同的提议号。其实现方法依赖具体的实现，目前我们只需要假设这一点即可。一个值在包含该值的提议被大多数的acceptor接受时被选取。在这种情况下，我们称该提议（和提议的值）被选取。 我们可以允许有多个提议被选取，但是我们必须保证所有被选取的提议都有相同的值。为了保证这一点，我们对提议号做如下归纳： P2： 如果有值$v$的提议被选取，那么被选取的每个有更大的提议号的提议的值都为$v$。 因为提议号全都是有序的，条件P2保证了“只有一个值被选取”的关键属性。 为了能被选中，提议必须要被至少一个acceptor接受。所以，我们可以通过满足如下的条件来满足P2： P2a： 如果一个有值$v$的提议被选取，那么被任意acceptor选取的每个有更大的提议号的提议的值都为$v$。 我们仍保持P1以确保某个提议被选取。因为通信是异步的，提议可以被某个从未收到任何提议的acceptor $c$选取。假设一个新proposer被“唤醒”，并提出了一个有不同值的提议号更高的提议。P1要求$c$接受该提议，这违背了P2a。为了同时维护P1和P2a，需要将P2a增强为： P2b： 如果一个有值$v$的提议被选取，那么被任意proposer提出的每个有更大的提议号的提议的值都为$v$。 因为提议在被acceptor提出前，必须先被proposer提出，因此P2b包含P2a，而P2a也包含P2。 为了探究如何满足P2b，让我们来考虑一下如何证明它成立。我们先假设某个提议号为$m$、值为$v$的提议被选取了，这意味着任何被提出的提议号为$n（n\u003em）$的提议同样有值$v$。我们通过对$n$进行归纳来使证明更加简单，因此我们可以通过如下方式证明。假设每个被提出的提议号为$m..(n-1)$的提议值为$v$，$i..j$表示从$i$到$j$的一组数，那么提议号为$n$的提议的值为$v$。对于将会被选取的提议号为$m$的提议，必须有某个acceptor的集合$C$，$C$由大多数的acceptor组成，且每个$C$中的acceptor的都接受了该提议。将其与归纳假设结合，可得知，$m$被选取的假设包含了： $C$中的每个acceptor都接受了一个提议号在$m..(n-1)$间的提议，且每个提议号在$m..(n-1)$且被任意acceptor接受的提议的值都为$v$。 因为任意由大多数acceptor组成的集合$S$中会包含$C$中的至少一个acceptor，我们可以得出，在确保如下的不变式成立时，提议号为$n$的提议值为$v$成立： P2c： 对于任意$v$和$n$，如果有提议号为$n$且值为$v$的提议被提出，那么有由大多数acceptor组成的集合$S$，（a）$S$中没有acceptor接受了提议号小于$n$的提议，或（b）$v$是被$S$中的acceptor接受的提议号小于$n$的所有提议中，提议号最高的提议的值。 因此，我们可以通过维护不变式P2c来满足P2b。 为了维护不变式P2c，想要提出一个提议号为$n$的proposer必须获悉（如果存在的话）已经被或将要被任意大多数acceptor接受的提议号小于$n$的提议中提议号最大的提议。获悉已经被接受的提议很简单，但是预测未来要被接受的提议就很困难。与其试图预测未来，proposer通过兑现“未来不存在这种接受情况”这一承诺来控制这一点。换句话说，porposer要求acceptor不再接受提议号小于$n$的提议。这导致采用如下算法来提出协议： proposer选取一个新的提议号为$n$的提议，并将一个请求发送给某个acceptor的集合的每个成员，要求对方做出如下响应： (a) 承诺永远不会再接受提议号小于$n$的提议； (b) 承诺永远不会再接受其已经接受过的提议号小于$n$的提议中提议号最大的提议（如果存在的话）。 我们称这样的请求为编号为$n$的prepare请求。 如果proposer收到了来自大多数acceptor的对其请求的响应，那么它可以提出一个提议号为$n$、值为$v$的提议，其中$v$是所有响应中提议号最高的响应的值，或者当响应者没有报告提议时，$v$可以是由proposer选取的任何值。 proposer通过向某个acceptor的集合发送接受该提议的请求来提出提议。（不需要与响应其最初请求的acceptor集合是相同的集合。）我们称这个请求为accept请求。 这描述了proposer的算法。那么acceptor的算法是怎样的呢？其可能接受两种来自proposer的请求：prepare请求和accept请求。acceptor可以在不影响安全性的情况下接受或忽略任何请求。所以，我们只需要说明其什么时候可以相应请求。acceptor总是可以响应prepare请求。当且仅当acceptor没有承诺不接受时，acceptor可以响应accept请求并接受其提议。换句话说： P1a： 当且仅当acceptor没有响应一个提议号大于$n$的prepare请求时，其可以接受一个提议号为$n$的提议。 显然，P1a包含了P1。 现在，我们在假设提议号唯一的条件下，得到了满足安全性的完整的选取值的算法。最终算法只需要一个小优化就能得到。 假设acceptor收到了一个提议号为$n$的prepare请求，但是它已经响应了提议号大于$n$的prepare请求（因此它承诺了不再接受提议号为$n$的新提议）。这样acceptor没有响应这个新prepare请求的理由，因为它不会接受该proposer想要提出的提议号为$n$的提议。所以，我们让acceptor忽略这样的prepare请求。我们还让acceptor忽略其已经接受的提议的prepare请求。 通过这个优化，acceptor只需要记住其曾经接受过的提议号最高的提议和其响应过的prepare请求中最高的提议号。因为无论是否发生故障，P2c都需要被保证，所以即使acceptor故障且随后重启，其也必须能够记住这个信息。需要注意的是，proposer总是可以丢弃一个提议并忘记关于该提议的一切，只要该proposer不再试图提出另一个有相同提议号的提议。 将proposer和acceptor的行为放在一起，我们可以发现算法操作包括如下两个阶段。 阶段1： （a）prposer选取一个提议号$n$，并向大多数acceptor发送有提议号$n$的prepare请求。 （b）如果acceptor收到了一个prepare请求，且其提议号大于任何它已经响应过的prepare请求的提议号，那么该acceptor承诺其不再接受任何提议号小于$n$的提议和（如果存在的话）其接受过的提议中提议号最高的提议。 阶段2： （a）如果proposer收到了来自大多数acceptor的对其（提议号为$n$的）prepare请求的响应，那么该propsoer会向这些acceptor中的每一个发送一个对于提议号为$n$、值为$v$的accept请求，其中$v$是这些响应中提议号最高的值，或者如果响应中没有报告任何提议，那么可以是任意值。 （b）如果acceptor收到了对提议号为$n$的accept请求，该acceptor会接受这个提议，除非它已经响应过提议号大于$n$的prepare请求。 proposer可以提出多个提议，只要它对每一个提议都按照算法执行即可。proposer可以在协议中的任意时间丢弃提议。（即使提议的请求和（或）响应在该提议被丢弃很久以后才到达目的地，仍能维持正确性。）当某个proposer开始试图提出一个提议号更高的提议时，丢","date":"2020-09-21","objectID":"/posts/paper-reading/paxos-made-simple/:4:2","tags":["Paxos","Translation"],"title":"《Paxos Made Simple》论文翻译","uri":"/posts/paper-reading/paxos-made-simple/"},{"categories":["Paper Reading"],"content":"2.3 值的获悉 为了获悉被选取的值，learner必须发现被大多数acceptor接受的提议。一个很显然的算法是，每当acceptor接受一个提议时，acceptor会响应所有的learner，向它们发送该提议。这样可以让learner尽快发现被选取的值，但是这要求每个acceptor响应每个learner——响应的数量等于acceptor的数量与learner数量的乘积。 对于“非拜占庭失效（non-Byzantine failures）”的假设可以让一个learner能够很容易地从另一个learner知悉一个值被接受的事件。我们可以让acceptor将接受值的事件响应给一个“高级的（dinsinguished）”，该learner反过来会在有值被选取时通知其他learner。这种方法需要所有的learner通过额外一轮操作来获取被选取的值。且这种方法的可靠性更低，因为这个“高级的”learner可能故障。但是这种方法所需的响应数量仅等于accetpor和learner的数量的和。 更通用的方法是，acceptor可以将其接受值的事件响应给某给高级的learner的集合，集合中的每个learner随后当值被选取时通知所有的learner。使用更大的learner集合能够提供更好的可靠性，但代价是通信更加复杂。 因为有消息丢失的情况，值可能在没有learner发现的情况下被选取。虽然learner可以询问acceptor它们接受了哪些提议，但是如果acceptor故障可能导致其无法得知一个特定的提议是否被大多数接受。在这种情况下，learner会仅在新的提议被选取时才会发现被选取的值是什么。如果learner需要知道值是否被选取，它可以让proposer通过之前描述的算法提出一个提议。 ","date":"2020-09-21","objectID":"/posts/paper-reading/paxos-made-simple/:4:3","tags":["Paxos","Translation"],"title":"《Paxos Made Simple》论文翻译","uri":"/posts/paper-reading/paxos-made-simple/"},{"categories":["Paper Reading"],"content":"2.4 保证进行 我们可以很容易地构造出一个情境，场景中两个proposer，每个proposer都持续地提出有不断增大的提议号的提议序列，但是没有任何一个提议被选取。proposer $p$完成以提议号$n_1$阶段1。随后，另一个proposer $q$以提议号$n_2（n_2\u003en_1）$完成阶段1。因为所有的acceptor已经承诺不再接受任何提议号小于$n_2$的新提议，proposer $p$的阶段2的提议号为$n_1$的accept请求会被忽略。因此，proposer $p$随后会以新的提议号$n_3（n_3\u003en_2）$来开始并完成阶段1，这会导致prposer $q$阶段2的accept请求会被忽略。以此类推。 为了保证进行，必须选取一个“高级的”porposer，它会作为唯一一个提出提议的propsoer。如果这个高级的proposer可以成功与大多数acceptor通信，且如果该它使用了有比之前使用过的更大提议号的提议，那么它提出的提议会被成功接受。如果高级的propsoer发现某个请求有更高的提议号，那么它可以通过丢弃当前的提议并重试的方式，最终它可以选择一个足够高的提议号。 如果系统中足够的部分（proposer、acceptor、和通信网络）正常运行，可以通过选取单个高级的proposer来保证系统活性。Fischer、Lynch和Patterson[1]的著名的研究结果表明，用来选举proposer的可靠的算法必须使用随机会实时的算法（例如使用超时时间）。然而，无论选举的成功与否，都能确保安全性。 ","date":"2020-09-21","objectID":"/posts/paper-reading/paxos-made-simple/:4:4","tags":["Paxos","Translation"],"title":"《Paxos Made Simple》论文翻译","uri":"/posts/paper-reading/paxos-made-simple/"},{"categories":["Paper Reading"],"content":"2.5 实现 Paxos算法[5]设想有一个进程网络。在Paxos共识算法中，每个进程会扮演proposer、acceptor、和learner中的一个角色。算法会选取一个leader，其会扮演高级的proposer和高级的learner的角色。Paxos共识算法正是上面描述的算法，其中请求和响应作为普通的消息发送。（响应消息会通过相应的提议号标识，以防混淆。）能在故障期间保存信息的稳定存储被用来维护acceptor必须记住的信息。acceptor会在真正发送响应之前将其要发送的响应记录在稳定存储中。 剩下的工作就是描述一种保证被提出的提议中没有两个提议的提议号相同的机制。不同的propsoer会从不相交的编号的集合中选取其提议号，所以两个不同的proposer永远不会提出有相同提议号的提议。每个proposer会在稳定存储中记住其已经试图提出过的有最大提议号的提议，并使用比任何使用的提议高更高的提议号来开始阶段1。 ","date":"2020-09-21","objectID":"/posts/paper-reading/paxos-made-simple/:4:5","tags":["Paxos","Translation"],"title":"《Paxos Made Simple》论文翻译","uri":"/posts/paper-reading/paxos-made-simple/"},{"categories":["Paper Reading"],"content":"3. 实现一个状态机 实现一个分布式系统的一个简单方式是将其作为向中央服务器提出指令的客户端的集合。该服务器可以被描述为一个动态的状态机，其按照某个顺序执行客户端的指令。该状态机有一个当前状态，其通过将指令作为输入并产生一个输出和新状态的方式执行一步。例如，分布式银行系统的客户端可能是出纳员，状态机的状态可能由所有用户的账户余额组成。取钱通过执行一个状态机指令实现，该指令会在当且仅当用户余额大于总取钱的量的时候减少账户余额，将旧余额和新余额作为生产的输出。 在使用单个中央服务器的实现中，如果服务器故障，那么该实现就会故障。因此，我们使用了一个服务器的集合，每个服务器独立实现了一个状态机。因为状态机是动态的，如果所有的服务器都按照相同的指令序列执行，那么他们产生的状态序列就是相同的。这样，客户端提出的指令可以使用任何一个服务器生成的输出。 为了保证所有服务器执行相同的状态机指令序列，我们实现了一系列的独立的Paxos共识算法的实例，被第$i$个实例选取的值将作为序列中的第$i$个状态机指令。每个服务器在每个算法的实例中扮演所有角色（roposer、acceptor、和learner）。目前，我假设服务器的集合是固定的，所以共识算法的所有实例都使用同一个agent集合。 在正常的操作中，一个服务器会被选举为leader，其会在共识算法的所有实例中作为高级的proposer（唯一能够试图提出提议的）。客户端将指令发送给leader，leader会决定每个指令在序列中的出现位置。如果leader决定一个特定的客户端指令为第135个指令，其会试图选取该指令为共识算法的第135个实例。通常这会成功。这也可能因为故障或因为另一个服务器也认为自己是leader并对第135个指令有其他想法而导致失败。但是共识算法会确保最多只有一个指令被选取为第135个指令。 让这种方法变得高效的关键是，在Paxos共识算法中，被提出的值直到第2阶段才会被选取。回忆一下，在proposer的算法的阶段1执行完成后，将要被提议的值已经被决定，或者proposer可以自由地提议任意值。 现在我将描述Paxos状态机的实现如何在正常操作中工作。之后，我将讨论什么可能发生错误。我考虑了在之前的leader刚刚故障后且新的leader被选举出来时会发生什么。（系统启动是一种特殊情况，其之前没有任何指令被提议。） 新的leader（也在共识算法的所有实例中作为learner）应该知道大部分已经被选取的指令。假设其知道指令1~134、138、和139——即共识算法中实例1~134、138、和139选取的值。（之后我们将看到指令序列的这种间隔是怎么产生的。）该leader随后将执行135~137和所有大于139的实例的阶段1。（我将在下文描述这是如何做到的。）假设执行结果决定了将在实例135、140中被提议的值，但是所有其他实例中被提议的值不受约束。leader随后会对实例135和140执行阶段2，从而选取指令135和140。 leader和任何其他知道所有leader知道的指令服务器，现在可以执行指令1~135。然而，它还不能还不能执行指令138~140，虽然它也知道这些指令，因为指令136和137还没被选取。虽然leader可以将接下来两个由客户端请求的指令作为指令136和137，但是我们让它通过提议的方式立即填补这个空隙。一个不会改变状态的特殊的“nop”指令会作为指令136和137。（leader通过执行共识算法的实例136和137的阶段2来实现。）一旦这些没有操作的指令被选取，指令138~140就可以被执行。 现在指令1~140都被选取了。leader还完成了对共识算法中所有大于140的实例的阶段1，且leader可以自由地在这些实例的阶段2中提交任意值。其将指令号141分配给了下一个被客户端请求的指令，将其作为共识算法中实例141在阶段2中的值提议。其将下一个收到的客户端指令作为指令142提议，以此类推。 leader可以在得知其提议的指令141被选取前提议指令142。其发送的关于提议指令141的消息全部丢失是有可能发生的，且在任何其他服务器知道leader提议的指令141前就选取了指令142也是可能发生的。当leader没有收到其期待的对实例141的阶段2的响应，leader会重新发送这些消息。如果一切正常，其提议的值会被选取。然而，它也可能先失败，在被选取的指令序列中留下一个间隙。总之，假设leader能先得到$\\alpha$个指令——也就是说，leader可以在指令1~$i$被选取后，提议指令$i+1$~$i+\\alpha$。那么，就有可能产生最大$\\alpha -1$的指令空隙。 一个新被选取的leader对共识算法中无限多的实例执行阶段1——在上述场景中，是实例135~137和所有大于139的实例。leader可以通过发送一个适当的短消息给其他服务器来让所有实例都有相同的提议号。在阶段1中，acceptor仅当其已经从某个proposer收到了阶段2的消息时才会回复超过一个简单的OK。（在该场景中，这仅在实例135和140中会发生。）因此，作为acceptor的服务器可以通过单个合理的短消息响应所有的实例。因此，执行这些无限多的阶段1的实例是没有问题的。 因为leader的故障和新leader的选举应该是很少见的事件，执行状态机指令的有效开销（即让指令或值达到共识的开销）仅为执行共识算法阶段2的开销。可以看出，Paxos算法阶段2的开销在所有考虑了故障的为了达成共识的算法中是最小的[2]。因此，Paxos算法本质上是最优的。 对系统正常操作的讨论假设，除了在当前leader故障和新leader的选取间的短暂时间外，系统中总是有单个leader。在不正常的情况下，leader的选举可能失败。如果没有服务器作为leader，那么不会有新的指令被提议。如果多个服务器认为他们都是leader，那么他们都可以在共识算法中的相同的实例中提议值，这可能会让任何值都不会被选取。然而，安全性还是被保证的——两个不同的服务器永远不会将不同的值选取为第$i$个状态机指令。选举处单个leader只用来确保继续运行。 如果服务器的集合会发生变化，那么必须有某种方式来决定哪些服务器实现了共识算法的哪些实例。最简单的方式是让状态机本身实现。当前服务器的集合可作为状态的一部分，且可被通过普通的状态机指令改变。我们可以让leader提前获取$\\alpha$是$i$个状态机指令后的到的状态指定的共识算法中实例$i+\\alpha$实现的。这样可以通过简单的方式实现任意的复杂的重配置算法。 ","date":"2020-09-21","objectID":"/posts/paper-reading/paxos-made-simple/:5:0","tags":["Paxos","Translation"],"title":"《Paxos Made Simple》论文翻译","uri":"/posts/paper-reading/paxos-made-simple/"},{"categories":["Paper Reading"],"content":"参考文献 [1] Michael J. Fischer, Nancy Lynch, and Michael S. Paterson. Impossibility of distributed consensus with one faulty process. Journal of the ACM, 32(2):374–382, April 1985. [2] Idit Keidar and Sergio Rajsbaum. On the cost of fault-tolerant consensus when there are no faults—a tutorial. TechnicalReport MIT-LCS-TR-821, Laboratory for Computer Science, assachusetts Institute Technology, Cambridge, MA, 02139, May 2001. also published in SIGACT News 32(2) (June 2001). [3] Leslie Lamport. The implementation of reliable distributed multiprocess systems. Computer Networks, 2:95–114, 1978. [4] Leslie Lamport. Time, clocks, and the ordering of events in a distributed system. Communications of the ACM, 21(7):558–565, July 1978. [5] Leslie Lamport. The part-time parliament. ACM Transactions on Computer Systems, 16(2):133–169, May 1998. ","date":"2020-09-21","objectID":"/posts/paper-reading/paxos-made-simple/:6:0","tags":["Paxos","Translation"],"title":"《Paxos Made Simple》论文翻译","uri":"/posts/paper-reading/paxos-made-simple/"},{"categories":["Paper Reading"],"content":"本篇文章是对论文weil-osdi06的原创翻译，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:0:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"作者 Sage A. Weil, Scott A. Brandt, Ethan L. Miller, Darrell D. E. Long, Carlos Maltzahn University of California, Santa Cruz {sage, scott, elm, darrell, carlosm}@cs.ucsc.edu ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:1:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"摘要 我们开发了分布式文件系统——Ceph，其能提供极好的性能、可靠性和伸缩性。Ceph使用为不可靠的对象存储设备（unreliable object storage devices，OSD）的异构动态集群设计的伪随机数据分布函数（pseudo-random data distribution function）CRUSH来替代分配表（allocation），从而最大化地分离了数据管理与元数据管理。我们通过数据副本、故障检测、和恢复的方式，将设备的智能译注1应用到运行专用本地对象文件系统的半自治OSD中。动态的分布式元数据集群提供了极为高效的元数据管理能力，并适用于大部分通用文件系统和科学计算文件系统的负载。从在各种负载下的性能测试中可以得出，Ceph有着极好的I/O性能和可伸缩的元数据管理能力，能够支持超过每秒25000次元数据操作。 译注1：这里的智能（intelligence）指OSD设备上的CPU与内存能够提供的能力，在后文中有提到。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:2:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"1. 引言 系统长期以来一直在追求改进文件系统的性能，文件系统的性能已被证明对极多类型的应用程序的整体性能至关重要。科学计算和高性能计算社区推动了分布式存储系统在性能和伸缩性的提升，这比更通用的需求提前几年。如NFS[20]等传统解决方案提供了一个简单地模型，即服务器导出文件系统层次，客户端可将这个文件系统层次映射到它们的本地命名空间中。尽管这种方式被广泛使用，这种client/server模型中的中心化设计已被证明对可伸缩性的表现有很大阻碍。 更近一些的分布式存储系统采用了基于对象存储的架构，这种架构将传统的硬盘被替换为智能对象存储设备（OSD），OSD合并了CPU、网络接口、和有底层磁盘或RAID的本地缓存[4, 7, 8, 32, 35]。OSD用一个新的接口取代的传统的块级接口，客户端可以通过这个接口读写更大（且通常大小不一）的命名对象中的字节区间，并将低级块分配决策留给了设备本身。客户端通常与以元数据服务器（metadata server，MDS）交互的方式来执行元数据操作（如open、rename），而直接与OSD通信来以执行文件I/O（read、write），这大改善了整体的可伸缩性。 采用这种模型的系统由于很少或没有分发元数据的负载，其还是会受可伸缩性限制的困扰。而继续依赖传统文件系统的原则（如分配列表和inode列表）且不愿将智能委托给OSD进一步限制了可伸缩性和性能，且增加了可靠性的开销。 因此，我们提出了Ceph，Ceph是一个提供了极好的性能与可靠性且具有无可比拟的可伸缩性的分布式文件系统。我们的架构基于如下的假设：PB级别的系统本质上是动态的————大型系统不可避免的需要增量构建、节点故障是常态而不是意外、负载的量和特征会随着时间不断改变。 Ceph通过使用生成函数来替代文件分配表的方式，将数据操作与元数据操作解耦。这让Ceph能够利用OSD已有的智能来分散数据访问、串行更新、副本与可靠性、故障检测和恢复的复杂性。Ceph采用了高适用能力的分布式元数据集群架构，这极大地提高了元数据访问的可伸缩性，并因此提高了整个系统的可伸缩性。我们将讨论那些驱动我们选择了这种架构的目标与负载假设、分析它们对系统可伸缩性和性能的影响、并与我们在实现功能性系统原型的经验相结合。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:3:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"2. 系统总览 Ceph文件系统有3个主要部件：（1）客户端：为主机或进程暴露类POSIX文件系统接口的实例；（2）OSD集群：共同存储所有的数据与元数据；（3）元数据服务器集群：管理命名空间（文件名与目录），同时协调安全性（security）、一致性（consistency）和连贯性（coherence）。如图1所示。我们称Ceph接口是类POSIX接口，因为我们为了更好地与应用程序的需求对齐并改进系统性能，我们对POSIX接口进行了适当的扩展，并选择性地放松了一致性语义。 图1 系统架构。客户端通过直接与OSD通信来执行文件I/O。每个进程既可以直接链接到客户端实例，也可以通过一个挂载的文件系统交互。图1 系统架构。客户端通过直接与OSD通信来执行文件I/O。每个进程既可以直接链接到客户端实例，也可以通过一个挂载的文件系统交互。 \" 图1 系统架构。客户端通过直接与OSD通信来执行文件I/O。每个进程既可以直接链接到客户端实例，也可以通过一个挂载的文件系统交互。 该架构的主要目标是伸缩性（用于数百PB或更多）、性能、和可靠性。伸缩性被从多维度考虑，包括系统整体的存储容量、吞吐量、和单个客户端、目录或文件的性能。我们的目标负载可能包括一些极端情况，如数万或数十万个主机并发地读取或写入同一个文件，或在同目录下创建文件。这种场景在运行在超级计算机集群上的科学计算程序中很常见，并在未来的通用的负载中越来越多地起着指示作用。更重要的是，我们发现分布式文件系统的负载本质上是动态的，随着活动的应用程序和数据集随着时间的变化，数据与元数据的访问也显著的变化。Ceph直接解决了伸缩性的问题，同时通过三个基本设计特性解决了高性能、可靠性和可用性，这三种设计为：数据与元数据解耦、动态分布式元数据管理、和可靠的自主分布式对象存储。 数据与元数据解耦： Ceph将文件元数据与管理与文件数据的存储最大化地进行了分离。元数据操作（如open、rename等）被元数据集群共同管理，同时，客户端直接与OSD交互来执行文件I/O（读取和写入）。基于对象的存储通过将低级的块分配决策交给单个设备以提高文件系统的可伸缩性。然而，与现有的基于对象的文件系统[4, 7, 8, 32]不同，Ceph使用更短的对象列表替代较长的每个文件的块列表，因此完全消除了分配列表。文件数据被分条（strip）为可预测的命名对象上，同时通过一个叫CRUSH[29]的专用数据分布函数将对象分配到存储设备上。这让任一方都可以计算（而不是查找）组成文件内容的对象的名称与位置，消除了维护与分发对象列表的需求，简化了系统设计，减少了元数据集群的负载。 动态分布式元数据管理： 因为文件系统元数据操作组成了传统文件系统负载的一半[22]，高效的元数据管理对整个系统的性能非常重要。Ceph使用了一种基于动态子树分区（Dynamic Subtree Partitioning）[30]的新式元数据集群架构，并智能地将管理文件系统目录层级的责任分布到数十甚至数百个MDS上。（动态的）分层分区在每个MDS的负载都保持了局部性，这促进了高效地更新和主动式预取来改进常见的负载的性能。重要的是，元数据服务器间的负载分布完全基于当前的访问模式，使Ceph在任何负载下都能够高效利用可用的DMS资源，并实现与MDS数量呈近线性的伸缩性。 可靠的自主分布式对象存储： 由成千上万个设备组成了大型系统本质上是动态的：它们是被增量构建的，当部署新存储或退役旧设备时，它们会跟着伸展或收缩，且大量的数据会被创建、移动、和删除。所有的这些因素都要求数据的分布变得能够高效利用可用资源并维护所需的数据副本等级。Ceph把数据迁移、复制、故障检测、和故障恢复的责任托付给了存储数据的OSD集群，同时在上层，OSD共同为客户端和元数据服务器提供一个逻辑对象存储。这种方法让Ceph能够更高效地利用每个OSD的智能（CPU和内存）来实现可靠的、高可用的、有着线性伸缩性的对象存储。 本文中，我们描述了Ceph的客户端的操作、元数据服务器集群、和分布式对象存储，还描述了我们的架构怎样影响它们的关键特性。我们还描述了我们原型的状态。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:4:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"3. 客户端操作 我们将介绍Ceph组件的整体操作，并通过描述Ceph的客户端的操作的方式介绍它们与应用程序的交互。Ceph的客户端运行在每个执行应用程序代码的主机上，并为应用程序暴露文件系统接口。在Ceph原型中，完全运行在用户空间，且既可以直接链接到它来访问，又可以通过FUSE（一种用户空间文件系统接口）[25]作为挂载的文件系统访问。每个客户端维护它拥有的文件数据缓存，该缓存与内核也或缓冲区缓存独立，使直接链接了客户端的应用程序能够访问它。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:5:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"3.1 文件I/O和功能 当进程打开一个文件时，客户端会向MDS集群发送一个请求。MSD会遍历文件系统的层次结构，将文件名转换为文件的inode，inode中包括唯一的inode编号、文件所有者、模式、大小、和其他的单文件元数据。如果文件存在并有访问权限，MDS会返回inode编号、文件大小和将文件数据映射到对象的分条策略的信息。MDS可能还会向客户端发送（如果其还没收到过）指定了哪些操作是被允许的功能（capability）信息。目前，这些功能包括4位，其控制客户端的读（read）、缓存读（cache reads）、写（write）、和缓冲写（buffer writes）的能力。在未来，其功能还将包括允许客户端向OSD证明其被授权了读写数据权限的安全密钥[13, 19]（目前，原型信任所有客户端）。后续操作中，MDS在文件I/O中的参与仅限于管理功能，以维护文件的一致性并实现适当的语义。 Ceph有很多将文件数据映射到一系列对象上的分条策略。为了避免任何对与新文件分配（allocation）相关元数据的需求，对象名简单地将文件inode编号和条带（stripe）号。接着，会使用CRUSH将对象的副本分配到OSD上，CRUSH是全局可知的映射函数（在章节5.1中介绍）。例如，如果一个或多个客户端打开了一个文件用来读取，MDS会为它们授权读取和缓存文件内容的功能。有了inode编号、布局、和文件大小，客户端可以命名并定位所有包含文件数据的对象，并能够直接从OSD集群读取。任何不存在的对象或字节区间被定义为文件的“洞”，或者“零值”。类似地，如果客户端打开了文件用来写入，它会被授权带缓存的写入的功能，其在该文件的任何偏移量上生成的任何数据会被简单地写入到适当OSD上的适当对象中。客户端在文件关闭时会放弃（relinquish）对应的功能，并向MDS提供新文件的大小（写入的最大偏移量），这会重新定义（可能）已有的包含文件数据的一系列对象。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:5:1","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"3.2 客户端同步 POSIX要求读取操作能够反映之前写入的任何数据，且写入操作时原子性的（也就是说，重叠的并发写入将会反映一个特定的写入顺序）。当文件被多个客户端打开时（多个writer或既有writer又有reader），MDS将会收回之前任何的缓存读取和缓冲区写入功能，强制同步客户端对该文件I/O。这样，每个应用程序的读取或写入操作将会被阻塞，直到OSD确认，这增加了OSD中存储的每个对象的串行更新和同步的负担。当写入跨对象边界时，客户端会请求对受影响的对象的独占锁（由这些对象所在的各自OSD授权），并立即提交写入并解锁操作，以实现所需的串行性。对象锁还可被用于在大型写入时，通过获取锁并异步冲刷（flush）数据来掩盖延迟。 意料中的是，同步I/O对应用程序的性能有很大的影响，特别是对那些进行少量读写的应用程序来说更加明显，这时延迟造成的，其至少需要与OSD的一次往返的延迟。尽管在通用的负载中，读写共享的情况相对比较少[22]，但是在可写计算应用程序中，这种场景是非常常见的[22]，且这种情况下性能通常很重要。因此，当应用程序不需要依赖严格的标准一致性时，通常希望能够放松一致性来减少开销。尽管Ceph支持通过全局的开关来放松一致性，正如许多其他分布式文件系统在该问题上做的一样[20]，但是这是一种不精确且不能令人满意的觉接方案：要么性能会下降，要么会在系统范围下丢失一致性。 正由于这个原因，高性能计算（high-performance computing，HPC）社区[31]提出了一系列的对POSIX I/O的高性能计算扩展接口，Ceph中实现了这些接口中的一个子集。其中最引人注意的是，open操作的O_LAZY标识符允许应用程序显式地放松对共享写文件通常的连贯性要求。管理自己连贯性（例如HPC负载中常见的模式，通过写入同一个文件的不同部分）的性能敏感型程序在执行I/O时就可以通过缓冲区写入或通过缓存读取，否则只能同步执行。如果需要，应用程序可以进一步显式地通过两种额外的调用进行同步：lazyio_propagate会将给定的字节区间冲刷到对象存储中、lazyio_synchronize会确保过去的修改会在任何后续的读取中反映。因此，为了保持Ceph同步模型的简单性，其在客户端间通过同步I/O提供正确的读-写和共享写语义，并扩展了应用程序接口来放松性能敏感的分布式程序的一致性。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:5:2","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"3.3 命名空间操作 客户端与文件系统命名空间的交互由元数据服务器集群管理。读操作（如readdir、stat）和更新（如unlink、chmod）都由MDS同步应用，以确保串行、一致性、正确的百密性（correct security）、和安全性（safety）。为了简单起见，客户端不使用元数据锁或租约。特别是对于HPC的负载，回调能够提供好处很小，但复杂性的潜在开销很高。 相反，Ceph为大多数通用元数据访问场景做了优化。在readdir之后对每个文件执行stat（例如，ls -l）是一个非常常见的访问模式，且是在大目录下臭名昭著的性能杀手。Ceph中的readdir仅需一次MDS请求，它会拉取整个目录。包括inode的内容。默认情况下，如果readdir后面会立刻接一个或多个stat，那么被缓存的简短的信息会被返回；否则，缓存的信息就被丢弃。虽然这种方法在中间inode修改可能不会被注意到的情况下稍稍放松了连贯性，但是我们还是十分乐于通过这种交换来大幅改进性能。这种行为可被readdirplus[31]扩展显式地捕捉到，它会返回整个目录的lstat的结果（正如再一些专用OS的实现中getdir已经做的那样）。 Ceph可以通过更久地缓存元数据来允许一致性被进一步放松，这很像早期版本的NFS做的那样，其通常缓存30秒。然而，这种方法会在某种程度上打破连贯性，通常这对应用程序来说是很重要的，比如那些使用stat来判断一个文件是否被更新过的应用程序。如果这样做，这些应用程序可能会执行不正确的行为，或要等待旧的缓存值超时。 我们再次选择了提供正确的行为并扩展了对性能有不利影响的接口。这种选择可通过下例清楚的说明：对一个被多个客户端并发为写入而打开的文件的stat操作。为了返回正确的文件大小和修改时间，MDS会收回任何写入的功能，以立刻停止更新并采集最新的大小和所有writer的修改时间。其中最大值会被返回，随后被撤销的功能会被重新下发以执行后续进程。尽管停止多个writer似乎有些过于激进，但为了保证合理的串行化，这时必需的。（对于单个writer，可以从正在写入的客户端检索到正确值，而不需要打断进程。）不需要连贯性行为的应用程序（也就是需求与POSIX接口不一致的受害者）可以使用statlite[31]，其通过一个位掩码来执行哪些inode字段不需要连贯性。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:5:3","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"4. 动态分布式元数据 元数据操作经常会占用文件系统一半的负载[22]且操作位于关键路径中，这对MDS集群的整体性能来说是至关紧要的。元数据管理也成为了分布式文件系统中伸缩性的重要挑战：尽管在增加更多存储设备时，容量和总I/O速率几乎可以任意伸缩，但是元数据操作设计更大程度的相互依赖关系，这使可伸缩的一致性与连贯性管理变得更加困难。 Cpeh中的文件和目录的元数据非常小，其由几乎整个目录里的条目（文件名）和inode（80B）组成。不像传统的文件系统，Ceph中没有文件分配（allocation）元数据是必要的——对象名由inode号组成，并通过CRUSH分布到OSD中。这简化了元数据负载并使我们的MDS能够高效的管理大量的文件，无论文件有多大。我们的设计通过双层存储策略（two-tiered storage strategy）进一步追求减少元数据相关的磁盘I/O，并最大化局部性（locality），且通过动态子树分区（Dynamic Subtree Partitioning）[30]来高效利用缓存。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:6:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"4.1 元数据存储 尽管MDS集群的目标是通过其内存缓存来满足大部分请求，但是为了安全起见，元数据的更新必须被提交到磁盘上。一系列大量、有界、懒惰冲刷的日志（journal）让每个MDS可以高效、分布式地将其更新的元数据流式写入到OSD集群中。每个MDS的几百MB的日志中也会有重复的元数据更新（在大多数负载中很常见），因此当旧的入职条目最终被冲刷到长期存储时，许多条目已经被废弃了。尽管MDS的恢复目前还没在我们的原型中实现，日志还是按照如下功能设计的：当一台MDS故障时，另一台节点可以快速重新扫描日志并恢复故障节点内存缓存中的重要内容，以恢复文件系统的状态。 这种策略是两全其美的：可以高效（顺序）地流式更新到磁盘，且大大较少了重写的负载，这允许长期磁盘存储的布局可对未来的读访问进行优化。特别是，inode被直接嵌入到了目录中，这让MDS可以通过单词OSD读请求预拉取整个目录，并高度利用大部分负载中的目录局部性[22]。每个目录的内容会使用与元数据日志和文件数据相同的分条和分布策略写入OSD集群中。inode编号会按照一定范围分配给元数据服务器，且在我们的原型中inode编号被认为是不可变的，尽管之后在文件删除时它们可能会被简单地回收。辅助锚表（auxiliary anchor table）用来保存很少见的有多个硬链接的可通过编号全局寻址的inode，所有这些都不需要使用非常常见的有庞大、稀疏且笨重的inode表的单链接文件。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:6:1","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"4.2 动态子树分区 我们的主拷贝缓存策略（primary-copy caching strategy）让一个权威的MDS负责管理对任意给定元数据片段的缓存一致性和串行更新。大部分现有的分布式文件系统使用某种基于子树的静态分区的形式来授权（通常强迫管理员将数据集分割为更小的静态“卷”），一些最近的实验性文件系统使用了哈希函数来分布目录和文件元数据[4]，这为分摊负载而牺牲了局部性。这两种方法都有严重的局限性：静态子树分区无法应对动态的负载和数据集，而哈希破坏了元数据的局部性和实现高效元数据预拉取与存储的重要的可能性。 图2 Ceph动态地将目录层级的子树根据当前的负载映射到元数据服务器中。每个独立的目录仅当它们成为热点时，才会被通过哈希分布到多个节点中。图2 Ceph动态地将目录层级的子树根据当前的负载映射到元数据服务器中。每个独立的目录仅当它们成为热点时，才会被通过哈希分布到多个节点中。 \" 图2 Ceph动态地将目录层级的子树根据当前的负载映射到元数据服务器中。每个独立的目录仅当它们成为热点时，才会被通过哈希分布到多个节点中。 Ceph的MDS集群基于一种动态子树分区策略[30]，其在一组节点间自适应地分层分布缓存元数据，如图2所示。每个MDS会通过随时间指数衰减的计数器来测量每个目录层级的元数据的流行度（popularity）。任何操作都会增长受影响的inode和从它们开始向上直到根目录所有计数器，这为每个MDS提供了描述最近负载分布的加权树。每个一段时间，MDS的负载值会被进行比较，且目录层级的适当大小的子树会被迁移，以保证负载最终会被分布。使用共享的长期存储和谨慎构造的命名空间锁，让这种迁移可以通过将内存缓存中的适当的内容传输到新的被授权的节点实现，这样可以减少对连贯性锁或客户端功能的影响。为了安全起见，导入的元数据会被写入新MDS的日志中，且两边的额外日志确保了授权的转移不会受中间发生的故障影响（类似于两段式提交）。最终得到的基于子树的分区会保持粗粒度，以减少前缀复制开销并保持局部性。 当元数据被复制到多个MDS节点时，inode的内容会被划分为三组，每组都有不同的一致性语义：安全组（所有者、模式）、文件组（大小、修改时间）、和不可变组(inode编号、创建时间、布局)。不可变组的字段永远不会改变，而安全组合文件组会被单独的有限状态机管理，每个有限状态机都有不同的一系列状态和转移，这样的设计是为了在减少锁的争用的同时适应不同的访问和更新模式。例如，在路径遍历时，会需要对“所有者”和“模式”进行安全检查，但二者很少变化，仅需很少的状态；而因文件锁控制MDS发送给客户端的功能，其反映了更广的客户端访问模式， ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:6:2","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"4.3 流量控制 将目录层级在多台节点上分区可以均衡负载，但是不是总能解决热点（hot sopt）和瞬时拥堵（flash crowd）问题，在这些问题中，许多客户端会访问同一个目录或文件。Ceph通过它对元数据流行度的了解，将热点宽泛地分布，这仅在需要时执行，且一般情况下不会曹正相关的额外开销和目录局部性的损失。被大量读取的目录（例如被多个open打开）的内容会被有选择地在多个节点上备份以分布负载。特别大或者有大量写入负载的目录（如目录有许多文件的创建）的内容会按文件名哈希并分布在集群中，以牺牲目录的局部性来换取负载的平衡。这种自适应的方法让Ceph适用于各种分区粒度，让文件系统在特定环境下的不同部分能够有最有效的分区粒度策略，让系统能同时获得粗粒度和细粒度带来的好处。 每个MDS的响应都为客户端提供了有关授权和相关inode和其任何副本及祖先inode的更新后的信息，让客户端了解文件系统中与客户端交互的部分的元数据分区。之后的元数据操作将会基于给定路径中已知的最深前缀，直接与被授权的结点进行（对于更新操作），或者与随机一份副本进行（对于读取操作）。通常，客户端会得知不流行（没被做副本的）的元数据的位置，且可以直接与适当的MDS通信。而对于访问流行元数据的客户端，它们会被告知元数据贮存在不同的或多个MDS节点上，这可以有效限制认为元数据中任一部分贮存在任一MDS上的客户端的数量，这样可以在潜在的热点和瞬时拥堵产生前分散负载。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:6:3","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"5. 分布式对象存储 从上层来看，Ceph的客户端和元数据服务器视对象存储集群（可能有上万或上十万个OSD）为一个对象存储和命名空间。Ceph的可靠自主分布式对象存储（Reliable Autonomic Distributed Object Store，RADOS）在容量和整体性能方面实现了线性伸缩性，这是通过将对象备份、集群扩展、故障检测和恢复等方面的管理分布式地授权给OSD实现的。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:7:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"5.1 使用CRUSH分布数据 Cpeh必须将PB级的数据分布到由数千个存储设备组成的不断演进的集群上，这样可以有效地利用设备存储和带宽资源。为了避免不平衡（例如，最近部署的设备几乎是空闲或空的）或负载不对称（例如，新的、热点的数据仅在新设备上），我们使用了这样一种策略：随机分布新数据、对已有数据随机二次抽样并迁移到新设备、均匀地重分配被移除的设备中的数据。这种随机的方法具有鲁棒性，其可以在任何潜在的负载中有同样好的表现。 图3 文件被分条为许多对象，且被分组到放置组（placement group，PG）中，并通过专用的副本放置函数——CRUSH分布到OSD上。图3 文件被分条为许多对象，且被分组到放置组（placement group，PG）中，并通过专用的副本放置函数——CRUSH分布到OSD上。 \" 图3 文件被分条为许多对象，且被分组到放置组（placement group，PG）中，并通过专用的副本放置函数——CRUSH分布到OSD上。 Ceph首先使用一个简单的哈希函数将对象映射到放置组（placement group，PG）中，该哈希函数中有一个可调的位掩码，以控制PG的数量。我们选定的值给每个OSD大约100个PG，来通过平衡每个OSD维护的副本相关的元数据的总量，以平衡OSD利用率的差异。接着，会将放置组通过CRUSH算法（Controlled Replication Under Scalable Hashing）分配给OSD，CRUSH是一个伪随机数据分配函数，其可以高效地将每个PG映射到一个有序的存储对象副本的OSD列表上。这与传统的方法（包括其他的基于对象的文件系统）的不同之处在于，数据分配不依赖任何块或对象列表元数据。为了定位任意对象，CRUSH仅需要放置组和OSD集群映射，二者是对组成存储集群的设备的紧凑、分层的描述。这种方法有两个关键优势：首先，这是完全分布式的，任何一方（客户端、OSD或MDS）可以独立地计算任何对象的位置；第二，映射不会频繁更新，这消除了任何与数据分布相关的元数据交换。这样做，CRUSH同时解决了数据分布问题（“我该把数据存在哪儿”）和数据定位问题（“我把数据存到哪儿了”）。按照设计，对存储集群的较小的修改对已有的PG映射影响非常小，这减少了由于设备故障或集群扩展而导致的数据迁移。 集群映射的结构层次被构造为与集群的物理或逻辑组成和潜在的故障源保持一致。例如，用户可以为一个由满是OSD的机架格（shelf）、满是机架格的机架（rack cabinet）、和多行机架（row of cabinet）组成的设备构造一个4层的映射。每个OSD还有一个权值，用来控制分配给它的相对的数据总量。CRUSH会基于放置规则（placement rule）把PG映射到OSD，放置规则定义了副本级别和任何放置上的约束。例如，用户可能想把每个PG在3个OSD上做副本，且所有副本都在同一行机架（以限制机架行间的备份流量）但位于不同的机架上（以减少电源电路或边缘开关故障带来的影响）。集群映射还包括一个离线或非活跃设备的列表和一个时期号（epoch number），每次映射变化时该时期号会增加。所有OSD请求会被打上客户端映射的时期号的标签，这样，所有方会对当前的数据分布达成一致。增量的映射更新会在协作的OSD间共享，如果映射更新，OSD的回复中也会带上这个数据。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:7:1","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"5.2 副本 与像Lustre[4]这样的系统不同，Lustre假设用户可以在SAN上使用RAID或故障转移机制（fail-over）来构建足够可靠的OSD，而我们假设PB或EB的系统中故障是正常时间而非异常事件，且在任意时间点都会有几个OSD可能无法使用。为了维护系统的可用性并在系统伸缩时仍能保证数据安全，RADOS使用一种主拷贝备份（primary-copy replication）的变体[2]来管理其自己的数据副本，同时采取措施以减少对性能的影响。 数据被按照放置组备份，每个放置组被映射到一个由n个OSD组成的有序列表上（为了n路备份）。客户端将所有的写请求发送到对象的PG中第一个没有故障的OSD中（主OSD，primary OSD），其会为该对象和PG分配一个新的版本号，并将写请求进一步传递给所有其他的备OSD（replica OSD）上。在每个副本都应用了更新并响应主OSD后，主OSD会将更新应用到本地并通知客户端这次写入。读请求会被定向到主OSD。这种方法为客户端省去了副本间同步或串行所带来的复杂性，在有其他writer或故障恢复时，这会变得很麻烦。这种方法还会将做副本消耗的带宽从客户端转移到OSD集群内部网络中，在我们的期望中，OSD集群内部网络会有更多的可用资源。中间的备OSD故障会被胡烈，因为任何的后续恢复操作（章节5.5）都将可靠地恢复副本的一致性。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:7:2","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"5.3 数据安全性 在分布式存储系统中，数据被写入共享存储的关键原因有两个。首先，客户端希望它们的更新对其他客户端可见。这一过程应该完成得很快：写入尽可能快地可见，特别是多writer既有reader又有writer的情况下，需要强制客户端同步操作。第二，客户端希望确切地知道它们写入的数据是否被安全地在磁盘上备份、数据是否可以在断点或其他故障时幸存。RADOS在得知更新时，会将同步与安全性分离，让Ceph能够实现高效应用程序同步中的低延迟，与良好定义的数据安全性语义。 图4阐述了在对象写入时发生的消息发送。主OSD将更新进一步传递给备OSD，并在更新被应用到所有OSD的内存缓冲区缓存后回复一个ack，让客户端的同步POSIX返回。当数据被安全地提交到磁盘时，会发送一个最终的commit（可能在数秒后）。我们仅在更新被完全被分到所有无缝容错的单个OSD后才会发送ack，尽管这回增加客户端的延迟。默认情况下，客户端还会缓冲写入请求，直到它们提交，以避免放置组中所有OSD同时断点时发生数据丢失。在这种情况下恢复时，RADOS允许在接受新的更新之前，在固定的时间内重放（replay）之前已知的（有序的）更新。 图4 RADOS在写入被应用到所有备份该对象的OSD的缓冲区缓存后回复ack。仅在写入被安全地提交到磁盘后，最终的commit通知才会发送到客户端。图4 RADOS在写入被应用到所有备份该对象的OSD的缓冲区缓存后回复ack。仅在写入被安全地提交到磁盘后，最终的commit通知才会发送到客户端。 \" 图4 RADOS在写入被应用到所有备份该对象的OSD的缓冲区缓存后回复ack。仅在写入被安全地提交到磁盘后，最终的commit通知才会发送到客户端。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:7:3","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"5.4 故障检测 及时的故障检测对维护数据安全是非常重要的，但是当集群扩展到几千台设备时，这会变得很困难。对于特定的故障，如磁盘错误或数据损坏，OSD可以自己报告。而对于使OSD网络不可用的故障，则需要主动监控，RADOS通过让每个OSD监控与其共享PG的对等节点（peer）来分配监控任务。在大多数情况下，已有的副本的流量会被用作被动的存活确认，而不需要额外的通信开销。如果OSD最近没有收到对登记诶单的消息，那么会显式地发送一个ping。 RADOS会送两个维度考察OSD的存活性：OSD是否可以访问、其是否被CRUSH分配了数据。没有相应的OSD最初会被标记为down状态，任何的主要职责（更新、串行化、副本）会被临时地传递给其PG中的下一个OSD。日过OSD没有快速恢复，其会被在数据分布中标记为out，另一个加入每个PG的OSD会重新复制其中的内容。而向故障OSD执行挂起操作的客户端只需要简单地重新提交到新的主OSD即可。 因为各种网络异常都可能导致OSD的网络间歇性中断，所以我们使用一个小的监控集群采集故障报告，并集中过滤出瞬时的或系统的问题（如网络分区）。监控者（仅被部分实现）采用选举、主动对等节点监控、短期租约、和两段式提交的方式共同提供对集群映射的一致且可用的访问。当映射更新并反映出任何故障或恢复时，会向受影响的OSD提供增量的映射更新，然后利用现有的OSD间的通信将更新扩散到整个集群中。分布式的检测可以在不过量增加监控负担的同时实现快速的检测，通知还可以解决集中式仲裁而导致的不一致。最重要的是，RADOS通过将OSD标记为down而不是out的方式，避免了因系统问题导致的大范围的数据重做副本的问题（例如断电后半数OSD挂掉）。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:7:4","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"5.5 恢复和集群更新 OSD集群会因OSD故障、恢复和显式的集群修改（如部署新存储）而改变。Ceph用相同的方式处理所有的这种改变。为了实现快速恢复，OSD为每个对象维护一个版本号并为每个PG维护一个最近改变（更新或删除的对象名和版本号）的日志（类似于Harp[14]中地副本日志）。 当一个活动的OSD收到了一份集群映射更新时，它会遍历本地存储的PG并计算CRUSH映射来决定它作为主OSD或备OSD负责哪些PG。如果OSD是PG的备OSD，OSD会向主提供其PG当前的版本号。如果OSD是PG的主OSD，岂会采集当前（和之前的）备OSD的PG版本号。如果主OSD缺少大部分的PG的最近状态，它会重新计算来自PG当前或之前的OSD中的最近PG修改的日志（或者如果需要的话，会完整地执行计算），以决定正确的（最近的）PG内容。主OSD随后向每个备OSD发送增量的日志更新（或者如果需要的话，发送完整的内容），这样所有方都会知道PG的内容应该是什么，即使这与它们本地存储的对象可能不匹配。仅在主OSD决定了正确PG的状态并将其分享给其他所有备OSD后，才允许对对象进行I/O。然后，OSD就会独立负责根据它们的对等节点计算丢失或过失的对象。如果OSD收到对过时或丢失的对象的请求，它会推迟处理，并将这个对象移到恢复队列的前端。 例如，假设osd1故障并被标记为down，那么osd2会接管pgA并作为其主OSD。如果osd1恢复，其会在启动时请求最近的映射，且一个监控者会将其标记为up。当osd2收到导致映射变化的更新时，它会意识到其不再是pgA的主OSD，并将pgA的版本号发送给osd1。osd1将会重新计算来自osd2的pgA的日志条目，告知osd2其内容是最新的，并随后当任何更新的对象在后台恢复完成时开始处理对应的请求。 因为故障恢复完全由OSD独立驱动，每个被故障OSD影响的PG会（很可能地）在不同的放置的OSD中并行地恢复。这种基于快速恢复机制（Fast Recovery Mechanism，FaRM）的方法，减少了恢复时间并提改进了整体数据安全性。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:7:5","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"5.6 使用EBOFS的对象存储 尽管很多的分布式文件系统使用像ext3的本地文件系统来管理下层存储[4, 12]，我们发现它们的接口和性能都不适合对象的负载[27]。已有的内核接口限制了我们了解对象的更新在何时被安全地提交到磁盘上的能力。同步的写入或者日志提供了我们需要的安全性，但是会带来严重的延迟和性能的损失。最重要的是，POSIX接口无法支持原子性数据和元数据（如属性）更新事务，这对维护RADOS的一致性是非常重要的。 因此，Ceph的每个OSD通过EBOFS管理其本地对象存储，EBOFS是一个基于区段B树（Extent and B-Tree）的文件系统。EBOFS的实现完全在用户空间中，且直接与原始块设备交互，这让我们能够定义我们自己的下层对象存储接口并更新语义，这将更新串行化（为了同步）与磁盘提交（为了安全性）分离开来。EBOSF支持原子事务（如对多个对象的写入和属性更新），且更新函数在内存缓存更新时返回，同时提供了异步的提交通知。 使用用户空间的方法，除了提供了更好的灵活性且更容易实现外，还避免了与Linux VFS和也缓存的笨重的交互，这二者是为不同的接口和负载设计的。大部分的内核文件系统在一定时间后懒式地将更新冲刷到磁盘，而EBOFS主动的调度磁盘写入，并在I/O操作等待中且后续的更新导致磁盘写入变得不必要时，EBOFS会取消它这次写入。这让我们的下层磁盘调度器有更长的I/O队列，且相应的调度效率会提高。用户空间调度器还可以更简单地对负载按优先级排序（例如，客户端I/O vs 恢复）或提供服务质量保证[36]。 EBOFS的设计核心是一个鲁棒性的、灵活地、完全集成了B树的服务，其被用作在磁盘上定位对象、管理块分配、和索引采集（PG）。块分配按照区间起点和长度的对管理的，而不采用块列表，以保持元数据紧凑。磁盘上的空闲块区间按照大小和位置分类，使EBOFS能够在磁盘上快速定位写入位置附近的空闲空间或相关数据，同时限制了长碎片的量。为了性能和简单起见，除了每个对象的块分配信息外，所有的元数据都被保存在内存中（即使对容量很大的文件系统来说，这些信息也非常小）。最后，EBOFS积极地使用写入时复制（copy-on-write）：除了超级块（superblock）更新外，数据总是被写入到未分配的磁盘区域。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:7:6","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"6. 性能与可伸缩性评估 我们通过一系列的微批量的benchmark来评估我们的原型以展示其性能、可靠性、和可伸缩性。在所有的测试中，客户端、OSD、和MSD都是用户进程，运行在双处理器、SCSI磁盘、并通过TCP通信的Linux集群上。通常，每个OSD或MDS运行在其自己的主机上，而在生成负载时，数十或数百个客户端可能共享同一个主机。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:8:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"6.1 数据性能 EBOFS提供了优秀的性能和安全性语义，同时，CRUSH生成的数据的均衡分布和副本与故障恢复的委托机制让整体的I/O性能随OSD集群的大小伸缩。 6.1.1 OSD吞吐量 图5 每个OSD的写入性能。水平线表示受物理磁盘影响的上限。副本对OSD吞吐量的影响很小，即使OSD的数量是固定的，n路复制会将总的有效小吞吐量削减n倍，因为复制的数据必须被写入到n个OSD中。图5 每个OSD的写入性能。水平线表示受物理磁盘影响的上限。副本对OSD吞吐量的影响很小，即使OSD的数量是固定的，n路复制会将总的有效小吞吐量削减n倍，因为复制的数据必须被写入到n个OSD中。 \" 图5 每个OSD的写入性能。水平线表示受物理磁盘影响的上限。副本对OSD吞吐量的影响很小，即使OSD的数量是固定的，n路复制会将总的有效小吞吐量削减n倍，因为复制的数据必须被写入到n个OSD中。 我们从测量14个OSD节点组成的集群的I/O性能开始。图5展示了在不同的写入大小（x轴）和副本数下每个OSD的吞吐量（y轴）。负载由20个额外节点的400个客户端生成。如图中的水平线所示，性能基本上受限于原始磁盘带宽（约58MB/s）。副本让磁盘I/O翻了两倍或三倍，当OSD数量固定时，减少了相关客户端的数据速率。 图6 EBOFS与通用文件系统的性能比较。尽管在我们的原型中，小的写操作会受粗粒度的锁影响，但是对于大于32KB的写入操作，EBOFS几乎会让磁盘饱和。因为EBOFS在大量增量地写入数据时，会把数据安置到很大的区间中，所以其有明显更好的读性能。图6 EBOFS与通用文件系统的性能比较。尽管在我们的原型中，少量的写操作会受粗粒度的锁影响，但是对于大于32KB的写入操作，EBOFS几乎会让磁盘饱和。因为EBOFS在大量增量地写入数据时，会把数据安置到很大的区间中，所以其有明显更好的读性能。 \" 图6 EBOFS与通用文件系统的性能比较。尽管在我们的原型中，小的写操作会受粗粒度的锁影响，但是对于大于32KB的写入操作，EBOFS几乎会让磁盘饱和。因为EBOFS在大量增量地写入数据时，会把数据安置到很大的区间中，所以其有明显更好的读性能。 图6比较了EBOFS与通用文件系统（ext3、ReiserFS、XFS）在处理Ceph的负载时的性能。客户端同步地写入大文件，将其分条为16MB的多个对象，并随后读回数据。尽管少量的读取和写入的性能受EBOFS粗粒度的线程和锁的影响很大，但是当写入大小超过32KB时，EBOFS几乎可以使可用的磁盘带宽饱和，且其读取性能比其他文件系统有明显的优势，因为数据被安排到与其大小匹配的磁盘区间上，即使数据很大也是如此。性能通过一个新的文件系统测量.使用早期的EBOFS的实验表明，其产生的碎片比ext3少很多，但是我们还没在用了很久的文件系统上测量过当前实现的情况。无论如何，我们预期用了很久以后，EBOFS的性能也不会比其它的差。 6.1.2 写入延迟 图7 不同写入大小和副本数下写入延迟。对于少量的写入来说，超过两个的副本所带来的额外开销很小，因为副本的更新时并发发生的。对于大量的同步写入来说，传输时间占了大部分的延迟。对于超过128KB的写入，客户端通过请求排他锁和异步冲刷数据，掩盖了部分的延迟。图7 不同写入大小和副本数下写入延迟。对于少量的写入来说，超过两个的副本所带来的额外开销很小，因为副本的更新时并发发生的。对于大量的同步写入来说，传输时间占了大部分的延迟。对于超过128KB的写入，客户端通过请求排他锁和异步冲刷数据，掩盖了部分的延迟。 \" 图7 不同写入大小和副本数下写入延迟。对于少量的写入来说，超过两个的副本所带来的额外开销很小，因为副本的更新时并发发生的。对于大量的同步写入来说，传输时间占了大部分的延迟。对于超过128KB的写入，客户端通过请求排他锁和异步冲刷数据，掩盖了部分的延迟。 图7展示了单次同步写入不同大小的数据（x轴）和副本数时的延迟（y轴）。因为主OSD同时将更新传输给所有备OSD，对于少量的写入来说，超过两个的副本所带来的额外开销很小。对于大量的写入，传输的开销占了大部分的延迟；单副本1MB的写入（没在图中给出）花了13ms，而三副本的时间长了2.5倍（33ms）。Ceph的客户端在写入超过128KB的数据时通过请求排他锁和异步地将数据冲刷到磁盘上，掩盖了部分的延迟。或者，共享写入的应用程序可以选择使用O_LAZY。由于这样会放松一致性，客户端可以缓冲少量的写入并仅提交大量的写入，异步地写入到OSD中；应用程序能看到的延迟仅为因客户端填满了其缓存而等待数据冲刷到磁盘时的延迟。 6.1.3 数据分布与可伸缩性 图8 OSD的写入性能随OSD集群的大小线性增长，直到交换机在24个OSD时达到饱和为止。当有更多的PG时，OSD的利用率差异被降低，且CRUSH和哈希的性能提升。图8 OSD的写入性能随OSD集群的大小线性增长，直到交换机在24个OSD时达到饱和为止。当有更多的PG时，OSD的利用率差异被降低，且CRUSH和哈希的性能提升。 \" 图8 OSD的写入性能随OSD集群的大小线性增长，直到交换机在24个OSD时达到饱和为止。当有更多的PG时，OSD的利用率差异被降低，且CRUSH和哈希的性能提升。 Ceph的数据性能随OSD的数量近似线性地增长。CRUSH伪随机地分布数据，这样OSD的利用率可以通过二项分布或正态分布精确地建模——这是通过完全随机过程的期望得到的[29]。使用率的差异会随着组数量的增加而减少：对于每个OSD上有100个PG的情况，标准差为10%；对于1000个PG的情况，其标准差为3%。图8展示了使用CRUSH、简单哈希函数的集群中每个OSD的吞吐量随集群伸缩的变化，其线性的分条策略将数据分布在可用的OSD上的4096或32768个PG中。线性分条可以很好地平衡负载以增加吞吐量，这给对比提供了benchmark，但与简单的哈希函数一样，它不能处理设备故障或其他的OSD集群更改。因为通过CRUSH或哈希函数的数据放置是随机的，PG数量更少时吞吐量更低：更大的OSD利用率差异在多客户端相互纠缠的情况下导致请求队列长度各异。因为设备可能有很小的概率过载或过度使用，这回拖慢性能，因此CRUSH可以通过卸载集群映射中标记的特定的OSD来修正这种情况。不想哈希和线性的策略，CRUSH还能在集群扩展时减少数据迁移，同时维护数据均衡分布。CRUSH计算复杂度是$O( \\log n )$（对于由n个OSD组成的集群来说），在集群增长到有数百或数千个OSD时，也只需要几十毫秒。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:8:1","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"6.2 元数据性能 Ceph的MDS集群提供了有极好的可伸缩性的增强POSIX语义。我们通过没有任何数据I/O的部分负载来测量性能；在这些实验中，OSD仅存储元数据。 6.2.1 元数据更新延迟 我们首先考虑元数据更新相关的延迟（例如mknod或mkdir）。单个客户端创建一系列的文件和目录，MDS为了安全起见，必须同步地将其记录到OSD集群的日志中。我们考虑有一个无磁盘的MDS和一个有本地磁盘的OSD，该MDS的所有元数据存储在共享的OSD集群中，而该OSD为其日志的主OSD。**图9(a)**展示了两种情况下在不同的元数据副本数（x轴）（0表示没有日志）时与元数据更新相关的延迟（y轴）。日志条目首先会被写入主OSD中，然后被被分到所有额外的OSD中。对于由本地磁盘的情况，MDS到（本地的）主OSD的第一跳花费的时间最少，而两副本的延迟达到了最初的两倍，并近似与无磁盘模型中一份副本的情况。在两种情况下，超过两副本后额外的延迟很少，因为副本更新是并行的。 图9 使用本地磁盘的方式可以避免最初的网络往返，从而降低写入延迟。当使用readdirplus或放松了的一致性要求时，消除了MDS在readdir后的stat时的交互，让读取操作能从缓存中受益。图9 使用本地磁盘的方式可以避免最初的网络往返，从而降低写入延迟。当使用readdirplus或放松了的一致性要求时，消除了MDS在readdir后的stat时的交互，让读取操作能从缓存中受益。 \" 图9 使用本地磁盘的方式可以避免最初的网络往返，从而降低写入延迟。当使用readdirplus或放松了的一致性要求时，消除了MDS在readdir后的stat时的交互，让读取操作能从缓存中受益。 6.2.2 元数据读取延迟 元数据读取（如readdir、stat、open）的行为更加复杂。**图9(b)**展示了客户端对10000个嵌套的目录通过readdir遍历每个目录并对每个文件使用stat时的总时间（y轴）。MDS的热缓存减少了readdior的次数。后续的stat不受影响，因为inode的内容嵌入到了目录中，使仅需一次OSD访问就可以将完整的目录内容拉取到MDS的缓存中。通常，在更大的目录中，stat的总次数会占大部分延迟。后续的MDS交互可通过使用readdirplus来消除，其显式地将stat和readdir的结果捆绑到一次操作中；也可以通过通过放松POSIX请求来使紧跟在readdir后的stat可以从客户端的缓存中获取（默认情况）。 6.2.3 元数据伸缩 图10 在不同负载和集群大小下的每个MDS的吞吐量。随着集群增长到128个节点，在大部分的负载下，性能降低不超过完全线性（水平）扩展的50%，这使该系统比已有的系统有大幅的性能提升。图10 在不同负载和集群大小下的每个MDS的吞吐量。随着集群增长到128个节点，在大部分的负载下，性能降低不超过完全线性（水平）扩展的50%，这使该系统比已有的系统有大幅的性能提升。 \" 图10 在不同负载和集群大小下的每个MDS的吞吐量。随着集群增长到128个节点，在大部分的负载下，性能降低不超过完全线性（水平）扩展的50%，这使该系统比已有的系统有大幅的性能提升。 我们使用Lawrence Livermore National Laboratory（LLNL）的alc Linux集群中由430个节点组成的一个分区来评估元数据的可伸缩性。图10展示了每台MDS的吞吐量（y轴）作为MDS集群大小（x轴）的函数，其中水平线表示完全线性伸缩。在makedirs负载中，每个客户端差UN构建一个4层嵌套的目录树，每个目录中有10个文件和1个子目录。在较小的集群中，每台MDS的吞吐量为2000ops/sec， 随着集群扩展到128个MDS时每个MDS的吞吐量大概为1000ops/sec（每台效率下降了50%，总吞吐量为100000ops/sec）。在makefiles的负载中，每个客户端在同一个目录下创建数千个文件。当检测到大量的写入时，Ceph会哈希化共享目录并放松目录的修改时间的连贯性，以将负载分摊到所有MDS节点上。openshared的负载演示了每个客户端反复打开关闭10个共享文件的共享读取。在openssh负载中，每个客户端会在私有目录中重放获取到的一次编译过程中对文件系统的跟踪。一种情况下使用共享的/lib目录，而另一种情况使用/usr/include，第二种情况使用的目录会被大量读取。openshared和openssh+include的负载的共享读取量是最大的，其伸缩表现最差，我们认为这是因客户端对备OSD的选择较差而导致的。openssh+lib比简单的分别makedir的伸缩性更好，因为其有相对少的元数据修改和相对少的共享。尽管我们认为，对于更大的MDS集群来说，在消息传递层的网络和线程的争用会进一步降低性能，但是我们访问大型集群的时间有限，无法进行详细的调查。 图11展示了分别在4、16、64个节点组成的MDS集群中的makedir负载下，延迟（y轴）和每台MDS吞吐量（x轴）的关系图。较大的集群的负载分布不完美，导致平均每台MDS的吞吐量更低（当然，其总吞吐量更高）且延迟稍高。 图11 不同集群大小下的平均延迟与每台MDS的吞吐量关系（makedir负载下）。图11 不同集群大小下的平均延迟与每台MDS的吞吐量关系（makedir负载下）。 \" 图11 不同集群大小下的平均延迟与每台MDS的吞吐量关系（makedir负载下）。 尽管这不是完美的线型伸缩，但是128个运行着我们的原型系统的MDS节点组成的集群还是能够提供超过每秒25万次的元数据操作（128台节点的每台节点2000ops/sec）。因为元数据的事务与数据I/O独立，且元数据大小与文件大小无关，所以该性能可能在数百PB或更大的存储中也是如此，这取决于平均文件大小。例如，科学计算应用程序在LLNL的Bluegene/L上的检查点创建可能使用了64000个双处理器的结点，每个节点在相同的目录下写入一个独立的文件（就像makefile的负载那样）。当前的存储系统的元数据操作在6000ops/sec时达到峰值，且完成每个检查点的创建需要花几分钟的时间，而Ceph的MDS集群可以在2秒内完成。如果每个文件仅为10MB（按照HPC的标准来说非常小）且OSD能支撑50MB/sec，这样的集群在使25000台OSD（与50000分副本）饱和的情况下能够以1.25TB/sec的速率写入。250GB的OSD能够构建出超过6PB的系统。更重要的是，Ceph的动态元数据分布让（任何大小的）MDS集群基于当前的负载重新分配资源，即使之前所有客户端都访问被分配到一台MDS上的元数据时也可以，这让Ceph的策略比任何静态分区策略都更具有通用性和适应性。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:8:2","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"7. 开发经历 我们惊喜地发现，用分布函数来替代文件分配元数据大大简化了我们的设计。尽管这对函数本身的需求更高，但是一旦我们意识到我们的需求是什么，CRUSH就能够体提供必要的可伸缩性、灵活性、和可靠性。这极大地简化了我们的元数据负载，同时为客户端和OSD提供了与数据分布相关的完整、独立的信息。后者能够让我们把数据复制、迁移、故障检测、和恢复的责任委托给OSD，将这些机制分摊开来可以有效地利用整体的CPU和内存。RADOS为一些列能用在我们OSD模型中的进一步的增强打开了一扇大门，例如位错误检测（如GFS中的[7]）和基于负载的动态数据复制（类似AutoRAID[34]）。 尽管使用现有的内和文件系统作为本地对象存储很诱人（正如许多其他的系统做的那样[4, 7, 9]），但是我们很早就认识到了，为对象负载量身定制的文件系统能够提供更好的性能[27]。我们没有预料到的是现有的文件系统与我们需求的差异，这在开发RADOS的副本和可靠性机制时变得非常明显。运行在空户空间中的EBOFS的开发速度惊人的快，其提供了非常让人满意的性能并给出了完全符合我们需求的接口。 开发Ceph中我们学到的最重要的知识之一是，MDS负载均衡器对整个系统的可伸缩性的重要性，和选择在什么时候将哪些元数据迁移到哪儿的复杂性。虽然原侧上我们的设计和目标看起来相当简单，但实际上将不断增长的负载分布到超过100个MDS上还有很多额外的细节。最值得注意的是，MDS的性能受很多方面性能的限制，包括CPU、内存（即缓存效率）、网络与I/O的限制，在任一时间性能都可能受任一方面的性能限制。另外，定量地在总吞吐量和公平性之间做出平衡是很难的；在一些情况下，不均衡的元数据分布也可以提高整体吞吐量[30]。 客户端接口的实现带来了比预期更大的挑战。尽管使用FUSE的方式通过避免涉及到内核极大地简化了实现，但是这样也引入了一系列其自己的特性。DIRECT_IO绕过了内核的页缓存，但是不支持mmap，这迫使我们采用修改FUSE以使空白的页失效的解决方案。FUSE坚持执行其自己的安全性检查，这导致即使是简单的应用程序也会大量调用getattr（stat）。最后，内核和用户空间之间的基于页的I/O限制了整体I/O速率。尽管直接链接到客户端避免了FUSE的问题，用户空间过量的系统调用又引入了一系列新问题（其中大部分问题我们还没有完全研究），是客户端必可避免地需要有内核中的模块。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:9:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"8. 相关工作 高性能可伸缩的文件系统一直以来都是HPC社区的目标之一，HPC往往会给文件系统带来沉重的负载[18, 27]。尽管许多文件系统试图满足这一需求，但是它们都没有提供与Ceph等级相同的可伸缩性。像OceanStore[11]和Farsite[1]这样的大规模系统是为提供PB级的高可靠存储、并使数千个客户端能同时访问不同的数千个文件设计的，但是由于其子系统的瓶颈（如名称查找），它们无法在上万个协作的客户端访问少量文件时提供较高的性能。相反，像Vesta[6]、Galley[17]、PVFS[12]、和Swift[5]这样的并行文件与存储系统为跨多个磁盘分条的数据提供了广泛的支持，但是缺少对可伸缩的元数据访问或高可用的鲁棒性数据分布提供健壮的支持。例如，Vesta允许应用程序将数据放在磁盘上，且允许每个磁盘独立访问文件数据而不需要参考共享的元数据。然而，像其他的并行文件系统一样，Vesta没有提供对元数据查找的可伸缩的支持。因此，这些文件系统在访问大量小文件或需要很多元数据操作时，往往性能很差。它们通常还会遇到块分配的问题：块或中心化分配，或通过基于锁的机制分配，这使它们在面对来组数千个客户端到数千个磁盘的写请求时，无法很好地伸缩。GPFS[24]和StorageTank[16]将元数据管理和数据管理部分解耦，但是它们受基于其使用的基于块的磁盘和元数据分布结构限制。 像LegionFS[33]这样的基于网格的文件系统，被设计用于广域访问，且没有为高性能的本地文件系统优化。类似地，GFS[7]是为非常大的文件和有大量的读取和文件追加的的负载优化。像Sorrento[26]一样，其目标是不使用POSIX语义的很窄的一类应用程序。 最近，许多文件系统和平台（包括Federated Array of Bricks（FAB）[23]、pNFS[9]）都是围绕网络附加存储[8]设计的。Lustre[4]、the Panasas File System[32]、zFS[21]、Sorrento、和Kybos，都基于一种基于对象的存储范式[3]，且最接近Ceph。然而，这些系统都没有将Ceph提供的可伸缩和可适应的元数据管理及可靠性与容错合并在一起。特别是Lustre和Panasas，它们没有将责任委托给OSD，这限制了它们实现有效的分布式元数据管理，因此限制了它们的可伸缩性和性能。此外，除了使用了一致性哈希[10]的Sorrento，所有的这些系统都使用显式的映射来指定对象存储在哪儿，限制了在新存储部署时的重均衡支持。这会导致负载不对称与资源利用率低，而Sorrento的哈希化的分布缺少CRUSH中对高效数据迁移、设备权重、和故障域的支持。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:10:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"9. 展望 Ceph的一些核心元素目前还没有被实现，包括MDS故障恢复和一些POSIX调用。两种安全性相关的架构和协议的变体还在考虑中，目前都没有实现[13, 19]。我们还计划调研通过客户端回调进行元数据的命名空间到inode的转换的实用性。对于文件系统的静态区域，这可以在为了去读打开文件时不需要与MDS交互。一些其他的MDS增强还在计划中，包括对目录结构的任意子树创建快照的能力[28]。 尽管当一个目录或文件出现瞬时的大量访问时，Ceph可以动态地为元数据做副本，但目前还没为文件数据实现这种机制。我们计划让OSD能够基于负载动态地为单个对象调整副本等级，并将读取流量分摊到同一PG的多个OSD中。这会让少量数据有可伸缩的访问能力，并使用类似于D-SPTF[15]的机制为OSD提供细粒度的负载均衡。 最后，我们正在开发一种服务质量体系，以结合基于分类的流量优先级和基于OSD管理的预留的带宽和延迟保证。除了为对QoS有需求的应用程序提供支持外，这还会帮助均衡RADOS的副本与一般负载下的恢复操作。计划中还有很多对EBOFS的增强，包括改进的分配逻辑、数据清洗、和能提高数据安全性的校验和或其他位错误检测机制。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:11:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"10. 结论 Ceph通过独有的设计，解决了存储系统的三个重要挑战：可伸缩性、性能、和可靠性。通过去除几乎所有现有系统中有的像分配列表之类的设计，我们最大限度地将数据与元数据管理分离，让二者可以独立伸缩。这种分离依赖于CRUSH，CRUSh是一个生成伪随机分布的数据分布函数，其让客户端计算对象的位置而不是查找对象的位置。CRUSH强制数据的副本在故障域间分布以提高数据安全性，同时有效地应对了大型存储集群本身的动态特性，如其设备故障、扩展、和集群重组是常态。 RADOS利用了智能OSD来管理数据副本、故障检测与恢复、下层的磁盘分配、调度、和数据迁移，且不涉及任何中央服务器。虽然对象可以被视为文件并存储在通用的文件系统上，但是EBOFS通过解决Ceph中特定的负载和接口需求，提供了更合适的语义和更好的性能。 最后，Ceph的元数据管理架构解决了高度可伸缩存储中最棘手的问题之一——如何高效地提供一个遵循POSIX的、且性能可以随元数据服务器的数量伸缩的统一的目录层次结构。Ceph的动态子树分区是一种独特的可伸缩的方法，其既有高效率，又提供了适应不同多变的负载的能力。 Ceph基于LGPL协议，可在https://ceph.sourceforge.net/中访问。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:12:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"致谢 这项工作是在美国能源部的支持下，由加州大学劳伦斯利弗莫尔国家实验室根据合同W-7405-Eng-48进行的。这项研究的部分资金由Lawrence Livermore、Los Alamos、和Sandia National Laboratories提供。我们要感谢Bill Loewe、Tyce McLarty、Terry Heidelberg和LLNL的其他所有人，他们向我们讲述了他们的存储试验和困难，并帮我们取得了两天的alc专用访问时间。我们还要感谢IBM提供的32节点OSD集群来做性能测试，以及美国国家科学基金会（National Science Foundation）为交换机升级买单。我们的领导Chandu Thekkath、匿名的审稿人和Theodore Wong都提供了宝贵的反馈，我们也要感谢存储系统研究中心的学生、教师和赞助者的工作与支持。 ","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:13:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"参考文献 [1] A. Adya, W. J. Bolosky, M. Castro, R. Chaiken, G. Cermak, J. R. Douceur, J. Howell, J. R. Lorch, M. Theimer, and R. Wattenhofer. FARSITE: Federated, available, and reliable storage for an incompletely trusted environment. In Proceedings of the 5th Symposium on Operating Systems Design and Implementation (OSDI), Boston, MA, Dec. 2002. USENIX. [2] P. A. Alsberg and J. D. Day. A principle for resilient sharing of distributed resources. In Proceedings of the 2nd International Conference on Software Engineering, pages 562-570. IEEE Computer Society Press, 1976. [3] A. Azagury, V. Dreizin, M. Factor, E. Henis, D. Naor, N. Rinetzky, O. Rodeh, J. Satran, A. Tavory, and L. Yerushalmi. Towards an object store. In Proceedings of the 20th IEEE / 11th NASA Goddard Conference on Mass Storage Systems and Technologies, pages 165-176, Apr. 2003. [4] P. J. Braam. The Lustre storage architecture. https://www.lustre.org/documentation.html, Cluster File Systems, Inc., Aug. 2004. [5] L.-F. Cabrera and D. D. E. Long. Swift: Using distributed disk striping to provide high I/O data rates. Computing Systems, 4(4):405-436, 1991. [6] P. F. Corbett and D. G. Feitelson. The Vesta parallel file system. ACM Transactions on Computer Systems, 14(3):225-264, 1996. [7] S. Ghemawat, H. Gobioff, and S.-T. Leung. The Google file system. In Proceedings of the 19th ACM Symposium on Operating Systems Principles (SOSP ‘03), Bolton Landing, NY, Oct. 2003. ACM. [8] G. A. Gibson, D. F. Nagle, K. Amiri, J. Butler, F. W. Chang, H. Gobioff, C. Hardin, E. Riedel, D. Rochberg, and J. Zelenka. A cost-effective, high-bandwidth storage architecture. In Proceedings of the 8th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), pages 92-103, San Jose, CA, Oct. 1998. [9] D. Hildebrand and P. Honeyman. Exporting storage systems in a scalable manner with pNFS. Technical Report CITI-05-1, CITI, University of Michigan, Feb. 2005. [10] D. Karger, E. Lehman, T. Leighton, M. Levine, D. Lewin, and R. Panigrahy. Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the World Wide Web. In ACM Symposium on Theory of Computing, pages 654-663, May 1997. [11] J. Kubiatowicz, D. Bindel, Y. Chen, P. Eaton, D. Geels, R. Gummadi, S. Rhea, H. Weatherspoon, W. Weimer, C. Wells, and B. Zhao. OceanStore: An architecture for global-scale persistent storage. In Proceedings of the 9th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), Cambridge, MA, Nov. 2000. ACM. [12] R. Latham, N. Miller, R. Ross, and P. Carns. A next-generation parallel file system for Linux clusters. LinuxWorld, pages 56-59, Jan. 2004. [13] A. Leung and E. L. Miller. Scalable security for large, high performance storage systems. In Proceedings of the 2006 ACM Workshop on Storage Security and Survivability. ACM, Oct. 2006. [14] B. Liskov, S. Ghemawat, R. Gruber, P. Johnson, L. Shrira, and M. Williams. Replication in the Harp file system. In Proceedings of the 13th ACM Symposium on Operating Systems Principles (SOSP ‘91), pages 226-238. ACM, 1991. [15] C. R. Lumb, G. R. Ganger, and R. Golding. D-SPTF: Decentralized request distribution in brick-based storage systems. In Proceedings of the 11th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), pages 37-47, Boston, MA, 2004. [16] J. Menon, D. A. Pease, R. Rees, L. Duyanovich, and B. Hillsberg. IBM Storage Tank-a heterogeneous scalable SAN file system. IBM Systems Journal, 42(2):250-267, 2003. [17] N. Nieuwejaar and D. Kotz. The Galley parallel file system. In Proceedings of 10th ACM International Conference on Supercomputing, pages 374-381, Philadelphia, PA, 1996. ACM Press. [18] N. Nieuwejaar, D. Kotz, A. Purakayastha, C. S. Ellis, and M. Best. File-access characteristics of parallel scientific workloads. IEEE Transactions on Parallel and Distributed Systems, 7(10):1075-10","date":"2020-09-14","objectID":"/posts/paper-reading/weil-osde06/:14:0","tags":["Ceph","Translation"],"title":"《Ceph: A Scalable, High-Performance Distributed File System》论文翻译","uri":"/posts/paper-reading/weil-osde06/"},{"categories":["Paper Reading"],"content":"本篇文章是对论文Web caching with consistent hashing的原创翻译，转载请严格遵守CC BY-NC-SA协议。 注意 这是一篇1999年的论文，其中的一些问题在当今条件下已经不是问题。但是还是可以通过本文了解当时的困境与解决方案。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:0:0","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"作者 David Karger 1, Alex Sherman Ł,1, Andy Berkheimer, Bill Bogstad, Rizwan Dhanidina, Ken Iwamoto, Brian Kim, Luke Matkins, Yoav Yerushalmi MIT Laboratory for Computer Science, 545 Technology Square, Room 321, Cambridge, MA 02139, USA ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:1:0","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"摘要 对万维网性能测量的关键是内容被提供给用户的速度。随着Web流量的增加，用户面对着日益增加的时延和数据分发故障。Web缓存是已被探索果的用来改进性能的关键策略之一。 许多缓存系统的重要问题是如何在给定时间内决定在哪儿缓存什么数据。解决方案包括组播（multicast）查询和目录策略。 在本文中，我们提供了一个基于一致性哈希的新的Web缓存策略。一致性哈希可以替代组播和目录策略，且在负载均衡和容错方面有许多其他优势。一致性哈希的性能在过去的工作中已经通过理论分析过。在本文中，我们描述了一个基于一致性哈希的系统实现和实验。实验支持了一致性哈希能够提供性能改进的论点。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:2:0","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"1. 引言 随着万维网成为信息分发的主要媒介，能够高效可靠地交付Web流量的机制成为了需求。然而，党建的流量交付方法容易出现不可预测的实验和频繁的故障。这些延时和故障的两个主要原因是网络拥堵和服务器超载。数据在拥堵的网络中传输缓慢。过载的服务器（面对超过其资源所能支持的并发请求）要么会拒接服务，要么会以很慢的速度提供服务。因为网络和服务器的基础设施没有跟上互联网的巨大增长，所以网络拥堵和服务器过载是很常见的。 服务器和网络可能在没有任何事先通知的情况下过载。例如，在晚间新闻中被称为“当天的酷网站”的站点，可能不得不处理第二天增长了一万倍的流量。因此，提前计划能带来的好处很有限，处理负载的最佳策略是适应不断变化的环境。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:3:0","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"1.1 Web缓存 缓存被用作提高Internet的数据交付性能和可靠性。即使在原始服务器已经过载或到该服务器的网络拥堵的情况下，附近的缓存仍可以快速地为（缓存中的）页面提供服务。这一好处给仅关心自己的服务的用户带来的利用缓存的理由，且如果缓存能被广泛应用，那么还会带来另一个好处：如果请求能够被附近的缓存截获，那么去往源服务器的流量就会减少，减少服务器上的网络流量会为所有用户带来好处。 通过一台单独的共享缓存的机器为一组用户提供服务的这种最简单的做法有很多缺点。如果缓存机器故障，那么所有用户都不能连接Web。即使缓存机器正常运行，单台缓存机器能提供服务的用户数还是受限，且在一段高强度的使用后可能成为瓶颈。最后，单个缓存可以达到的命中率主要受两个因素限制。第一，因为可用的存储总量有限，当请求重复访问因缺少空间而被驱逐的对象时，会造成“假失配（false misses）”现象。第二，缓存能提供服务的用户数的限制与缓存想尽可能地聚合用户请求的目的相驳：通常，被聚合到一起的用户请求越多，每个用户请求已被其他用户请求过的对象时命中率就会越高。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:3:1","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"1.2 相关工作 为了达到容错、可伸缩并聚合大量请求（以提高命中率），一些团队[2, 4, 5, 7]提出了一些使用*协作式缓存（copperating caches）*的系统。这些系统都共享了确定的通用数据。每个client选取系统中的一个主（primary）缓存。来自该client的请求会通往主缓存。如果主缓存失配，请求会试图在其他协作缓存中定位被请求的资源，而不是直接访问内容服务器。如果成功，资源将会从（可能更近的）协作缓存中，而不是从较慢的内容服务器中取出。因此，其他协作缓存被用作“二级缓存”来减少主缓存的失配开销。 这两种系统的区别在于，当主缓存失配时的数据定位方式。一下策略会使用组播[7]或UDP广播[2]将请求广播到其他缓存中。这样做，除了查询的广播消耗了额外的带宽外，如果数据失配，主缓存在联系内容服务器之前必须等待所有协作缓存报告其适配，这会降低二级缓存失配时的性能。其他的策略使用了目录，这些策略或者是集中式的[5]或者需要反复广播以支持本地查询[4]。目录的查询或传输还是需要消耗带宽，且集中式的目录会成为新的系统故障点[5]。 这些系统的另一个问题是缓存中的数据冗余。任何一个缓存都可能收到对任一份数据的查询，这会导致缓存中保存了一份副本。在二级缓存命中时，网络带宽和时间被浪费在了将数据拷贝到另一个缓存上。更糟糕的是，这些拷贝会驱逐被请求的其他网页，这样削减了缓存命中数。如果协作缓存都“靠近”彼此，用户可能希望通过将每个对象仅保存在一个或少数几个机器上来得到一个（假失配更少的）更大的缓存[9]。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:3:2","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"1.3 我们的工作 本文中，我们提出了一种消除了所有缓存间通信，但允许整个系统中的缓存表现得像一整个缓存一样的方法。Cache Resolver是我们开发的分布式Web缓存系统，其通过让client自己决定从哪个缓存请求数据的方式，消除了失配时缓存建的通信。在失配时，用户的浏览器（客户端）会直接联系应该包含被请求的资源的缓存，而不是联系主缓存来定位被请求的数据应该在其他哪个缓存=上。浏览器通过一个能将资源（或URL）映射到一个动态变化的可用缓存的集合的哈希函数，来做出决定。 哈希方法比广播和基于目录的策略提供了几个好处。一台机器可以在本地计算哪个缓存应该包含给定的对象。使用单播就足以获得对象或确定它没被缓存，这与其他方案相比较少了网络使用。它还能够比需要等待所有缓存响应的组播策略更快地发现失配。它与基于目录的策略相比，减少了维护和查询的额外开销。它还不会使系统产生新的故障点。事实上，我们的策略展示了很强的可靠性，我们将在后文中讨论。 虽然以上的好处就似乎已经足以让让用户选择基于哈希的解决方案，但我们还是探索了另一个方面：它让我们将页面（资源）定位任务推给了独立的client端。基于哈希的策略可以很容易地在浏览器级别下实现，而基于目录的定位策略因开销过大而不能在每个浏览器中实现。因此，过去的策略会假设每个client总是联系一个固定的主缓存，当主缓存失配时，它会联系其它缓存来看它们是否有请求的页面（资源）。而我们让浏览器直接决定联系哪个缓存。这移除了对中间缓存的需要，以改进响应时间。另外，因为所有client在访问同一个给定的页面的时候会联系相同的缓存，因此，无论有多少个一起协作的缓存，我们的缓存系统在查询每个页面时只会经历一次失配，而不是每个页面会在每个缓存中都会（在主缓存中）失配一次。因为我们避免页面的冗余副本，为其他页面留出了更多在缓存中的空间，所以我们可以进一步减小失配率。 虽然基于哈希的策略有很多吸引人的性质，但是为了正确地实现它，必须考虑一些重要的问题。一篇理论方面的论文建立了一个被称为一致性哈希的工具来解决这些问题中的一些[6]。这是我们当前实现的工作的基础。 在Internet草案[3]中，也出现了相似的*缓存数组路由协议（Cache Arrayh Routing Protocal，CARP）*提议。Microsoft Proxu Cache[8]中使用了CARP。CARP从直觉上与我们的方法有很多相同之处，尽管它还没被从理论上证明。我们的提议与CARP的重要的不同之处在于我们的哈希算法是如何实现的。目前的浏览器不具备支持CARP这样额方案的所有功能。CARP将所有一致性哈希的的责任交给了浏览器，这样会有很多缺点，我们会在后文中讨论。相反，我们通过不寻常（但是正确）的方式使用DNS来为浏览器对哈希函数的使用提供支持。通过对浏览器进行修改，一致性哈希可以完全在浏览器中实现，无需依赖DNS。然而，从长远看来，通过DNS方法提供的一些好处可能会使其成为正确的选择。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:3:3","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"1.4 论文概览 在本文的第二章中，我们详细地描述了一致性哈希。在第三章中，我们描述了我们使用一致性哈希算法的Web缓存系统的实现，我们将我们的系统与其他Web缓存系统在第四章中进行了比较。在第五章中，我们提到了我们的缓存系统在其他方面的好处，如容错和负载均衡。在第六章中我们进行了总结。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:3:4","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"2. 一致性哈希 我们的系统基于一致性哈希，它是一篇以前的理论性的论文创建的策略[6]。这里，我们将介绍一致性哈希并描述其简单的实现方式。在概述其理论证明后，我们通过实验展示了在实际场景中它工作得很好。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:4:0","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"2.1 需求 我们的系统的目标是让任何一个client都能通过本地计算来将URL映射到包含它的缓存上。哈希是为实现这一目的被广泛使用的工具。例如，给定一个编号为0，…，22的缓存的23个缓存的集合，我们可能使用$h[u]=7u+4mod23$来将URL $u$映射到缓存$h$（我们可以将URL看做表示一个大数字的位字符串）。通常，根据对哈希函数的直觉，其倾向于把输入“随机地”分配的可能的位置上。这样的随机分布直观上看是均匀的，这意味着没有一个缓存负责处理不成比例的请求页面。哈希的这种负载均衡的特性是我们的应用程序高度需要的特性，因为负载不成比例的缓存将会成为整个系统的瓶颈。 不幸的是，标准的哈希在缓存系统中应用时会有一些缺点。或许最明显的缺点是，系统中的缓存机器数可能会随着时间上下波动。试想一下，当第24个缓存加入到刚才描述的系统中时会发生什么。一个很自然的变化是开始使用哈希函数$h’(u)=7u+4mod24$。不幸的是，在这种改变下，原本的每个URL会被映射到新的缓存上。这会使整个缓存系统中的URL都被刷新：如果系统查找一个在新位置上的URL，那么事实上它在旧位置上的缓存就没用了，会产生一次失配。信息通过Internet异步传播的这一事实会加剧这一问题。在任何一个时刻，不同的client对哪些缓存在线哪些缓存离线都有不同的信息。我们将一台给定的机器所知道的缓存集合成为它的视图（view），我们观察到，在任何时刻，系统中都有很多不同的视图。这样会有两个潜在的缺点。如果每个视图都会将URL映射到不同的缓存中，不就每个URL就会被在所有缓存中保存————这恰恰是我们想要避免的问题。另外，如果存在多种视图，那么会很难断言所有缓存都将收到相同的负载量————不同的视图可能将过多的负载导入东一个缓存中，即使每个单独的视图中的负载是均衡的。 因此，对我们的哈希函数来说，能够一致性地映射项目是很重要的：无论系统中是否存在多个变化中的视图，每个元素都应被映射到少数的机器上，这样，所有机器就会得到大致相同的负载。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:4:1","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"2.2 一致性哈希 一个能简单实现的一致性哈希函数[6]满足了上一节中描述的需求。首先，选择某个能将字符串映射到范围$[0,…,M]$中的标准的基（base）哈希函数。再除以M，其可被看做一个映射到区间$[0,1]$（译注：此处指连续区间，此处的除以是数学上的除法而不是计算机的整除）的哈希函数，这一区间可被视为一个单位圆注2。同时，将系统中的每个缓存映射为单位圆上的一个点。现在，将URL分配给其在单位圆上顺时针移动遇到的第一个缓存，如图1所示。 注2：为了方便起见，我们称单位周长的圆为单位圆。而通常，单位圆指的是半径为1的圆。 图1 (i)URL和缓存都通过标准哈希函数映射为圆上的点。URL被分配给在圆顺时针方向最近的缓存。项1、2、3被映射到缓存A。项4、5被映射到缓存B。 (ii)当加入新缓存时，仅那些在圆上顺时针方向最接近该新缓存的URL被重新分配。这样，当我们加入新缓存C时，仅项1、2移动到了新缓存C。项不会在之前已存在的缓存间移动。图1 (i)URL和缓存都通过标准哈希函数映射为圆上的点。URL被分配给在圆顺时针方向最近的缓存。项1、2、3被映射到缓存A。项4、5被映射到缓存B。 (ii)当加入新缓存时，仅那些在圆上顺时针方向最接近该新缓存的URL被重新分配。这样，当我们加入新缓存C时，仅项1、2移动到了新缓存C。项不会在之前已存在的缓存间移动。 \" 图1 (i)URL和缓存都通过标准哈希函数映射为圆上的点。URL被分配给在圆顺时针方向最近的缓存。项1、2、3被映射到缓存A。项4、5被映射到缓存B。 (ii)当加入新缓存时，仅那些在圆上顺时针方向最接近该新缓存的URL被重新分配。这样，当我们加入新缓存C时，仅项1、2移动到了新缓存C。项不会在之前已存在的缓存间移动。 一致性哈希很容易实现，所有“缓存点”被存储在一个二叉树中，通过一次二叉树查询就可以（在哈希URL点后）找到“URL点”顺时针方向的后继节点。这可以在$O( \\log n)$时间内一致性哈希到$n$个缓存。另一种实现[6]将圆划分为等长的段，并根据段对缓存点“分组”，这样，不管有多少个缓存，都可以在常数时间内查找。值得注意的是，CARP[3, 8]中提出的方案查找时间与缓存数量是线性关系，因此在大量缓存的情况下可伸缩性小得多。 出于[6]中详细介绍的技术原因，为每个缓存点制作少量副本是非常重要的————即将每个缓存的几个副本映射到单位圆上不同的“随机”的点上。这会让URL到cache的分布更均匀。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:4:2","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"2.3 分析 现在，我们将解释为什么刚刚描述的一致性哈希策略有我们需要的性质。这是对形式论证[6]的直观总结。 我们的论证基于这样的直觉：基哈希函数把URL和缓存“随机”地映射为单位圆上的点。思考当我们向系统中添加一个新的缓存c时会发生什么。该缓存被映射到单位圆上，并“窃取”了某些其它缓存中的URL。被窃取的是哪些URL呢？是那些在圆上处于c附近的URL。根据我们的直觉，这些URL在圆上是随机的。因此，在圆上靠近c的可能很少，这意味着只有很少的URL被窃走。显然，那些没被新的缓存窃走的URL是不会移动的，我们因此可以推断出我们想要的第一个性质：当我们添加一个缓存的时候，只有少量的URL位置改变了。因此，系统中大部分的缓存项在系统被修改后仍可被命中。 相似的论点也适用于多视图问题。对于给定的缓存和项，仅当在某个视图中该缓存是离该项最近的缓存时，才会被选中。但如果缓存离该项很远，那么（根据缓存的位置是随机的这一直觉）很有可能在每个视图中，会有某个离这个项更近的缓存。这样就防止了该缓存被这一项选中。因此，仅那些靠近一个项的缓存才不得不保存它。而关于缓存位置随机的直觉告诉我们，对任何一个项，只有少数的缓存靠近它。换句话说，对任何一个项来说，即使系统中有很多缓存，也仅有少量几个缓存负责它。 这些直觉可表示为如下的形式化定理[6]。该定理涉及到一个良好的基哈希函数（用来将URL和缓存映射到单位圆上的哈希函数）。[6]中证明了按照一定原则构造的随机的通用哈希函数的表现良好。而在实践中，能够很好混合数据的标准的哈希函数（如MD5）就基本足够了。 定理 2.1[6] ： 设系统中有$m$个机器和$c$个client，每个client中有一个由任意半数缓存机器组成的集合的视图。如果每个缓存机器有$ \\Omega ( \\log m ) $个副本（译注：这里的副本值得不是数据副本，而是在单位圆上有缓存的点的副本，如章节2.2结尾中提到的那样。），且所有副本和URL都通过良好的基哈希函数映射到了单位圆上，那么一下性质成立： 平衡（Balance）： 在任意一个视图中，URL在该视图中的缓存机器上的分布是均匀的。 负载（Load）： 在所有视图（的聚合）中，没有一台机器上有超过$O( \\log c) \\times 平均URL数量 $个URL。 散布（Spread）： 没有URL被存储在超过$O( \\log c)$个缓存上。 因此，在动态变化和不确定的Internet域中，一致性哈希是一种很好的哈希策略。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:4:3","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"2.4 实现结果 我们列出了一些能够证明一致性哈希有用的简要分析。第一，一致性哈希是相对较快的操作。在我们的实验环境中，我们建立了一个由100个缓存且每个缓存在单位圆上有1000份副本的缓存视图。我们在奔腾II 266MHz芯片上对一致性哈希的每一个动态步骤进行计时（动态步骤包括URL字符串求值、基哈希函数求值、查找二叉树）。平均每次对哈希函数的调用需要$20 \\mu s$。这大约是通过10Mbps的以太网从本地缓存传输20kB文件所需总时间的0.1%。如果我们在底层缓存视图的表示中使用桶数组（bucket array）而不是当前使用的常规的二叉树，那么这$20 \\mu s$的值还能被再次显著减小。 我们还测量了一些能显示一致性哈希能在缓存间实现良好均衡的指标，正如其低负载的性质保证的那样。我们使用了来自theory.lcs.mit.edu的Web服务器的一周的日志。在此期间，总计有26804个不同的URL请求。我们在不同数量的缓存上通过我们的哈希函数计算这些不同的URL，来看一致性哈希在缓存间能把文件分布得多好。 从以上的数据可以看出，每一条数据的标准差都很小，平仅在3%左右。当数据集变大时，这一数字还会变得更好。 在第二个实验中，我们使用多个视图将1500个网页名映射到缓存中，其设计到80台机器，其中5台机器在每个视图中的上下线情况都不同。此时，$(项, 缓存)$对的数量上升到了1877个，与基数相比仅增加了25%。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:4:4","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"3. 我们的系统 译者注：该系统是在当时技术受限的情况下设计的。 正如上文所述，我们向实现的系统似乎很简单：几个标准、非交互式的Web缓存与浏览器中的一些哈希逻辑相结合。但是当我们开始实现我们的系统时，我们很快就发现当前的浏览器还不够灵活，无法独立支持一致性哈希。因此，为了构建一个兼容当前浏览器的系统，我们广泛使用了域名系统（DNS）以支持一致性哈希。我们的Web缓存系统————Cache Resolver，由3个主要组件组成：用来存储内容的实际缓存机器、直接向虚拟缓存请求的用户浏览器、使用一致性哈希来将虚拟缓存转换为特定的缓存机器的物理地址的域名服务器（也成为决议器）。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:5:0","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"3.1 缓存 我们安装了一个发行版的Squid代理缓存包。如果其中存储了有效的数据副本，那么代理缓存会用该数据进行应答。否则，它将从原始Web服务器拉取数据并存储一个副本。Squid使用了LRU替代策略。为了实现我们稍后将讨论的负载均衡和故障转移，我们会在与缓存在同一个物理机上运行一个用来监控Squid进程的额外的软件。当该软件被我们系统中的其他单元查询时，它会回复其状态（存活或死亡）和用字节传输速率来表示的负载，这个负载是缓存在过去30秒的时间内为Web查询提供服务的负载。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:5:1","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"3.2 到缓存的映射 为了实现李璐蓝旗的一致性哈希，我们最初希望利用在大多数常用的浏览器（Netscape 2.0x或更高版本 和 IE3.01或更高版本）中出现的autoconfiguration函数。用户可以用JavaScript编写一个函数，在每次请求时这个函数会被调用，它会基于被请求的URL字符串来选择一个代理缓存列表以进一步联系。用户按照列表的顺序联系代理缓存，知道其联系到了响应所需数据的代理缓存。 不幸的是，autoconfiguration过于受限而无法支持一致性哈希。最根本的问题是，autoconfiguration脚本只被手动下载一次。一旦代理缓存的集合改变，那么映射函数可能会变得不正确。 为了解决这一问题，我们决定使用DNS。我们可以建立我们自己的DNS服务器，并修改它们以支持一致性哈希。这个一致性哈希会在域名解析时传给浏览器。更准确的说，我们编写了一个autoconfigure脚本，它会将输入的URL通过标准哈希映射到1000个命名的虚拟缓存的区间中。接下来我们通过DNS将这1000个缓存名通过一致性哈希映射到真实的缓存IP地址上。 在我们的测试中，我们发现某些操作系统下的某些版本的浏览器即使在其对虚拟名解析过期后，也不会重新调用DNS。如果被这种方式解析到的缓存离线，那么浏览器就无法加载页面。为了改善这一问题，我们的脚本实际上反悔了一个命名缓存列表（其中有5个命名缓存），而不是只有一个，以弥补浏览器“故障”的问题。通过返回一个列表，我们可以让这些故障的浏览器有更高的可能性以解析到工作中的命名物理缓存。列表中的最后一个值是“DIRECT”，浏览器会知道此事其他命名缓存都失效了，直接连接内容服务器。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:5:2","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"3.3 DNS服务器 我们的DNS系统的主要功能是将用户的JavaScript函数生成的虚拟命名解析为缓存的真实物理IP地址。我们使用了许多DNS服务器，每台都运行着未修改的实现了DNS协议的BIND 8.0发行版。BIND会从一个记录文件中读取虚拟命名到IP地址的映射，该文件被另一个名为“dnshelper”的进程动态更新。dnshelper监视缓存机和，并运行一致性哈希将所有的1000个虚拟命名（见章节3.2）映射到仅存活的缓存机器的区间中。如果可用的缓存集合改变，dnshelper会通过信号通知BIND重新加载包含了新的映射的记录文件。 大量的虚拟缓存一位置每个缓存只接受整个系统负载的一小部分。一致性哈希保证分配给每个缓存的虚拟命名的数量在各个缓存中均匀分布。这两个事实共同确保了网页的负载会均匀地分布在缓存上。我们将在章节5.2中使用一致性哈希聊描述更高级的负载均衡策略。 3.3.1 关于DNS的讨论 我们对DNS的使用某种程度上违背了我们要讲定位功能放在浏览器中的计划。然而这样做可以增加几道防线。NDS是定位对象的标准工具，我们使用了它的能力而不是去费力实现我们自己的协议。注意，当请求任何页面时，通常一个无缓存的浏览器会执行DNS解析来找到网页服务器。我们只是简单地用一个标识符来替代实际的服务器。这种DNS解析通常会在附近的DNS解析器中完成，因此不会增加网页请求的额外延时。我们的系统依赖于DNS的正常功能，而本来任何的DNS故障都会导致用户访问中断，所以我们并没有真正为系统增加新的故障点。 关于DNS的第二个争论是，如果我们的系统能够证明它自己，那么可以取消请求前的查询。DNS仅用来创建到IP地址的1000个命名的集合的映射。只需对浏览器进行少量修改，就可以轻松地将这种映射存储在浏览器中。事实上，目前浏览器已经可以保留约10个DNS条目的缓存，我们需要做的仅是将这个缓存增加到1000个。由于一致性哈希的属性，这些条目甚至不需要特别去更新到最新版。因此，浏览器可以采用十分懒惰的方式更新其映射，如果其响应失败就丢掉这个缓存，并在其打开到一个缓存的连接时懒式地下载可用缓存的更新。 尽管我们能够去掉DNS，但是目前我们没有理由这样说。在我们的实验中，根本注意不到DNS解析对我们系统性能的影响。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:5:3","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"4. 测试 因为我们相信我们的设计可以保证比我们研究过的现有的Web缓存系统提供更好的性能，所以我们实现了我们自己的缓存系统。在本文中，我们将我们的Cache Resolver与两个类似的系统进行了比较：Harvest系统和在章节1.2中描述的CRISP。我们得到的挤过证实了我们的假设。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:6:0","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"4.1 测试配置 为了测试我们的系统，我们使用了由连接到一个100Mbps的交换机的7台机器组成的网络。四种三台机器运行Squid，即代理缓存程序。另一台机器被指定为Web服务器，其被放在10Mbps的连接上以让到Web服务器的数据传输开销更高。另一台机器运行BIND的副本，且为指定的域名服务器，其使用一致性哈希将1000个命名解析到3个代理缓存上。第6个机器是用来运行测试驱动的。最后一个机器或被用作CRISP的测试目录，或Harvest系统中的父级缓存的测试。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:6:1","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"4.2 测试驱动 我们使用Surge[1]作为测试驱动程序，Surge是一个由Boston University开发的Web负载生成工具。Surge的设计者对Web流量进行了研究，它们开发的生成器能够更合理地模拟对象请求的大小和频率的分布。在测试前，Surge会生成一个文件数据库并拷贝到Web服务器。我们生成了一个包含不同大小的1500个文件的数据库，其总大小为34Mb。Surge还会生成测试期间的请求序列，每个文件可能被超过一次请求。client的数量和每个client中运行的线程的数量是Surge的参数。之后，Surge会按照请求计划一起模拟Surge创建的client。每个请求表示一个嵌入了许多Web文件的对象，文件可能在不同对象间有重叠，且不同文件可能被请求不同够次数。Surge模拟的Web服务器负载的数学分布在[1]有详细的描述。 我们修改了Surge以在两个不同的模式下运行：Common模式和Cache Resolver模式。Common模式表示通用的缓存配置，一系列用户总是使用一个本地缓存，当缓存失配时，该缓存或者从其他换种中拉取数据，或者从主内容服务器中拉取数据。对于Common模式，我们在三个client上运行Surge，每个cliente金合器自己的代理缓存通信。（Surge中的一个client会运行许多个线程，以代表一系列用户。）Cache Resolver模式是为了测试我们的系统而设计的。在该模式下，每三个client执行一个类似用户使用的autoconfiguration函数的简单的哈希函数。该函数以被请求的URL作为输入，并返回一些列虚拟命名。Surge的client通过我们的DNS单元把命名解析为IP地址。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:6:2","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"4.3 结果 我们在不同的缓存代理配置下运行了对比Cache Resolver模式和Common模式的测试。接着，我们分析了代理缓存的日志来比较不同的测试中的失配率。我们期望看到Cache Resolver模式的失配率较低，其原因有二。首先，Cache Resolver模式为特定的缓存分配数据。另外，在这些测试中有缓存容量在9Mb到36Mb间的情况。因为数据库总大小为34MB，较小的缓存会有更高的失配率，因为可能在LRU中被强制挤出。在每个我们运行的测试中，缓存已经从之前额数据存储中清空。在每个测试中，三个缓存的容量相等。在不同的测试中，缓存的容量选取为9、12、18、24、30个36Mb。在最初的一系列测试中，我们没有配置让代理缓存与另一个代理缓存通信。在失配时，它们简单地从源服务器拉取数据。在这种基本配置下，图2显示了Common模式比Cache Resolver模式有更高的失配率。当缓存容量更小时，失配率的差异甚至更高，因为Common模式下的数据冗余带来了更严重的负面影响。 图2 Common模式 vs Cache Resolver模式。x轴表示实验中使用的所有缓存的大小（Mb），y轴在左图中表示失配率，在右图中表示平均延迟（ms）。图2 Common模式 vs Cache Resolver模式。x轴表示实验中使用的所有缓存的大小（Mb），y轴在左图中表示失配率，在右图中表示平均延迟（ms）。 \" 图2 Common模式 vs Cache Resolver模式。x轴表示实验中使用的所有缓存的大小（Mb），y轴在左图中表示失配率，在右图中表示平均延迟（ms）。 我们还使用我们的缓存测试了另外三个常用的缓存系统配置。我们测试了Sibling配置，即所有三个缓存都被配置为sibling，且在失配时使用组播协议来检查数据是否在其他缓存上。我们还测试了Hierarchy配置，即我们sibling按照Harvest方法的风格添加了另一个缓存作为父级缓存。最后，我们还测试了CRISP配置，即建立一个中央目录，它会被所有的缓存在失配时查询。对于每个系统，我们测量了主缓存的失配率（用户查询的第一个缓存的失配率）和系统失配率（整个缓存系统的失配率，即没有缓存持有所需数据的情况）。图3展示了三种配置下的主缓存失配率，其与图2中Common Cahce模式的失配率相似，且所有配置的系统失配率几乎都与Cache Resolver模式的失配率一样好。对于这三个系统，主缓存失配会导致额外的缓存间通信，包括检查哪个缓存持有所需数据及缓存间数据传输。另外，缓存间数据传输会导致数据冗余，这使缓存的磁盘和RAM中剩余空间变小，最重要的是RAM空间是为用户提供快速服务的空间。当用户通过哈希函数决定使用哪个缓存时，可以避免关键循环中的额外通信及带来的负面效果。 图3 三种额外配置的失配率与延迟。图3 三种额外配置的失配率与延迟。 \" 图3 三种额外配置的失配率与延迟。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:6:3","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"5. 扩展 本章中，我们将讨论对我们基本系统的扩展，其为用户提供了局部性、负载均衡和容错。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:6:4","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"5.1 局部性 无论哪种缓存策略，用户的延迟都很大程度上受缓存服务器的临近度影响。我们的系统确保用户总能被在他们物理上本地区域的缓存服务。我们的缓存按照地理区域划分，用户仅被在其区域内的缓存服务。我们将能够决定用户地理位置区域的信息放在了用户浏览器的JavaScript函数中。该JavaScript函数是被定制化的：当用户下载它的时候，用户可以选择一个区域。虚拟命名会按照以下格式被JavaScript函数生成：A456.ProxyCache3.com，其中456是URL的哈希，3表示地理区域变量。 随后我们把我们的DNS系统划分为两层结构。最上层DNS服务器解析将包含地理位置信息的命名部分，并将用户的DNS解析器定向到一组与用户地理区域相关的底层DNS服务器。底层DNS服务器被按照缓存的指定地理位置区域放置，它们将虚拟命名解析为这些缓存的IP地址。 例如，我们解析A456.ProxyCache.com这样的命名的时候，用户DNS解析器首先将其定向到顶层DNS，其解析命名的第二部分ProxyCache3，随后该DNS将其定向到一组底层DNS服务器，它们会解析命名的第一部分A456。底层DNS根据本地服务器的地理位置解析，这些服务器对于用户来说也是在本地的。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:6:5","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"5.2 高级负载均衡：热门网页 我们在第二章中提到，因为每个局部区域的服务器被分配了相同比例的虚拟命名，所以我们实现了服务器之间的负载均衡。当大多数的项被以相同的频率请求时，这可能是一种很好的近似。然而，在万维网上，许多项是非常流行的，而其他项却不是。例如，像CNN头版这种流行的项，其被请求的频率比其他项多得多。这种“热门”的项会导致缓存了这些项的缓存服务器的负载很高。这样的服务器很容易被请求到过载，其要么挂掉要么只能以非常缓慢的速度为用户提供服务。 为了解决这种情况，在理想情况下，我们想知道那些资源是热门的并确保热门资源能被更多的缓存服务。这一决策需要由最终决定虚拟命名和IP地址的映射的底层DNS服务器进行。其解决方案是，讲“热门”的虚拟命名映射到一个IP地址列表，而不是一个IP地址。默认情况下，当配置为返回一个列表时，BIND DNS循环调度该列表，所以当在DNS中查询虚拟命名时，仅一小部分用户会得到一个特定的IP地址。因此，热门虚拟命名的负载将会被分散到BIND DNS返回的IP地址列表中。 不幸的是，确定哪些虚拟命名热门哪些不热门并不是容易的事。然而，我们可以从每个缓存上的负载的测量值汇总得到一些启示。因为DNS服务器知道虚拟命名到IP地址的映射，我们可以确定哪个虚拟命名的集合负责有高负载的特定内存。这个集合包括一个或多个虚拟命名。因为我们想要防止服务器过载，我们会主动地将映射到热门服务器的所有虚拟命名分散到该区域的所有缓存中。（这意味着我们立即将集合中的每个虚拟命名映射为该缓存区域中的所有缓存IP地址的列表，这样这个虚拟命名的负载就会被分散到整个区域中。）然后，我们通过每次删除一个IP地址的方式慢慢减小映射的大小。如果任一次删除导致了服务器上的负载再次增加，我们将删除的IP地址恢复，并从分散的命名的其他自己中删除。通过这种方式，很快我们就能使映射到不同缓存列表大小的虚拟命名达到平衡。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:6:6","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"5.3 容错 除了高效的缓存管理和负载均衡，我们的系统还有非常高的容错能力。首先，因为物理IP地址是用户通过虚拟命名抽象出来的，所以可以避免用户访问到个别故障的缓存。当试图连接到这些机器时，浏览器不需要设置超时，因为（缓存故障时）虚拟命名会被解析到替代的IP地址。另外，我们的系统消除了单点故障，如CRIPS的中央目录故障。如果中央目录故障，CRISP系统就会崩溃，缓存机器会作为独立的缓存工作。在我们的系统中，不存在单独负责关键任务的单元，如CRIPS中的中央目录、为一组用户提供系统入口点的主缓存。在Cache Resolver中，只要DNS机器正常工作，那么虚拟命名就会被解析。且只要区域中的一些缓存存活，那么虚拟命名就会被解析为它们的IP地址。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:6:7","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"6. 结论 在第四章中，我们将我们的系统与其他缓存系统进行了比较。我们论证了，当用户负责知道哪个缓存有所需数据时，可以避免用户请求的关键循环中的重大开销。哪个缓存持有数据的信息可通过哈希函数提供给用户。由于大型网络（如Internet）的用户可能对实时的缓存有不一致的视图，所有我们间隙使用一致性哈希函数，即使用户视图有冲突，其也可以很好地均衡数据。此外，我们还描述了使用了一致性哈希函数的系统实现，以展示在万维网中集成这样一个系统是非常实用的。我们的系统解决了局部性问题、缓存间的负载均衡、并拥有其他Web缓存系统缺少的高层容错能力。总之，我们相信，通过对缓存的高效使用，一致性哈希能够大幅提升万维网的效率。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:7:0","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"致谢 感谢Frans Kaashoek提供的帮助和建议。 ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:8:0","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"参考文献 [1] P. Barford and M. Crovella, Generating representative Web workloads for network and server performance evaluation, in: Sigmetrics, 1997. [2] A. Chankhunthod, P. Danzig, C. Neerdaels, M. Schwartz and K. Worrell, A hierarchical Internet objectcache, in: USENIX, 1996. [3] J. Cohen, N. Phadnis, V. Valloppillil and K.W. Ross, Cache array routing protocol v. 1.0, http://www.ietf.org/internet-dra fts/draft-vinod-carp-v1-03.txt, September 1997. [4] L. Fan, P. Cao, J. Almeida and A.Z. Broder, Summary cache: a scalable wide-area Web-cache sharing protocol, Technical Report 1361, Computer Science Department, University of Wisconsin, Madison, February 1998. [5] S.A. Gadde, J. Chase and M. Rabinovich, A taste of crispy squid, in: Workshop on Internet Server Performance, June 1998, http://www.cs.duke.edu/ari/cisi/crisp/ [6] D. Karger, E. Lehman, T. Leighton, M. Levine, D. Lewin and R. Panigrahy, Consistent hashing and random trees: distributed cachine protocols for relieving hot spots on the World Wide Web, in: Proc. 29th Annu. ACM Symp. on Theory of Computing, 1997, pp. 654–663. [7] R. Malpani, J. Lorch and D. Berger, Making World Wide Web caching servers cooperate, in: Proc. 4th Int. World Wide Web Conference, 1995, pp. 107–110. [8] Microsoft Proxy Server, White paper, http://www.microsoft.com/proxy/guide/CarpWP.asp, 1998. [9] P. Yu and E.A. MaxNair, Performance study of a collaborative method for hierarchical caching in proxy servers, Technical Report RC 21026, IBM T.J. Watson Research Center, 1997. ","date":"2020-09-10","objectID":"/posts/paper-reading/web-caching-with-consistent-hashing/:9:0","tags":["Consistent Hashing","Translation"],"title":"《Web caching with consistent hashing》论文翻译","uri":"/posts/paper-reading/web-caching-with-consistent-hashing/"},{"categories":["Paper Reading"],"content":"本篇文章是对论文RDD-NSDI12-FINAL138的原创翻译，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:0:0","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"作者 Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael J. Franklin, Scott Shenker, Ion Stoica University of California, Berkeley ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:1:0","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"摘要 我们提出了一个能够使开发者在大型集群上执行内存式计算且带有容错的分布式内存的抽象——Resilient Distributed Datasets（RDD，弹性分布式数据集）。RDD的想法由在当前计算框架中处理效率不高的两类应用程序驱动：迭代算法和交互式数据挖掘工具。在这两种情况下，将数据保存在内存中能够将性能提高一个数量级。为了实现有效地容错，RDD提供了共享内存的一个受限的形式，其基于粗粒度的变换而不是细粒度的共享状态更新。然而，我们发现RDD足以表示很广泛的计算类型，包括最近像Pregel一样的专门针对迭代任务的程序模型，以及在新应用程序中这些模型表达不出的模型。我们在被称为Spark的系统中实现了RDD，并通过各种用户程序和benchmark来评估这个系统。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:2:0","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"1. 引言 像MapReduce[10]和Dryad[19]之类的集群计算框架已广泛引用于大规模数据分析。这些系统让用户可以通过一系列高层操作来编写并行计算，不需要担心工作的分布与容错。 尽管当前的框架提供了大量访问集群资源的抽象，它们仍缺少对利用分布式内存的抽象。这使它们对一类新兴的重要应用程序来说效率很低。这类新兴的应用程序会复用（reuse）多个计算的中间结果。数据复用在很多迭代（iterative）式机器学习和图算法（包括PageRank、K-means聚类、逻辑回归等）中很常见。另一个备受关注的使用场景是交互式数据挖掘，该场景下用户会在同一个数据的子集上运行多个临时查询。不幸的是，在大部分当前的框架中，在计算间（如两个MapReduce的job间）复用数据的唯一方式是将其写入外部稳定存储系统，如分布式文件系统。由于这样做需要数据副本、磁盘I/O和序列化等占用大部分程序执行时间的操作，这会导致非常可观的额外开销。 由于意识到了这一问题，研究者们已经开发了为一些需要复用数据的应用程序专门设计的框架。例如，Pregel[22]是一个为迭代式图计算设计的系统，其能将中间数据保存在内存中；HaLoop[7]提供了迭代式MapReduce接口。然而，这些框架仅支持特殊的编程模式（例如，循环一系列MapReduce的step），并在这些模式中进行隐式数据共享。它们不支持更普遍的数据复用抽象，如允许用户将几个数据集加载到内存中并运行跨这些数据集的临时查询。 在本文中，我们提出了一种新的抽象——resilient distributed datasets（RDD），其能在广泛的应用程序中进行高效数据复用。RDD是能容错的并行数据结构，其使用户能够显式地将中间结果在内存中持久化，并控制它们的分区以优化数据位置，且能够对其使用丰富的操作。 设计RDD的主要挑战是定义能够高效地提供容错的编程接口。已有的为集群中内存式存储（分布式共享内存[24]、键值存储[25]数据库和Piccolo[27]等）设计的抽象，提供了基于细粒度更新（fine-grained update）变更状态（如表中的单元格）的接口。在这种接口中，提供容错的唯一几种方式是将数据跨机器做副本或跨机器记录更新日志。这两种方法对于数据敏感性工作负载来说开销过于高昂，因为它们需要通过集群的网络复制大量的数据，而集群的网络带宽远低于RAM，且它们造成了大量的存储额外开销。 与这些系统不同，RDD提供了基于粗粒度（coarse-grained）的变换（如map、filter和join）接口，其对许多数据项应用相同的操作。这使它们能够通过记录构建数据集（它的普系统）使用的变换而不是对实际数据使用的变换的方式，来高效地提供容错注1。如果RDD的一个分区丢失，RDD有足够的关于它如何被从其它RDD导出的信息，来重新计算仅这一分区。因此，丢失的数据可被恢复，数据恢复速度通常非常快，且不需要开销高昂的副本。 注1：在一些RDD中，当数据的延伸链增长得很大时，对数据建立检查点非常有用。我们将在章节5.4中讨论如何操作。 尽管基于粗粒度变换的接口最初似乎非常有限，但RDD仍非常适用于许多并行程序，因为这些程序本身就对许多数据项应用相同的操作。事实上，我们发现RDD可以高效地表示很多集群编程模型，目前这些模型被不同的系统分别提出，其包括MapReduce、DryadLINQ、SQL、Pregel和HaLoop，以及新式应用程序中无法表示的模型，如交互式数据挖掘。RDD的这种仅通过引入新的框架就能适配过去已经满足了的计算需求的能力，在我们看来是RDD抽象能力最令人信服的证据。 我们在被称为Spark的系统中实现了RDD，该系统被在UC Berkeley和许多公司的研究和生产应用程序中使用。Spark在Scala编程语言[2]中提供了一个类似DryadLINQ[31]的很方便的语言集成的编程接口（language-integrated programming interface）。另外，Spark可被在Scala解释器中交互式查询大数据集时使用。我们认为Spark是第一个能够以交互所需的速度使用通用编程语言在集群中进行内存数据挖掘的系统。 我们通过小批量benchmark和在我们的应用程序中测量的方式来评估RDD和Spark的性能。我们发现Spark在迭代式应用程序中比Hadoop快了20倍，在真实的数据分析报告中快了40倍，且可在交互时以5~7秒的延时来扫描1TB的数据集。更重要的是，为了说明RDD通用性，我们在Spark之上实现了Pregel和HaLoop编程模型作为相对小的库（每个200行代码），包括他们使用的位置优化。 本文从RDD（第二章）和Spark（第三章）的概览开始。接着我们讨论了RDD的内部表示法（第四章）、我们的实现（第五章）和实验结果（第六章）。最后，我们讨论了RDD如何实现现有的几个集群编程模型（第七章），调查了相关工作（第八章）并进行总结。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:3:0","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"2. Resilient Distributed Datasets（RDD） 本章提供了RDD的概览。首先，我们定义了RDD（章节2.1），然后介绍了它们在Spark中的编程接口（章节2.2）。接着，我们将RDD与细粒度的共享内存抽象进行了对比（章节2.3）。最后，我们讨论了RDD模型的限制（章节2.4）。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:4:0","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"2.1 RDD抽象 从形式上看，RDD是一个只读的分区的记录的集合。RDD仅能通过确定性（deterministic）操作，从（1）稳定存储中的数据或（2）其他RDD上创建。我们将这些操作成为“变换”，以与RDD上的其他操作进行区分。变换的例子包括map、filter和join。注2 注2：尽管单独一个RDD是不可变的，但是还是可以通过使用多个RDD来实现可变状态，以表示数据集的多个版本。我们使RDD不可变是为了使其能够更容易描述谱系图（lineage graph），但这等价于将我们的抽象变为多版本数据集并在谱系图中跟踪版本号。 RDD不需要在所有时间都被实体化（materialized）。RDD有足够关于它是如何从其他数据集（它的谱系图，lineage graph）导出的信息，以能够从稳定存储中的数据计算它的分区。这是一个很强大的属性：本质上讲，如果RDD不能在故障后重构，那么应用程序就无法引用它。 最后，用户还能控制RDD的另两个方面：持久化（persistence）和分区（partitioning）。用户可以指出他们需要复用哪个RDD并为其选自一个存储策略（如内存存储）。用户还可以要求RDD的元素基于每个记录中的key跨机器分区。这对位置优化很有帮助，如保证两个将要被join到一起的数据集会按照相同的哈希分区。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:4:1","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"2.2 Spark编程接口 Spark通过类似DryadLINQ[31]和FlumeJava[8]的语言集成的API来暴露RDD，每个数据集被表示为一个对象，变换通过调用这些对象中的方法来实现。 编程人员从通过对稳定存储中的数据进行变换（如map和filter）来定义一个或更多的RDD开始。接下来，编程人员可以在action中使用这些RDD，actions是给应用程序返回值或将数据导出到存储系统的操作。action的例子包括count（返回数据集中元素数）、collect（返回元素本身）、save（将数据集输出至存储系统）。像DryadLINQ一样，Spark在RDD第一次被在action使用时懒式计算它，因此Spark可以将变换流水线化。 另外，编程人员可以调用persist方法来指定他们在未来的操作中需要复用的RDD。Spark默认会会将RDD在内存中持久保存，但Spark会在没有足够RAM的时候将其写入磁盘。用户可以可用persisi的标识请求其他的存储策略，如仅在磁盘中存储RDD或跨机器对RDD做副本。最后，用户可以对每个RDD设置持久化优先级，以指定内存中的那个数据会最先被写入到磁盘。 2.2.1 样例：终端日志挖掘 假设一个Web服务发生了错误，且有一个想要通过搜索Hadoop文件系统（HDFS）中TB级的日志来找到原因的操作。通过使用Spark，该操作可以仅将跨一系列节点的错误信息从日志装入RAM，并交互式地查询它们。该编程人员首先需要编写如下Scala代码： lines = spark.textFile(\"hdfs://...\") errors = lines.filter(_.startsWith(\"ERROR\")) errors.persist() 第一行定义了一个以HDFS文件（该文件由多行纯文本组成）作为后端的RDD，第二行获取了一个从该RDD过滤得到的新RDD。第三行要求errors在内存中持久化，这样它就可以被其他的查询共享。注意filter的参数是一个Scala闭包的表达式。 此时，集群中没有任何任务执行。然而，用户现在可以在action中使用这个RDD。例如，想要统计消息数量： errors.count() 用户也可以使用该RDD和其变换结果进行更多变换，就像下面代码中的那样： // Count errors mentioning MySQL: errors.filter(_.contains(\"MySQL\")).count() // Return the time fields of errors mentioning // HDFS as an array (assuming time is field // number 3 in a tab-separated format): errors.filter(_.contains(\"HDFS\")) .map(_.split('\\t')(3)) .collect() 在第一个涉及errors的action执行后，Spark会将errors的分区保存在内存中，这样大大提高了随后对其的计算速度。需要注意的是，基RDD lines没有被载入到RAM中。因为错误信息可能仅占数据的一小部分（小到足够放入内存中），所以这样是合理的。 最后，为了阐明我们的模型如何实现容错，我们在图1中展示了第三个查询的谱系图。在该查询中，我们从基于lines上过滤器的结果errors开始，在运行collect前对其应用更多的filter和map。Spark的调度器会后续的两个变换流水线化，并向持有errors分区缓存的节点发送一系列任务来对计算它们。另外，如果errors的一个分区丢失，Spark会通过仅对其响应的lines的分区应用filter来重建这一分区。 图1 我们的例子中第三个查询的谱系图。方框表示RDD，箭头表示变换图1 我们的例子中第三个查询的谱系图。方框表示RDD，箭头表示变换 \" 图1 我们的例子中第三个查询的谱系图。方框表示RDD，箭头表示变换 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:4:2","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"2.3 RDD模型的优势 为了理解作为分布式内存抽象（distributed memory abstraction）的RDD模型的好处，我们在表1中将其与分布式共享内存（distributed shared memory，DSM）进行了对比。在DSM系统中，应用程序可以读写全局地址空间的任意位置。主要注意的是，在该定义下，不仅包括了传统的共享内存系统[24]，还包括应用程序对共享状态进行细粒度写入的系统，这类系统包括Piccolo[27]，其提供了共享的DHT（Distributed Hash Table）和分布式数据库。DSM是一种非常通用的抽象，但是它的通用性使其很难以在商用集群上实现高性能和容错能力。 表1 RDD与DSM的对比 方面 RDD DSM 读 粗粒度或细粒度 细粒度 写 粗粒度 细粒度 一致性 不重要（不可变） 取决于app或runtime 故障恢复 细粒度且使用谱系图额外开销较小 需要检查点和程序回滚 掉队者缓解 可通过任务备份实现 难 任务位置选择 基于数据位置自动化 取决于app（runtime目标为透明性） 没有足够RAM的行为 类似现有的数据流系统 性能低（使用swap？） RDD和DSM的主要区别是，RDD只能通过粗粒度的变换创建（“写入”），而DSM允许对每个内存位置进行读写。注3这将RDD的使用限制在执行批量写入的应用程序中，但也使其能够进行更高效的容错。在实际情况下，RDD不需要承担检查点的开销，因为其可通过谱系图恢复。注4除此之外，在故障发生时，RDD中仅丢失的分区需要被重新计算且它们可以在不同节点上并行地重新计算，不需要回滚整个程序。 注3：需要注意的是，RDD的读操作仍可以使细粒度的。例如，应用程序将RDD当做大型只读查找表来对待。 注4：在一些应用程序中，其仍可以对谱系图链较长的RDD创建检查点，我们将在章节5.4中讨论。然而，因为RDD是不可变的，这一操作可造后台执行，并且不需要像DSM一样对整个应用程序进行快照。 RDD的第二个好处是它们本身不可变的性质让系统能够通过备份较慢的任务的方式缓解较慢的节点（掉队者），就像MapReduce中的那样[10]。备份任务在DSM中很难实现，因为一个任务的两份副本会访问内存中相同的位置，并干扰彼此的更新。 最后，RDD还比DSM多提供了两个好处。第一，对于RDD中的批量操作，运行时可以基于数据位置来调度任务以提高性能。第二，当没有足够内存来保存RDD时，只要它仅会被基于扫描的操作使用，那么它就可以优雅地降级（degrade）。RAM放不下的分区可被保存在磁盘中，并将提供与当前的并行数据系统相似的性能表现。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:4:3","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"2.4 不适用于RDD的应用程序 正如引言中讨论的那样，RDD最适合对数据集中所有元素应用相同操作的的批处理程序。在这些情况下，RDD可以将每一次变换作为谱系图中的一步来高效地记住它们，并在不需要记录当量数据的情况下恢复丢失的分区。RDD不太适用于对共享状态进行细粒度的一不更新，例如为Web应用程序或增量Web爬虫设计的存储系统。对于这些应用程序，使用执行传统的更新日志和数据检查点的系统会更高效，如数据库、RAMCloud[25]、Percolator[26]和Piccolo[27]。我们的目标是为批量分析提供高效的编程模型，将异步应用程序的问题留给专用的系统解决。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:4:4","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"3. Spark编程接口 Spark在Scala[2]（一种运行在Java VM上的静态类型函数式编程语言）中提供了类似DryadLINQ[31]的RDD抽象的语言继承的API。我们选择Scala的原因是其集简介（便于交互式使用）和高效（因为其采用静态类型）于一身。然而， RDD的抽象并不一定需要函数式语言。 为了使用Spark，开发者需要编写一个连接集群中worker的驱动程序，如图2中所示。该驱动程序定义了一个或多个RDD和在RDD智商的变换。驱动程序中的Spark代码还会追踪RDD的谱系图。worker是长久存在的进程，它们可以将操涉及的RDD分区存储在RAM中。 图2 Spark Runtime。用户的驱动程序启动了多个worker，worker从分布式文件系统中读取数据块并将计算出的RDD分区在内存中持久保存。图2 Spark Runtime。用户的驱动程序启动了多个worker，worker从分布式文件系统中读取数据块并将计算出的RDD分区在内存中持久保存。 \" 图2 Spark Runtime。用户的驱动程序启动了多个worker，worker从分布式文件系统中读取数据块并将计算出的RDD分区在内存中持久保存。 正如我们在章节2.2.1中的日志挖掘样例一样，用户提供通过传递闭包（字面函数，function literals）的方式为像map之类的RDD操作提供参数。Scala将每个闭包表示为一个Java对象，这些对象可被序列化，以通过网络床底该闭包并在另一个节点上载入。Scala还会将任何绑定在闭包中的变量作为Java对象的字段保存。例如，用户可以编写如var x=5; rdd.map(_ + x)的代码来将RDD中的每个元素加5.注5 我们在每个闭包被创建时保存，因此在这个map的例子中，尽管$x$改变了，也会被加5。 RDD本身是由元素类型确定的静态类型对象。例如，RDD[Int]是整型的RDD。然而，我们大部分的例子都省略的类型，因为Scala支持类型推断。 尽管我们在Scala中暴露RDD的方法从概念上讲很简单，我们还是=不得不使用反射[33]来处理Scala闭包对象的相关问题。我们还需要更多的工作来使Spark在Scala解释器中可用，这将在章节5.2中讨论。尽管如此，我们仍不必修改Scala编译器。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:5:0","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"3.1 Spark中的RDD操作 表2列出了Spark中可用的主要的RDD变换和aciton。我们给出了每个操作的签名，在方括号中给出了参数类型。变换是定义一个新的RDD的懒式操作，而action会启动计算以向程序返回值或将数据写入外部存储。 需要注意的是，有些操作（如join）仅在RDD的键值对上可用。另外，我们的选择的函数名与其他Scala中的API和其他函数式语言相匹配。例如，map是一个一对一的映射，而flatMap会将每个输入值映射为一个或多个输出（类似于MapReduce中的map）。 除了这些操作外，用户来可以请求持久化RDD。另外，用户可以获取RDD分区顺序，它通过Partitioner类表示，用户可以根据它对另一个数据集进行分区。如groupByKey、reduceByKey、sort等操作会自动地产生按哈希或范围分区的RDD。 表2 Spark中可用的RDD变换和aciton。Seq[T]表示一个类型的T的元素序列。表2 Spark中可用的RDD变换和aciton。Seq[T]表示一个类型的T的元素序列。 \" 表2 Spark中可用的RDD变换和aciton。Seq[T]表示一个类型的T的元素序列。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:5:1","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"3.2 应用程序样例 我们用两个迭代式应用程序补充了章节2.2.1中的数据挖掘样例：逻辑回归和PageRank。后者还展示了如何控制RDD的分区来提高性能。 3.2.1 逻辑回归 许多机器学习算法本身就是迭代式的，因为它们运行如梯度下降法等迭代优化生成器以获得最大化的函数。因此。如果将数据保存在内存中，它们可以运行的快得多。 作为一个样例，如下的程序实现了逻辑回归[14]，逻辑回归是一种通用的分类算法，其寻找一个能够最佳划分两个点集（如垃圾邮件与非垃圾邮件）的超平面$w$。该算法使用梯度下降法：$w$从一个随机值开始，在每一轮迭代中，会对$w$的函数求和，以使$w$向更优的方向移动。 val points = spark.textFile(...) .map(parsePoint).persist() var w = // random initial vector for (i \u003c- 1 to ITERATIONS) { val gradient = points.map{ p =\u003e p.x * (1/(1+exp(-p.y*(w dot p.x)))-1)*p.y }.reduce((a,b) =\u003e a+b) w -= gradient } 我们从定义一个持久化的RDD points开始，它是在文本文件上使用map变换的结果，map变换将文本的每一行解析为一个Point对象。接下来，我们对points循环执行map和reduce来对当前$w$的函数求和，进而计算每一步的梯度。在多次迭代间，将points保存在内存中可以得到20倍的速度提升，正如我们在章节6.1中展示的那样。 3.2.2 PageRank 在PageRank[6]中有更复杂的数据共享模式。PageRank算法对每个文档，迭代地累加其他链接到它的文档的贡献值，来更新该文档的rank值。在每一轮迭代中，每个文档向与它相邻的文档发送$\\frac{r}{n}$的贡献值，其中$r$是它的rank，$n$是与它相邻的文档数。接下来，更新其rank值到$ \\alpha / N + ( 1 - \\alpha ) \\sum c_i$，其中$ \\sum c_i $是其收到的贡献值的和，$N$是文档的总数。我们可以在Spark中按如下方式编写PageRank： // Load graph as an RDD of (URL, outlinks) pairs val links = spark.textFile(...).map(...).persist() var ranks = // RDD of (URL, rank) pairs for (i \u003c- 1 to ITERATIONS) { // Build an RDD of (targetURL, float) pairs // with the contributions sent bu each page val contribs = links.join(ranks).flatMap { (url, (links, rank)) =\u003e links.map(dest =\u003e (dest, rank/links.size)) } // Sum contributions by URL and get new ranks ranks = contribs.reduceByKey((x,y) =\u003e x+y) .mapValues(sum =\u003e a/N + (1-a)*sum) } 这个程序的RDD谱系图如图3所示。在每轮迭代中，我们基于上一轮迭代的contribs和ranks和静态的lnks数据集创建了一个新的ranks数据集。注6该图的一个有趣的特征是，随着迭代次数的增加，该图会越来越长。因此，在有许多次迭代地任务中，有必要可靠地备份ranks的某些版本，以减少故障恢复次数[20]。用户通过一个RELIABLE标识符调用persist来实现这一点。然而，需要注意的是，links数据集不需要被备份，因为它的分区可通过在输入文件的块上重跑map来高效地重建。通常情况下，这个数据集要比ranks大得多，因为每个文档中有许多连接，但每个文档仅有它自己的一个rank值，所以采用谱系图的方式对其进行恢复比对程序在内存中的整个状态设置检查点会节省更多系统时间。 注6：需要注意的是，尽管RDD是不可变的，程序中的ranks和contribs变量在每轮迭代中都指向不同的RDD。 图3 PageRank中数据集的谱系图图3 PageRank中数据集的谱系图 \" 图3 PageRank中数据集的谱系图 最后，我们可以通过控制RDD的分区方式来优化PageRank中的通信。如果我们为links指定一个分区方式（例如，将link的列表基于哈希算法在节点间分区），我们可以将ranks采用同样的方式分区，以保证对links和ranks的join操作不需要进行通信（因为每个URL的rank将于其link的列表在相同的机器上）。我们还可以编写一个自定义的Partitioner类对互相链接的页面进行分组（例如按照域名分区）这两种优化都可以在我们定义links时通过调用partitionBy来表达： links = spark.textFile(...).map(...) .partitionsBy(myPartFunc).persist() 在这次最初的调用后，links和ranks间的join操作会自动地将给每个URL的贡献聚合到该link所在的机器上，计算其新的rank值，并将改值加入到它的link中。这种迭代间的一致分区方式是类似Pregel的专用框架的主要优化方式之一。RDD让用户能够直接地表达这一目标。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:5:2","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"4. RDD的表示 将RDD作为一种抽象提供的挑战之一是为其选择一种可以在大量的变换中追踪谱系图的表示法。在理想情况下，实现RDD的系统需要提供尽可能多的变换操作（如表2中的操作），且允许用户以任意方式组合这些操作。我们提出了一个简单的基于图的RDD表示法，其实现了这些目标。我们在Spark中使用了这种表示法，以在不为每一个调度器添加特殊逻辑的情况下支持大量变换，这大大简化了系统设计。 简而言之，我们提出了一种能够通过通用的结构来表示每个RDD的方式，其暴露了5种信息：分区的集合，分区是数据集的原子单位；对父RDD的依赖的集合；一个基于其父数据集计算该数据集的函数；分区策略和数据放置位置的元数据。例如，有一个表示HDFS文件的RDD，其对该文件的每个块都有一个分区，且知道每个块在哪台机器上。同时，该RDD上的map操作结果的分区数与该RDD相同，在计算该RDD的元素时，会对其父级数据应用map函数。我们在表3中总结了这些接口。 表3 Spark中用来表示RDD的接口 操作 含义 partitions() 返回Partition对象的列表 preferredLocations(p) 列出p因数据位置而可被快速访问的节点 dependencies() 返回依赖的列表 iterator(p,parentIters) 给定对其父分区上的迭代器，计算分区p的元素 partitioner() 返回用来指定RDD是通过哈希还是范围分区的元数据 在设计这个接口的过程中最有趣的问题是，如何表示两个RDD间的依赖关系。我们发现将依赖分为以下两种类型是足够且有帮助的：窄（narrow）依赖，父RDD中每个分区都仅被最多一个子RDD使用；宽（wide）依赖，多个子分区可能依赖同一个父分区。例如，map会产生窄依赖，而join会产生宽依赖（除非父分区是基于哈希分区的）。图4展示了其他的例子。 图4 窄依赖和宽依赖的例子。每个方格是一个RDD，其分区以涂色的矩形表示。图4 窄依赖和宽依赖的例子。每个方格是一个RDD，其分区以涂色的矩形表示。 \" 图4 窄依赖和宽依赖的例子。每个方格是一个RDD，其分区以涂色的矩形表示。 这一划分很有用，其原因有二。窄依赖允许在集群的节点上流水线式执行计算，该节点可以计算所有的祖先分区（译注：即可以将多步窄依赖压缩，在一个节点上对之前所有的窄依赖流水线式计算）。例如，用户可以在基于逐元素计算的filter操作后紧接着进行一个map操作。相反，宽依赖需要所有来自父分区的数据都可用，而且需要将这些数据通过类似MapReduce方式的操作将这些数据在节点间“shuffle”。第二，窄依赖的节点在故障后可以更高效地恢复，因为其仅都是的父分区需要被重新计算且这些分区可以在不同节点中并行地重新计算。相反，对于有宽依赖的谱系图，单节点故障可能导致RDD的所有祖先分区中都有部分分区丢失，其需要完整地重新执行。 该RDD的通用接口使在Spark中通过少于20行代码实现大部分变换成为了可能。事实上，即使是Spark的新用户也可以在不知道调度细节的情况下实现新的变换（例如，简单地和各种各样的join操作）。我们在下文中描述了一些RDD的实现。 HDFS文件： 我们的样例中输入RDD为HDFS中的文件。对于这些RDD，partitions会返回该文件每个块的分区（以及每个Partition对象中保存的块的偏移量），preferredLocations给出块所在的节点，iterator会读取块。 map： 对任何RDD调用map都会返回一个MappedRDD对象。该对象与其父RDD有相同的分区数和首选位置，但是在它的iterator方法中，会将传递到map中的函数应用到其父RDD的记录中。 union： 在两个RDD上调用union会返回一个分区为这两个父RDD的分区的并的RDD。每个子分区通过对应的父分区通过窄依赖计算得到。注7 sample： sample操作与map操作类似，但是其RDD会为每个分区保存一个随机数生成器种子，以确定性地采样父RDD中的记录。 join： 对两个RDDjoin可能产生两种窄依赖（如果它们都使用相同的partitioner来以哈希或范围的方式分区）、两种宽依赖、和一种宽依赖与窄依赖的混合（日过一个父RDD有partitioner而另一个没有）。在其中任一种情况下，输出RDD都有一个partitioner（或者从父RDD中继承，或者是一个默认的哈希分区器）。 注7：需要注意的是，union操作不会丢弃重复的值。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:6:0","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"5. 实现 我们在Scala中使用大概14000行代码实现了Spark。系统运行在Mesos集群管理器[17]上，使其能够与Hadoop、MPI和其他应用程序共享资源。每个Spark程序作为一个单独的Mesos应用程序运行，其有自己的驱动（master）和worker，且应用程序间的资源共享由Mesos处理。 Spark可以使用Hadoop现有的输入插件API从任意Hadoop输入源（如HDFS或HBase）读取数据，并运行原版的Scala。 现在我们将概括该系统中在技术上有趣的部分：作业调度器（章节5.1）、允许交互式使用的Spark解释器（章节5.2）、内存管理（章节5.3）和对检查点的支持（章节5.4）。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:7:0","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"5.1 作业调度 Spark作业（译注：本文从此处开始区分作业job和任务task）调度器使用了在第四章中描述的RDD的表示法。 总的来看，我们的调度器与Dryad[19]的很像，但它会额外考虑被在被持久化的RDD中哪些分区在内存中可用。当用户在一个RDD上运行action（例如count或save）时，调度器会查阅RDD的谱系图并建立stage的DAG（Directed Acyclic Graph，有向无环图），如图5中的样例那样。每个stage包含尽可能多的能被流水线化的窄依赖变换。stage的便捷是宽依赖需要的shuffle操作，或任何可以缩短父RDD计算的已被计算过的分区。调度器会为每个stage启动task来计算缺失的分区，知道计算出目标RDD。 图5 Spark执行作业的stage的样例。实线框表示RDD。有填充的矩形是分区，如果分区已经在内存中那么使用黑色表示。为了在RDD上运行action，我们根据宽依赖建立stage，且每个stage中有流水线式的窄依赖变换。在图中，stage 1的输出RDD已经在RAM中，所以我们只需先后运行stage 2和stage 3。图5 Spark执行作业的stage的样例。实线框表示RDD。有填充的矩形是分区，如果分区已经在内存中那么使用黑色表示。为了在RDD上运行action，我们根据宽依赖建立stage，且每个stage中有流水线式的窄依赖变换。在图中，stage 1的输出RDD已经在RAM中，所以我们只需先后运行stage 2和stage 3。 \" 图5 Spark执行作业的stage的样例。实线框表示RDD。有填充的矩形是分区，如果分区已经在内存中那么使用黑色表示。为了在RDD上运行action，我们根据宽依赖建立stage，且每个stage中有流水线式的窄依赖变换。在图中，stage 1的输出RDD已经在RAM中，所以我们只需先后运行stage 2和stage 3。 我们的调度器使用延时调度[32]的方式基于数据的位置将任务分配到机器上。如果一个任务需要出在一个节点的内存中可用的分区，我们会将任务发送到该节点上。否则，任务会被发送达到在分区所在的RDD提供的首选位置（例如HDFS文件）上处理。 对于宽依赖（即shuffle依赖），我们在持有其分区的节点上计算中间记录以简化故障恢复，这很像MapReduce中计算map的输出的方式。 如果任务失败，只要其stage的父RDD仍可用，我们就会在另一个节点上重跑它。如果一些stage已经不可用（例如，因为shuffle的“map侧”的输出丢失），我们会重新提交任务来并行地计算缺失的分区。目前我们不需要对调度器故障进行容错，尽管备份RDD谱系图是一件很简单的事。 最后，尽管目前Spark中的所有计算都是为响应驱动程序中调用的action运行的，我们还是计划让集群中的任务（如map）调用lookup操作，该操作提供了按key对哈希分区的RDD中元素的随机访问功能。在这种情况下，任务将需要让调度器计算所需的缺失的分区。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:7:1","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"5.2 集成解释器 Scala中包括一个交互式shell，其与Ruby和Python中的类似。考虑到内存中数据低延时的特性，我们希望能通过解释器交互地运行Spark来查询大型数据集。 正常情况下，Scala解释器会为用户输入的每一行编译一个类，将其加载到JVM中调用它的函数。这个类包括一个包含这行代码中变量和函数的单例对象，并在初始化方法中运行该行代码。例如，如果用户输入var x = 5并随后输入println(x)，解释器会定义一个包含x的类Line1，因此第二行代码会被编译成println(Line1.getInstance().x)。 我们对Spark中的解释器做了两种改变： 类传输： 允许worker节点获取每行代码创建的类的bytecode，我们让解释器通过HTTP来提供这些类的服务。 代码生成修改： 正常情况下， 为每行代码创建的单例对象会被对应的类的静态方法访问。这意味着，当我们序列化一个引用了之前行代码中变量的闭包时，如上例中的Line1.x，Java不会跟踪对象图来传递封装了x的Line1实例。因此，worker节点不会接受x。我们修改了代码生成逻辑，以直接在每行代码生成的对象中引用实例。 图6展示了在我们的修改后，解释器如何将用户输入的一系列代码翻译为Java对象。 图6 展示Spark解释器如何将用户输入的两行代码翻译为Java对象的样例。图6 展示Spark解释器如何将用户输入的两行代码翻译为Java对象的样例。 \" 图6 展示Spark解释器如何将用户输入的两行代码翻译为Java对象的样例。 我们发现Spark解释器在处理存储在HDFS中的轨迹数据时非常有用，这些数据是我们在研究和探索用的数据集的一部分。我们还计划交互式运行高层的查询语言，如SQL。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:7:2","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"5.3 内存管理 Spark为提供了3个存储持久化的RDD的选项：作为反序列化的Java对象在内存中存储、作为序列化的数据在内存中存储、在磁盘中存储。第一个选项提供了最好的性能，因为Java VM可以直接访问每个RDD中的元素。第二个选项让用户在空间有限时，可以选择一个比Java对象图对内存利用率更高的表示，其代价是性能较低。注8第三个选项在RDD过大以至于无法在RAM中保存时很有用，但在每次使用时都需要较大开销来重新计算。 注8：开销取决于应用程序对每字节数据进行多少计算，对于轻量级处理，开销可能为2倍。 为了管理有限的可用内存，我们在RDD的级别上使用了LRU驱逐策略。当有一个新的RDD分区计算完成但是没有足够空间保存时，我们会将最早被访问的RDD的分区驱逐出去，除非它与新分区是同一个RDD。如果二者是同一个RDD，我们会在内存中保持旧的RDD，以防止同一个RDD的分区循环换入换出。因为大部分操作会在整个RDD上运行，这个在内存中的旧分区有很大可能在接下来会被使用，所以这样做非常重要。目前为止，这种默认的策略在我们的应用程序中表销量好，但是我们还是试过“持久化优先级（persistence proiority）”给用户对每个RDD的更多控制权。 最后，目前集群中的每个Spark都有其自己独立的内存空间。在未来的工作中，我们计划通过统一的内存管理器来跨Spark实例共享RDD。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:7:3","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"5.4 对检查点的支持 尽管在发生故障后，总是可以使用谱系图来恢复RDD，但是当RDD的谱系图关系链很长时，这种方式会非常耗时。因此，将一些RDD在稳定存储中建立检查点是很有用的。 通常，检查点对有很长的宽依赖谱系图的RDD很有帮助，如在我们PageRake样例中的ranks数据集（章节3.2.2）。在这些情况下，集群中一个节点的故障可能会导致其每个祖先RDD中都有一些数据分片丢失，如要完整地重新计算[20]。相反，对于在稳定存储的窄依赖RDD，如我们逻辑回归样例中的points（章节3.2.1）和PageRank样例中的links（章节3.2.2），不值得使用检查点。如果一个节点故障，这些RDD中丢失的分区可以在其他节点中并行地重新计算，与复制整个RDD的开销相比仅占很小的比例。 目前，Spark提供了检查点API（persist参数的REPLICATE标识符），但将对哪些数据建立检查点的决定权留给了用户。然而，我们也在研究如何自动创建检查点。因为我们的调度器知道每个数据集的大小即第一次计算它使用的时间，调度器应该能够选择出一组最优的需要建立检查点的RDD来最小化系统恢复时间[30]。 最后，需要注意的是，RDD只读的特性使其在建立检查时比使用通用的共享内存策略监理检查点更简单。因为不需要考虑一致性问题，RDD可在后台被写出，而不需要暂停程序或分布式快照策略。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:7:4","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"6. 性能评估 我们通过在Amazon EC2和用户程序的benchmark上的一系列实验对Spakr和RDD进行了评估。总体上看，我们得到的结果如下所示： Spark在迭代式机器学习和图应用程序中的速度比Hadoop快20倍。这个速度的提升来自于通过将Java对象保存在内存中而避免了I/O和反序列化的开销。 我们的用户编写的应用程序性能表现和扩展性良好。在实际环境中，我们使用Spark来加速一个分析报告的生成，其比在Hadoop中运行快了40倍。 当节点故障时，Spark可以通过仅重建丢失的RDD分区的方式快速恢复。 Spark可在5~7秒内交互式查询1TB的数据集。 首先，我们我们给出了迭代式机器学习程序（章节6.1）和PageRank（章节6.2）中的benchmark，并与Hadoop进行对比。接着，我们评估了Spark中的故障恢复（章节6.3）和数据集无法放入内存时的行为（章节6.4）最后，我们用户应用程序的实验结果（章节6.5）和交互式数据挖掘（章节6.6）。 除非提到的另外情况，我们在测试中使用的都是m1.xlarge EC2节点，有4核和15GB的RAM。我们使用HDFS进行存储，块大小为256MB。在每个测试前，我们清理了OS缓冲区缓存以精确测量IO开销。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:8:0","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"6.1 迭代式机器学习应用程序 我们实现了两个迭代式机器学习应用程序：逻辑回归、和k-means，来比较下列系统的性能表现： Hadoop： Hadoop 0.20.2 stable release。 HadoopBinMem： 一种Hadoop的改进，在首次迭代时将输入数据转换为一种低开销的二进制格式，以消除在之后的迭代中的文本解析，并将其保存在仅使用内存的HDFS实例中。 Spark： 我们的RDD实现。 我们使用25~100台机器，每个算法在100GB的数据集上迭代10次。两个程序的关键区别在于为每字节数据执行的计算量。k-meas每次迭代时间主要取决于计算，而逻辑回归较不计算敏感，其对反序列化和I/O的时间消耗更为敏感。 因为通常机器学习算法需要10次迭代才能收敛，我们单独报告了首轮迭代时间和之后的迭代时间。我们发现通过RDD来共享数据可以大幅加速后续的迭代。 图7 Hadoop、HadoopBinMem和Spark中逻辑回归和k-means的首轮迭代和最后一轮迭代执行时间（在100个节点组成的集群中使用100GB的数据）。图7 Hadoop、HadoopBinMem和Spark中逻辑回归和k-means的首轮迭代和最后一轮迭代执行时间（在100个节点组成的集群中使用100GB的数据）。 \" 图7 Hadoop、HadoopBinMem和Spark中逻辑回归和k-means的首轮迭代和最后一轮迭代执行时间（在100个节点组成的集群中使用100GB的数据）。 图8 Hadoop、HadoopBinMen和Spark中首轮迭代的后续迭代的运行时间。所有作业都处理了100GB数据。图8 Hadoop、HadoopBinMen和Spark中首轮迭代的后续迭代的运行时间。所有作业都处理了100GB数据。 \" 图8 Hadoop、HadoopBinMen和Spark中首轮迭代的后续迭代的运行时间。所有作业都处理了100GB数据。 首轮迭代： 所有三个系统在首轮迭代中都从HDFS中读取文本输入。正如图7所示，Spark在两个实验中比Hadoop稍快一点。这一不同是由于Hadoop中master和worker的心跳协议带来的巨大开销。HadoopBinMem最慢，因为其需要额外运行一个MapReduce作业来将数据转为二进制的，其还需要将数据通过网络在仅使用内存的HDFS实例中备份。 后续迭代： 图7还展示了后续迭代的平均运行时间，而图8展示了这些计算在集群中伸缩到了多大规模。对于逻辑回归，在100台机器上时Spark分别比Hadoop和HadoopBinMen快了25.3倍和20.7倍。对于对计算更敏感的k-means应用程序来说，Spark仍能达到1.9倍和3.2倍的提速。 对提速的理解： 我们惊讶的法线Spark甚至比仅用内存存储二进制数据的Hadoop（HadoopBinMem）的性能表现还要好20倍。在HadoopBinMem中，我们使用了Hadoop的标准二进制格式（SeqenceFile），且块大小为较大的256MB，且我们强制HDFS的数据目录在内存文件系统上。然而，Hadoop仍更慢，其原因有以下几点： Hadoop软件栈的最小开销； HDFS提供数据服务的开销； 将二进制记录转换为在内存中可用的Java对象的反序列化开销。 我们一依次研究了这些因素。为了测量（1），我们运行了不做任何操作的Hadoop作业。我们发现，为了完成作业配置的最小需求、任务启动和清理，这至少需要25秒的开销。考虑到（2），我们发现HDFS会执行多次内存拷贝，并对提供服务的每个块进行校验和。 最后为了测量（3），我们在单个机器上运行了一个微benchmark，在256MB的各种输入格式上运行逻辑回归运算。特别是，我们比较了从HDFS（其将显示HDFS栈的开销）和一个本地内存文件（内核可以高效地将数据传递给应用程序）输入的处理文本和二进制数据的处理时间。 图9 在单个机器上使用来自不同输入源的256MB数据运行逻辑回国的迭代时间。图9 在单个机器上使用来自不同输入源的256MB数据运行逻辑回国的迭代时间。 \" 图9 在单个机器上使用来自不同输入源的256MB数据运行逻辑回国的迭代时间。 我们将这些测试的结果展示在了图9中。内存式HDFS和本地文件的结果差异说明从HDFS读取会引入2秒的额外开销，即使数据在本地机器的内存中。文本和二进制输入的结果差异表明解析的开销为7秒。最后，即使从本地内存文件中读取，将预解析的二进制数据转为Java对象仍花费了3秒，这几乎与逻辑回归本身的开销一样。通过将RDD的元素直接作为Java对象保存在内存中，Spark可以避免所有的这些额外开销。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:8:1","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"6.2 PageRank 我们比较了Spark与Hadoop在54GB Wikipedia数据转储上运行PageRank的性能。我们运行了10轮PageRank算法的迭代，以处理约4,000,000篇文章组成的连通图。图10显示了，在30个节点上，仅使用内存存储的Spark就比Hadoop快了2.4倍。另外，通过控制RDD的分区使其在多轮迭代间保持，正如我们在章节3.2.2中讨论的那样，让速度进一步提升到了7.4倍。其结果在伸缩到60台节点时接近线性增长。 图10 Hadoop和Spark上PageRank的性能表现。图10 Hadoop和Spark上PageRank的性能表现。 \" 图10 Hadoop和Spark上PageRank的性能表现。 我们还评估了一个使用了我们在Spark上实现的Pregel来编写的PageRank版本的性能，我们在章节7.1中描述了它。其迭代次数与图10中的相似，但是时间长了大概4秒，因为Pregel需要在每轮迭代时运行额外的操作以让节点（vertice）“投票”是否完成作业。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:8:2","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"6.3 故障恢复 我们评估了k-means程序中一个节点故障后使用谱系图来重建RDD分区的开销。图11比较了在集群中的75台节点上运行的10轮迭代的k-means算法，在正常情况下与在第6轮迭代开始时有一台节点故障的情况下，每轮迭代的运行时间。在没有任何故障的情况下，每轮迭代由运行在100GB数据上的400个任务组成。 图11 出现故障时k-means的迭代时间。在第6轮迭代开始时，一台机器被kill掉，这导致了RDD需要使用谱系图重构部分RDD。图11 出现故障时k-means的迭代时间。在第6轮迭代开始时，一台机器被kill掉，这导致了RDD需要使用谱系图重构部分RDD。 \" 图11 出现故障时k-means的迭代时间。在第6轮迭代开始时，一台机器被kill掉，这导致了RDD需要使用谱系图重构部分RDD。 直到第5轮迭代结束时，迭代时间大概为58秒。在第6轮迭代开始时，其中一台机器被kill掉，这导致了运行在该机器上的任务和存储在其上的RDD分区丢失。Spark并行地在其他机器上重跑了这些任务，它们重读了相关的输入数据并通过谱系图重构了RDD，这使迭代时间增加到了80秒。当丢失的RDD分区被重建后，迭代时间又回到了58秒。 需要注意的是，即使使用了基于检查点的故障恢复机制，恢复时很有可能还是需要重跑至少几个迭代，这取决于建立检查点的频率。另外，系统需要通过网络复制100GB的工作集（输入数据的文本会被转为二进制），且需要在Spark中消耗而被的内存来备份它的RAM，或者需要等100GB的数据写入到磁盘。相反，在我们这个例子中的RDD的谱系图均小于10KB。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:8:3","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"6.4 内存不足时的行为 目前，我们保证集群中的每个机器有足够的内存来存储所有迭代中的所有RDD。我们很自然地会想到一个问题，Spark如何在没有能存储作业数据的足够内存时怎样运行。在该实验中，我们配置Spark在每台机器上不使用超过一定百分比的内存来存储RDD。我们在图12中展示了逻辑回归在不同内存存储空间下的实验结果。我们发现随着空间减少，性能优雅地降级。 图12 在25台节点上使用100GB数据的逻辑回归在内存中有不同的总数据量下的性能。图12 在25台节点上使用100GB数据的逻辑回归在内存中有不同的总数据量下的性能。 \" 图12 在25台节点上使用100GB数据的逻辑回归在内存中有不同的总数据量下的性能。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:8:4","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"6.5 使用Spark构建的用户应用程序 内存式分析： Conviva公司是一个音频分发公司，该公司使用Spark来加速胆量的数据分析个报告，这些任务过去运行在Hadoop上。例如，一个报告以一系列Hive[1]查询的形式为每一位顾客计算各种分析数据。这些查询都运行在数据的同一个子集上（匹配用户提供的过滤器的记录），但是会在不同组的域上聚合（average、percentile、COUNT DISTINCT），其需要不同的MapReduce作业。通过在Spark中实现这些查询并将它们共享的数据的子集加载到RDD中，该公司可以将这些报告的速度提高40倍。200GB被压缩过的数据上的报告，在Hadoop集群中需要运行20个小时，现在，只需要使用两台Spark机器就可以再30分钟内运行完。另外，Spark程序仅需96GB的RAM，因为其只需要存储RDD中匹配客户过滤器的行和列，不需要解压整个文件。 交通建模： Berkeley的Mobile Millennium项目[18]研究并行化了一个通过稀疏的汽车GPS测量数据推断道路交通堵塞情况的算法。其源数据是一个大都市区域的有10,000个连接的道路网络，和600,000个来自装有GPS车辆的点到点旅程的时间样本（每个旅程的路径可能包含多条道路网络）。使用交通模型，该系统可以估算通过每条道路连接所需的时间。该研究使用了最大期望算法（expectation maximization，EM）来训练这个模型，该算法会循环迭代map和reduceByKey两步。该应用程序在4核机器上，从20个节点伸缩到80个节点时，其性能近似是线性增长的，如图13(a) 所示。 Twitter垃圾邮件分类： Berkeley的Monarch项目[29]使用Spark来识别Twitter消息中的垃圾连接。他们在Spark上实现了一个类似章节6.1中样例的逻辑回归分类器，但是他们使用了一个分布式的reduceByKey来对梯度向量并行地求和。在图13(b) 中，我们给出了在250,000个URL和$10^7$个与每个URL的网络和页面内容中的属性相关的特征（维度）数据的50GB子集训练分类器的比例结果。这个比例不接近线性，因为每次迭代需要很高的固定的通信开销。 图13 两个通过Spark实现的用户程序每轮查询的运行时间。误差条表示标准差。图13 两个通过Spark实现的用户程序每轮查询的运行时间。误差条表示标准差。 \" 图13 两个通过Spark实现的用户程序每轮查询的运行时间。误差条表示标准差。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:8:5","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"6.6 交互式数据挖掘 为了演示Spark交互式查询大数据集的能力，我们用它来分析1TB的Wikipedia网页浏览日志（两年的数据）。在本实验中，我们使用了100个m2.4xlarge EC2实例，每个实例有8核和68GB的RAM。我们运行的查询的目的是计算下列数据的总视图：（1）所有页面（2）标题恰好匹配给定词的页面（3）标题部分匹配给定词的页面。每个查询都会扫描整个输入数据。 图14 在100台机器上的Spark交互式查询的响应时间，其扫描的输入数据集越来越大。图14 在100台机器上的Spark交互式查询的响应时间，其扫描的输入数据集越来越大。 \" 图14 在100台机器上的Spark交互式查询的响应时间，其扫描的输入数据集越来越大。 图14展示了分别在完整的数据集、一半的数据集和十分之一的数据上的查询的响应时间。即使在1TB数据下，Spark查询也仅需5~7秒。这比在磁盘上处理数据快乐一个数量级。例如，从磁盘上的文件查询数据需要170秒。这表明RDD使Spark成为了一个强大的数据挖掘工具。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:8:6","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"7. 讨论 尽管RDD因为其不可变的性质和粗粒度的变换，而似乎提供的接口受限，我们发现其适合很广泛的程序类型。特别地，RDD可以表示惊人多的集群编程模型，这些模型目前在各种独立的框架中被提出，这使用户可以将这些模型组合在一个程序中（例如，运行MapReduce操作来建一个图，再在Pregel上运行它）并在这些模型间共享数据。在本章中，我们将讨论RDD可以表示哪些编程模型与为什么RDD适用范围这么广（章节7.1）另外，我们讨论RDD中谱系图信息的我们追求的另一个好处，这有助于跨这些模型进行调试（章节7.2）。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:9:0","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"7.1 表示已存在的编程模型 RDD可以高效地表示大量集群编程模型，这些模型目前被相互独立地提出。这里的高效指RDD不仅可以被用作生产与这些编程模型相同的输出，RDD还可以应用这些框架中使用的优化，如将特定数据保存在内存中、分区以最小化通信、高效地从故障中恢复。这些能被RDD表示的模型包括： MapReduce： 该模型可通过Spark中的flatMap和groupByKey操作表示，如果其有combiner，那么combiner可用reduceByKey表示。 DryadLINQ： DryadLINQ系统通过更通用的Dryad运行时，提供了比MapReduce范围更广的操作，但这些操作都是与Spark中可用的RDD变换对应的批处理操作（map、groupByKey、join等）。 SQL： 像DryadLINQ表达式一样，SQL查询也会对记录的集合执行并行的数据操作。 Pregel： Google的Pregel[22]是为迭代式图程序设计的专用模型，乍一看它与其他系统中面向集合的编程模型有很大的不同。在Pregel中，一个程序作为一系列协调的“超步骤（superstep）”运行。在每个超步骤中，图中的每个顶点运行一个用户函数，用户函数可以更新该顶点相关的状态、改变图的拓扑结构、在下一个超步骤中给其它顶点发送它们要使用的消息。该模型可以表达许多图算法，包括最短路、二分图匹配和PageRank。 让我们通过RDD来实现这个模型的关键是，Pregel在每轮迭代中对所有顶点应用相同的用户函数。因此，我们可以将每轮迭代的顶点状态存储在一个RDD中，并执行批量变换（flatMap）来应用这个函数并生成一个消息RDD。随后我们可以将该RDD与顶点状态进行join以执行消息交换。同样重要的是，RDD可以让我们像Pregel中那样将顶点状态保存在内存中、通过控制它们的分区来最小化通信、在故障发生时支持局部恢复。我们在Spark之上实现了一个200行代码的Pregel库，更多细节请参考[33]。 迭代式MapReduce： 在最近提出的多个系统中（包括HaLoop[7]和Twister[11]），提供了一个迭代式的MapReduce模型，即用户给系统一系列的MapReduce来循环执行。系统在迭代间分区地持久化了数据，且Twister可将数据保存在内存中。这两种优化都可用RDD简单地表达出，并且我们可以使用Spark通过200行代码实现HaLoop库。 流批处理（Batched Stream Processing）： 最近的研究提出了许多为应用程序设计的增量处理系统，它们每隔一段时间使用新数据更新结果[21， 15， 4]。例如，一个应用程序每15分钟更新一次广告点击的统计，在这15分钟的时间窗口中的新数据需要被合并成中间状态。这些系统执行类似Dryad的批量操作，但是在分布式文件系统中保存应用程序状态。将中间状态放在RDD中会加快处理速度。 对RDD表达能力的解释： 为什么RDD能够表达这些各式各样的编程模型？原因在于RDD的限制对于多并行程序来说影响很小。特别是，尽管RDD仅能通过批量变换来创建，但实际上许多并行程序本身就要对大量记录应用相同的操作，这使这些并行程序很容易表达。类似地，因为用户可以创建多个RDD来表示相同数据集的不同版本，所以RDD的不可变性也不会成为阻碍。事实上，当今的许多MapReduce应用程序都运行在不允许更新文件的文件系统上，如HDFS。 最后一个问题是，为什么过去的框架没有提供同等级别的通用性。我们认为这时由于这些系统探索的是MapReduce和Dryad无法很好处理的特殊问题，如迭代等，而没有注意到这些问题的通用原因是缺乏数据共享抽象。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:9:1","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"7.2 利用RDD进行调试 最初我们在设计RDD时，可确定性重新计算的特性是为容错设计的，但同时这一特性也可以为调试提供便利。特别是，通过记录RDD在作业中创建的谱系图，用户可以（1）在作业之后重构这些RDD并可以让用于交互式地查询它们（2）通过重新计算任务依赖的RDD分区，在单线程调试器中重跑作业中的任何任务。RDD不像面向通用分布式系统的传统[13]的重放调试器，传统的重放调试器必须捕获或推断跨多节点的事件顺序，RDD的方法几乎没有增加任何的记录开销，因为其只需记录RDD的谱系图。注9目前，我们正在开发一个基于这些想法[33]的Spark调试器。 注9：RRD与这些系统不同，基于RDD的调试器不会重放用户函数中的不确定行为（例如不确定性的map），但是基于RDD的调试器至少可以报告数据的校验和。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:9:2","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"8. 相关工作 集群编程模型： 集群编程模型中的相关工作分为几类。第一，如MapReduce[10]、Dryad[19]和Ciel[23]之类的数据流模型，它们支持丰富的数据处理操作，但是通过稳定存储共享数据。RDD提出了一个比使用稳定存储更高效的数据共享抽象，因为RDD避免了数据备份、I/O和序列化的开销。注10 注10：主要注意的是，像RAMCloud[25]等在内存式数据库之上运行MapReduce/Dryad，仍需要备份数据和序列化，这对一些应用程序来说开销很大，如章节6.1中所示。 第二，一些数据流系统的高层编程接口（包括DryadLINQ[31]和FlumeJava[8]）提供了集成在语言中的API，用户通过像map、join这样的操作操作“并行集合（parallel collection）”。然而，在这些系统中，并行集合既可以表示磁盘上的文件也可以表示用来表达查询计划的暂时性的数据集。尽管系统会将数据在同一个查询的操作间流水线化（例如在map后再进行一次map），但它们不能在查询间高效地共享数据。因为并行集合模型的方便性，Spark的API也基于它，且语言集成的编程接口没有新奇的特性，但是使用RDD作为该接口下的存储抽象。这样，我们就使其能够支持广泛类型的应用程序。 第三，系统为需要数据共享的特殊类型的应用程序提供了高层的接口。例如，Pregel[22]支持迭代式图应用程序、Twister[11]和HaLoop[7]是迭代式MapReduce运行时。然而，这些框架为他们支持的计算执行隐式数据共享，且没有提供能让用户在他们选择的操作中共享他们选择的数据的通用抽象。例如，用户不能使用Pregel或Twister将数据集加载到内存然后决定在其上执行哪些查询。RDD提供了显式的分布式存储抽象，从而满足专用系统无法满足的应用程序的需求，如交互式数据挖掘。 最后，一些系统暴露了共享的可变状态，使用户可以执行内存式计算。例如，Piccolo[27]让用户可以运行并行函数来读取和更新分布式哈希表的单元格。分布式共享内存（DSM）系统[24]和类似RAMCloud[25]的键值存储提供了相似的模型。RDD与这些系统在两方面不同。第一，RDD提供了基于如map、sort和join等操作的高层编程接口，而Piccolo和DSM中的接口只能读取或更新表的单元格。第二，Piccolo和DSM系统通过检查点和回滚实现恢复，这比RDD的基于谱系图的方式在许多应用程序中开销更昂贵。最后，正如章节2.3中讨论的那样，RDD还比DSM提供了其他的优势，如掉队者缓解等。 缓存系统： Nectar[12]可以通过分析程序来识别公用子表达式[16]的方式在DryadLINQ作业间复用中间结果。如果在基于RDD的系统中添加这种能力将会很诱人。然而，Nectar不提供内存缓存（其将数据放置在分布式文件系统中），也不允许用户显式控制对哪些数据集持久化或如何对数据分区。Ciel[23]和FlumeJava[8]同样可以缓存任务结果，但是不提供内存缓存或显式控制缓存哪些数据。 Ananthanarayanan等人提出在分布式文件系统中加入内存缓存来利用数据访问的时间与空间位置[3]。尽管这种方法为已经在文件系统中的数据提供了更快的访问速度，但它在应用程序中共享中间结果的性能仍没有RDD好，因为它还是需要应用程序在不同阶段间将这些结果写入文件系统中。 谱系（Lineage）： 获取数据的谱系（lineage）或起源（provenance）信息一直是科学计算和数据库领域的研究主题。对于如解释结果的应用程序来说，该研究可以允许结果的解释被其他应用程序重新生产，并在工作流中出现bug或数据集丢失时重新计算数据。读者可以参考[5]和[9]来进一步了解这项工作。RDD提供的并行编程模型使用细粒度的谱系图的开销不是很高，因此可被用作故障恢复。 基于谱系图的恢复机制与在MapReduce和Dryad中的计算（作业）恢复机制相似，其会通过任务的DAG追踪依赖。然而，在这些系统中，当作业结束后谱系信息就会被丢弃，需要借助副本式存储系统来在计算间共享数据。相反，RDD使用谱系关系来在计算间高效地持久化内存数据，不需要副本和磁盘I/O的开销。 关系型数据库： RDD从概念上与数据库的视图很相似，持久化的RDD类似于具体化的（materialized）视图[28]。然而，就像DSM系统一样，数据库通常允许对所有记录进行细粒度的读写访问，为了容错需要记录操作和数据，且需要额外的开销来维护一致性。而RDD的粗粒度变换模型则不需要这些额外开销。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:10:0","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"9. 结论 我们提出了弹性分布式数据集（resilient distributed datasets，RDD），它是一个高效、通用、带有容错的用于在集群应用程序间共享数据的抽象。RDD可以广泛地表达并行程序，其包括很多位迭代式计算而提出的专用编程模型，和在新的应用程序中目前无法表示的模型。不像需要数据副本来进行容错的现有的集群存储抽象，RDD提供了基于粗粒度变换的API，这使RDD可以使用谱系图来高效地恢复数据。我们在被叫做Spark的系统中实现了RDD，其在迭代式应用程序中的性能表现比Hadoop好20倍，且被用作交互式查询数百GB的数据。 我们已经在spark-project.org上开源了Spark，岂可作为可伸缩的数据分析和系统研究工具。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:11:0","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"致谢 感谢Spark的最初的使用者，他们包括Tim Hunter, Lester Mackey, Dilip Joseph, and Jibin Zhan，感谢他们在自己真实的应用程序中试用我们的系统，并提供了很多建议，还在此过程中确定了一些研究挑战。同时感谢我们的领导者，Ed Nightingale和我们的额审稿人，感谢它们的反馈。本研究中部分工作由Berkeley AMP Lab sponsors Google, SAP, Amazon Web Services, Cloudera, Huawei, IBM, Intel, Microsoft, NEC, NetApp and VMWare, DARPA (contract #FA8650-11-C-7136), Google PhD Fellowship, the Natural Sciences and Engineering Research Council of Canada提供支持。 ","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:12:0","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"参考文献 [1] Apache Hive. http://hadoop.apache.org/hive. [2] Scala. http://www.scala-lang.org. [3] G. Ananthanarayanan, A. Ghodsi, S. Shenker, and I. Stoica. Disk-locality in datacenter computing considered irrelevant. In HotOS ’11, 2011. [4] P. Bhatotia, A. Wieder, R. Rodrigues, U. A. Acar, and R. Pasquin. Incoop: MapReduce for incremental computations. In ACM SOCC ’11, 2011. [5] R. Bose and J. Frew. Lineage retrieval for scientific data processing: a survey. ACM Computing Surveys, 37:1–28, 2005. [6] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. In WWW, 1998. [7] Y. Bu, B. Howe, M. Balazinska, and M. D. Ernst. HaLoop: efficient iterative data processing on large clusters. Proc. VLDB Endow., 3:285–296, September 2010. [8] C. Chambers, A. Raniwala, F. Perry, S. Adams, R. R. Henry, R. Bradshaw, and N. Weizenbaum. FlumeJava: easy, efficient data-parallel pipelines. In PLDI ’10. ACM, 2010. [9] J. Cheney, L. Chiticariu, and W.-C. Tan. Provenance in databases: Why, how, and where. Foundations and Trends in Databases, 1(4):379–474, 2009. [10] J. Dean and S. Ghemawat. MapReduce: Simplified data processing on large clusters. In OSDI, 2004. [11] J. Ekanayake, H. Li, B. Zhang, T. Gunarathne, S.-H. Bae, J. Qiu, and G. Fox. Twister: a runtime for iterative mapreduce. In HPDC ’10, 2010. [12] P. K. Gunda, L. Ravindranath, C. A. Thekkath, Y. Yu, and L. Zhuang. Nectar: automatic management of data andcomputation in datacenters. In OSDI ’10, 2010. [13] Z. Guo, X. Wang, J. Tang, X. Liu, Z. Xu, M. Wu, M. F. Kaashoek, and Z. Zhang. R2: an application-level kernel for record and replay. OSDI’08, 2008. [14] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Publishing Company, New York, NY, 2009. [15] B. He, M. Yang, Z. Guo, R. Chen, B. Su, W. Lin, and L. Zhou. Comet: batched stream processing for data intensive distributed computing. In SoCC ’10. [16] A. Heydon, R. Levin, and Y. Yu. Caching function calls using precise dependencies. In ACM SIGPLAN Notices, pages 311–320, 2000. [17] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. Joseph, R. H. Katz, S. Shenker, and I. Stoica. Mesos: A platform for fine-grained resource sharing in the data center. In NSDI ’11. [18] T. Hunter, T. Moldovan, M. Zaharia, S. Merzgui, J. Ma, M. J. Franklin, P. Abbeel, and A. M. Bayen. Scaling the Mobile Millennium system in the cloud. In SOCC ’11, 2011. [19] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad: distributed data-parallel programs from sequential buildingblocks. In EuroSys ’07, 2007. [20] S. Y. Ko, I. Hoque, B. Cho, and I. Gupta. On availability of intermediate data in cloud computations. In HotOS ’09, 2009. [21] D. Logothetis, C. Olston, B. Reed, K. C. Webb, and K. Yocum. Stateful bulk processing for incremental analytics. SoCC ’10. [22] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G. Czajkowski. Pregel: a system for large-scale graph processing. In SIGMOD, 2010. [23] D. G. Murray, M. Schwarzkopf, C. Smowton, S. Smith, A. Madhavapeddy, and S. Hand. Ciel: a universal execution engine for distributed data-flow computing. In NSDI, 2011. [24] B. Nitzberg and V. Lo. Distributed shared memory: a survey of issues and algorithms. Computer, 24(8):52 –60, Aug 1991. [25] J. Ousterhout, P. Agrawal, D. Erickson, C. Kozyrakis, J. Leverich, D. Mazieres, S. Mitra, A. Narayanan, G. Parulkar, ` M. Rosenblum, S. M. Rumble, E. Stratmann, and R. Stutsman。 The case for RAMClouds: scalable high-performance storage entirely in DRAM. SIGOPS Op. Sys. Rev., 43:92–105, Jan 2010. [26] D. Peng and F. Dabek. Large-scale incremental processing using distributed transactions and notifications. In OSDI 2010. [27] R. Power and J. Li. Piccolo: Building fast, distributed programs with partitioned tables. In Proc. OSDI 2010, 2010. [28] R. Ramakrishnan and J. Gehrke. Database Management Systems. McGraw-Hill, Inc., 3 edition, 2003. [29] K.","date":"2020-09-07","objectID":"/posts/paper-reading/rdd-nsdi12-final138/:13:0","tags":["RDD","Translation"],"title":"《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》论文翻译（RDD-NSDI12-FINAL138）","uri":"/posts/paper-reading/rdd-nsdi12-final138/"},{"categories":["Paper Reading"],"content":"本篇文章是对论文XORing Elephants: Novel Erasure Codes for Big Data的原创翻译，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:0:0","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"作者 Sathiamoorthy University of Southern California msathiam@usc.edu Megasthenis Asteris University of Southern California asteris@usc.edu Dimitris Papailiopoulos University of Southern California papailio@usc.edu Alexandros G. Dimakis University of Southern California dimakis@usc.edu Ramkumar Vadali Facebook ramkumar.vadali@fb.com Scott Chen Facebook sc@fb.com Dhruba Borthakur Facebook dhruba@fb.com ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:1:0","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"摘要 大型分布式存储系统通常使用副本来提供可靠性。最近，为了减少三副本系统带来的巨大开销，开始使用纠删码。在设计时，一般选择Reed-Solomon码（RS码）作为标准，其高昂的修复开销往往被认为是为了高效存储和高可用性而带来的不可避免的代价。 本篇论文展示了如何克服这一限制。我们提出了一个新的擦除码族，它们可以高效修复并提供比RS码更高的可靠性。我们通过分析表明，我们的编码在权衡局部性和最短距离时能做出最优的决策。 我们在Hadoop HDFS中实现了我们的新的编码方式，并与当前部署的使用了RS码的HDFS模块做了比较。我们修改的HDFS实现在修复时减少了约2倍的磁盘I/O和网络流量。新的编码方案的缺点是在修复时需要修复比RS码多出14%的存储，这是从信息论角度为了获得部性最优的开销。因为我们新的编码方案能够更快地修复故障，因此其能够提供比副本的方式高几个数量级的可靠性。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:2:0","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"1. 引言 MapReduce架构因其高伸缩性而在大数据管理中心变得越来越流行。在Facebook中，大型分析集群存储了PB级信息并使用Hadoop MapReduce处理很多的分析任务。其标准实现依赖一个通过利用了三副本的块来提供可靠性的分布式文件系统。副本策略的主要缺点在于其需要高达200%的额外存储开销，这一开销会反映在集群的开销上。当管理的数据快速增长时，这一开销会取代数据中心基础设施成为主要的瓶颈。 因此，Facebook和许多其他厂商正在切换到纠删码技术（通常指RS码）来在节约存储[4, 19]的同时引入冗余，特别对于那些更像是归档的数据。在本文中，我们展示了传统的编码在分布式的MapReduce架构中离最优有很大差距。我们介绍了新的编码方式，以解决分布式系统可靠性和信息论约束的主要挑战，这也显示了我们的结构是最优的。本文依赖于对一个使用了Hadoop MapReduce来做数据分析的大型Facebook生产集群（超过3000个节点、30PB的逻辑数据存储）的测量。Facebook最近开始部署一个依赖RS码的叫做HDFS RAID[2, 8]的开源HDFS模块。在HDFS RAID中，“cold（即很少被访问的）”文件的副本因子被降为1，并为其创建一个包含奇偶块的新的奇偶文件。 Facebook集群中使用的参数为，每个大文件的数据块被分组为10个条带（stripe），并对每个条带创建了4个奇偶校验块。太系统（被称为RS）可以容错任意4个块的故障，且其额外开销仅为40%。因此，RS码能提供比副本更强的健壮性和存储性能。事实上，该方案存储的额外开销对于该级别的可靠性来说是最小的[7]。实现了这种最佳的存储和可靠性折衷的编码被称作是Maximum Distance Separable（MDS）[31]的，RS码就[27]是MDS族中被最广泛使用的编码方式。 传统的纠删码在分布式环境中不是最优的，这时由于修复问题（Repair problem）：当一个节点故障时，通常每个条带中被存储在该节点上的一个块会丢失。即使仅有一个块丢失，RS码通常会使用需要传输10个块并重建这10个数据块的原始数据的方式的简单方案来修复[28]，这导致了在修复时产生了10倍的对带宽和磁盘I/O的额外负载。 最近，信息论的研究结果表明，和这种朴素方式[6]相比，能够使用更少的网络带宽来修复纠错码。最近已经有大量关于设计这种高效的可修复的编码的工作，在第六章中有对这些文献的概览。 **我们的贡献：**我们介绍一个新的擦除码族——Locally Repairable Codes（LRCs），其可以在网络带宽和磁盘I/O方面进行高效修复。分析表明，我们的编码方案在局部性方面，从信息论的角度是最优的。即，修复单块故障所需的其他块数量是最优的。我们从广义的RS奇偶校验触发，提出了随机化和显式LRC结构的两种方案。 我们还设计并实现了HDFS-Xorbas，这是一个使用LRCs来替换HDFS-RAID模块中RS码的模块。我们通过使用Amazon EC2和一个Facebook中的集群的经验来评估HDFS-Xorbas。需要注意的是，尽管LRCCs可以在任何数量条带和任意大小的奇偶校验块上使用，我们的实验还是基于RS(10,4)和将其扩展到(10,6,5)的LRC与当前生产集群比较。 我们的实验表明，Xorbas比正在生产环境中使用的RS码大概减少了2倍的磁盘I/O和修复时的网络流量。这种新编码方案的缺点在于其需要比RS多14%的存储，为了获取局部性，这一开销从信息论的角度看已经是最优的了。 一个很有趣的好处是：Xorbas由于能在读取降级时提供更高效的性能表现，因此能够更快地修复故障，这提供了更高的可用性。在一个简单的Markov（马尔可夫）模型的评估下，Xorbas的平均数据丢失时间（Mean Time to Data Loss，MTTDL。译注：存储设备从运行开始到因故障导致数据永久丢失的平均时间）比RS(10,4)多了2个0，比3副本多了5个0。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:3:0","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"1.1 修复的重要性 在Facebook，大型数据分析集群存储这PB级的数据并处理多个MapReduce分析任务。在有3000个节点的生产集群中存储了大概2亿2千万个块（每个块256MB），目前仅8%的数据正在使用RS编码（使用\"RAID”）。图1展示了对这个生产集群中最近的节点故障的跟踪。即使在已经推迟大部分的修复任务以避免对暂时性故障进行不必要的修复的情况下，通常每天仍会有20个或更多的节点故障并触发修复任务。一个数据节点通常会存储约15TB的数据，在当前的集群配置下，其修复时需要的流量大概占集群每日网络流量（2PB/day）的10%到20%。就像我们之前讨论过的，(10,4)RS编码修复每位时需要比冗余块的方式多出10倍的网络流量。我们估计当集群中50%的数据使用RS编码时，其修复占用的网络流量会使集群的网络连接完全饱和。我们的目标是设计一个更高效的编码方案，使能够在不遇到修复瓶颈的情况下允许更高比例的数据使用该编码方案。这将会节约数PB的存储额外开销并能够大大较小集群的开销。 图1 Facebook中由3000个节点组成的生产集群中一个月内节点故障数量图1 Facebook中由3000个节点组成的生产集群中一个月内节点故障数量 \" 图1 Facebook中由3000个节点组成的生产集群中一个月内节点故障数量 高效的可修复的编码方案在编码存储系统中变得越来越重要的原因还有4点。原因一，是读取降级（degraded read）问题。无永久性数据丢失的瞬时错误占数据中心错误事件的90%[9, 19]。当瞬时性错误发生时，如果一个编码了的条带的相关数据块不可用时，对该块的读取会被降级（degraded）。在这种情况下，丢失的数据块可以被修复进程重建，其目的并不在于容错，而是为了更高的数据可用性。这与标准的数据修复的区别在于，为了可用性而重建的块不需要被写入磁盘。因此，高效且快速的修复可以大幅提高数据可用性。 原因二，是高效节点退役（node decommissioning）问题。Hadoop提供了退役特性来使一个故障的数据节点退出。在该节点退役前，需要将基本数据拷贝出该节点，这个过程复杂且耗时。快速修复像对待计划性的修复一样来对待节点退役的情况，并启动一个MapReduce任务在不造成大量网络流量的情况下重新创建块。 原因三，是修复操作对其它并发的MapReduce任务的影响。一些研究表明MapReduce的主要瓶颈是网络[5]。正如我们提到的那样，修复占用的网络流量会在集群目前的网络带宽中占用客观的比例。使用的存储空间比数据中心的网络带宽增长得快得多，因此修复的网络占用问题会变得更加严重。存储密度的增长趋势让使用编码时能够局部修复变得更加重要。 最后，局部修复是促进跨数据中心的地理性（geographically）分布式文件系统的关键因素。多地（Geo-diversity）已被认为是未来改善时延和可靠性的关键方向之一[13]。在传统方式中，站点通过副本的方式跨数据中心来分布式存储数据。然而，这种方式显著地增加了总存储开销。因为这种规模下跨地理位置的RS码需要大量的广域网带宽，所以这是完全不切实际的。我们的工作能在稍微提高存储额外开销的情况下使局部修复成为可能。 显然，采用副本的方式优化以上四个问题会更好，但是其需要更大的额外存储开销。相反，MDS码能够在给定的可靠性需求下使用最少的存储开销，但是会在修复方面和以上提到的问题中很困难。一种审视这篇文章的贡献的方式为：本文提出了该问题的一个新的中间权衡点，其牺牲了一定的存储效率以获取其他的指标。 本文的剩余部分按照以下方式组织：我们首先给出了我们的理论结果，即局部可修复编码（Locally Repairable Codes，LRC）的构造和从信息论角度的理论最优性结果。我们将更多技术性的证明放在了附录中。第三章展示了HDFS-Xorbas架构，第四章讨论了基于Markov的可靠性分析。第五章讨论了我们在Amazon EC2和Facebook中的集群的实验评估。最后，我们在第六章中调查了相关工作，并在第七章中进行了总结。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:3:1","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"2. 理论贡献 极大距离可分码（Maximun distance sparable codes， MDS codes）经常在各种应用程序的通信和存储系统中被使用[31]。一个比例为$R=\\frac{k}{n}$的$(k,n-k)$-MDS码注1将一个大小为$M$的文件划分为$k$个大小相等的块，并随后将其编码为$n$个每个大小为$\\frac{M}{k}$的编码块。这里我们假设我们的文件大小和$k$个数据块的大小恰好相等以简化形式；大文件会被划分为多个条带，每个条带中有$k$个数据块，且每个条带都会被分别编码。 注1：在传统的编码理论文献中，编码被表示为$(n,k)$。其中$n$为数据块数量加奇偶校验块数量，通常被成为块长度（blocklength）。$(10,4)$RS码传统上被表示为RS$(n=14,k=10)$。所有RS码组成了最有名的MDS编码族。 $(k,n-k)$-MDS码可以保证$n$个编码块中的任意$k$个都可以被用来重建整个文件。易证，这是该冗余级别下可能实现的最佳容错条件：任意$k$个块的集合总大小为$M$，因此没有能够覆盖该文件的更小的块的集合。 容错能力可以通过最小距离这一指标来衡量。 定义 1 （最小编码距离，MINIMUM CODE DISTANCE）： 长度为$n$的编码的最小距离$d$，等于使文件不可恢复时，最少被擦除的块数。 顾名思义，MDS码有着能实现的最大距离，其$d_{MDS}=n-k+1$。例如，(10,4)RS的最小距离为$n-k+1=5$，这意味着想要数据丢失需要擦除5个或更多的块。 我们将关注的第二个指标为块的局部性。 定义 2 （块的局部性，BLOCK LOCALITY）： 当每个被编码的块是最多$r$个使用了该编码的其他被编码的块的函数时，那么这个$(k,n-k)$编码的块局部性为$r$。（译注：即对于一种编码，如果被编码的每个块都可以被最多$r$个使用了该编码的其他块通过运算表示，那么这个编码的局部性为$r$。） 有局部性$r$的编码有这样的属性：当任意一个块被擦除时，可以通过计算$r$个存在的使用该编码的块来快速修复丢失的编码块。这一概念最近在[10, 22, 24]中引入。 当我们需要较小的局部性时，每一个编码块需要可以通过已存在的编码块的较小的子集来修复。即，即使$n$,$k$增大，仍需要$r \\ll k$。以下事实显示了局部性和较好的距离冲突： 引理 1： 有参数$(k,n-k)$的MDS编码不能有比$k$更小的局部性。 引理1意味着MDS的局部性最差，因为其需要任意$k$个块才能重建整个文件，而不能仅重建一个块。即其局部性恰好是其最佳容错开销。 自然，我们需要解决的问题是：对于一个有着与MDS几乎相同的编码距离的编码，其可能的最佳局部性是多少。我们回答了这个问题，并构造了第一个有着高局部性的与MDS编码距离相近的编码族。我们提供了一个随机的和显式的编码族，其在所有的块上有对数的局部性，并有与MDS编码渐进相等的编码距离。我们称这些编码为$(k,n-k,r)$Locally Repairable Codes（LRCs，局部可修复编码），并在后续部分中介绍它们的构造。 定理 1： 存在$(k,n-k,r)$LRC，其有对数的块局部性$r=log(k)$，且编码距离$d_{LRC}=n-(1+\\delta_{k})k+1$。因此，任何有$k(1+\\delta_{k})$个编码块的子集都可被用来重建文件，其中$\\delta_{k}=\\frac{1}{log(k)}-\\frac{1}{k}$。 显然，如果我们固定LRC的编码率$R=\\frac{k}{n}$并增大$k$，那么其编码距离$d_{LRC}$几乎与$(k,n-k)$-MDS编码相同。因此有如下推论。 推论 1： 对于固定的编码率$R=\\frac{k}{n}$，LRCs有着与$(k,n-k)$-MDS编码渐进相等的编码距离。 $$ \\lim\\limits_{k\\to\\infty}\\frac{d_{LRC}}{d_{MDS}}=1 $$ LRCs构建在MDS编码之上（最常见的选择为RS码）。 将MDS编码的块分为对数大小的集合然后组合在一起后，可以得到有着对数的度（degree）的奇偶检验块。由于我们建立的信息论的权衡，我们证明了LRCs对给定的局部性有着最优的编码距离。我们的局部性-编码距离折衷十分普遍，因为其覆盖了线性编码和非线性编码，且其是对Gopalan等人最近的成果的推广[10]，他们的成果为线性的编码建立了一个相似的边界。我们证明的方法基于建立了一个类似Dimakis等人的工作中信息流图[6, 7]。我们的分析可在附录中找到。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:4:0","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"2.1 Xorbas中LRC的实现 现在我们将描述我们在HDFS-Xorbas中实现的一个显式$(10,6,5)$LRC码。对于每个条带，我们从有10个数据块$X_{1},X_{2},…,X_{10}$并在二进制的扩展字段$\\mathbb{F}_{2^{m}}$上使用$(10,4)$RS码来构造4个奇偶校验块$P_{1},P_{2},…,P_{4}$。这是目前在Facebook生产集群中使用的编码方案，由于RS奇偶校验，其可以容忍任意4个块的故障。LRCs的基本想法非常简单：通过增加额外的局部奇偶校验来使修复变得高效。我们通过**图2**展示了这一点。 图2 HDFS-Xorbas中实现的LRC。4个奇偶校验块$P_{1},P_{2},P_{3},P_{4}$通过标准RS码构造，局部的奇偶校验为单块故障提供了高效的修复能力。主要的理论挑战是如何选择系数$c_{i}$来最大化编码的容错能力。图2 HDFS-Xorbas中实现的LRC。4个奇偶校验块$P_{1},P_{2},P_{3},P_{4}$通过标准RS码构造，局部的奇偶校验为单块故障提供了高效的修复能力。主要的理论挑战是如何选择系数$c_{i}$来最大化编码的容错能力。 \" 图2 HDFS-Xorbas中实现的LRC。4个奇偶校验块$P_{1},P_{2},P_{3},P_{4}$通过标准RS码构造，局部的奇偶校验为单块故障提供了高效的修复能力。主要的理论挑战是如何选择系数$c_{i}$来最大化编码的容错能力。 通过添加局部奇偶校验块$S_{1}=c_{1}X_{1}+c_{2}X_{2}+c_{3}X_{3}+c_{4}X_{4}+c_{5}X_{5}$，单块故障可以通过仅访问另外5个块来修复。例如，如果块$X_{3}$丢失（或在不可用时被读取降级）其可以通过下式重建。 $$X_{3}=c_{3}^{-1}(S_{1}-c_{1}X_{1}-c_{2}X_{2}-c_{4}X_{4}-c_{5}X_{5}) \\tag {1}$$ 只要$c_{3}$不为0其倒数就存在，这时我们对所有局部奇偶校验块的系数的要求。我们可以通过选择$c_{i}$使所有线性方程线性无关。在附录中我们提出了一个随机的和一个确定的算法来构建这些系数。需要强调的是，确定性的算法的复杂度与编码的参数$(n,k)$呈指数关系，因此其仅在小规模的编码构造中有用。 添加这些局部奇偶校验的缺点是需要额外的存储。原本使用RS码需要为每10个块存储14个块，而3个局部奇偶校验块将存储开销增加到了$17/10$。有一个额外优化我们可以实现：因此系数$c_{1},c_{2},…,c_{10}$可被选取，所以我们可以使局部奇偶检验块满足一个额外的校准方程$S1+S2+S3=0$。因此我们可以不存储局部奇偶校验块$S3$而将其看做是一个隐式奇偶校验块。需要注意的是，为了获得这一性质，我们需要置$c_{5} ' +c_{6} ' = 1$。 当RS奇偶校验块中的一个块发生故障时，可以重建隐式校验块并用其修复故障。例如，如果$P_{2}$丢失，可以通过读取$P_{1},P_{3},P_{4},S_{1},S_{2}$这5个块并通过如下等式来恢复该块。 $$ P_{2}=( c_{2} ' )^{-1}(-S_{1}-S_{2}-c_{1} ' P_{1}-c_{3} ' P_{3}-c_{4} ' P_{4}) \\tag {2} $$ 在我们的理论分析中，我们展示了如何找到能使条件成立的非零的系数$c_{i}$（其必须依赖$P_{i}$，但不依赖数据）。我们童颜展示了HDFS RAID中的RS码中的实现方式，其选择了$c_{i}=1 \\forall i$，这是能够执行简单的XOR（异或）操作的充分条件。我们进一步证明了当给定局部性$r=5$且块长度$n=16$时的最大可行距离（$d=5$）。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:4:1","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"3. 系统描述 HDFS-RAID是一个在Apache Hadoop[2]上实现了RS编码和解码的开源模块。其提供了一个运行在HDFS之上的分布式RAID文件系统（Distributed Raid File System，DRFS）。存储在DRFS上的文件被划分为条带，即多个块的组。其对于每个条带都计算了一些奇偶校验块，且将其作为与原始文件对应的单独的奇偶校验文件存储。HDFS-RAID通过Java实现（大约12000行代码），且目前在包括Facebook的多个组织的生产环境中使用。 该模块由多个部件组成，其中最相关的是RaidNode和BlockFixer： RaidNode是负责创建和维护存储在DRFS中的所有数据文件的奇偶检验文件的守护进程。集群中的一个节点通常被设计为运行一个RaidNode。守护进程每隔一段时间会扫描HDFS文件系统并根据文件的大小和年龄来决定其是否需要使用RAID。在大型集群中，RAID是通过分布式的方式实现的，即将MapReduce任务分配给集群中的节点。在编码后，RaidNode会将使用了RAID的文件的副本等级降为1. BlockFixer是一个运行在RaidNode上的独立进程，其每个一段时间会使用了RAID的文件的丢失或损坏的块。当块被标记为丢失或损坏时，BlockFixer会使用其条带中幸存的块来重建它们，同样，这也是通过分配MapReduce修复任务实现的。需要注意的是，修复任务不是典型的MapReduce任务。修复任务是在MapReduce框架的下层实现的，其可以充分利用并行和调度属性，能够在一个控制机制下运行多个常规的任务。 RaidNode和BlockFixer都依赖下层组件ErasureCode。ErasureCode实现了纠错码（即擦除码）的编码和解码功能。在Facebook的HDFS-RAID中，我们通过ErasureCode实现了一个$(10,4)$RS纠错码（为每10个数据块创建4个奇偶校验块）。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:5:0","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"3.1 HDFS-Xorbas 我们的系统HDFS-Xorbas（也简称为Xorbas）是基于HDFS-RAID的修改，其合并了LRC。为了便于它和HDFS-RAID中实现的RS码，我们称后者为HDFS-RS。Xorbas中继承了ErasureCode类，并在传统的RS码智商实现了LRC。为了利用新的编码方案，我们也对RaidNode和BlockFixer类做了修改。 HDFS-Xorbas被设计为可以在大型Hadoop数据仓库中部署，如Facebook的集群。为此，我们的系统提供了向后兼容性：Xorbas同时支持LRC和RS编码，且可以通过仅添加局部XOR奇偶校验文件的方式将RS编码的文件增量修改为LRC编码的文件。为了与HDFS-RS继承，我们使用的特定LRCs被设计为Facebook中使用的$(10,4)$RS码的扩展。首先，文件会被使用RS码编码，接着会为其创建提供局部修复能力的额外的局部奇偶校验块。 3.1.1 编码 一旦RaidNode（根据配置文件中设置的参数）检测到一个文件适合RAID，，它会启动该文件的编码器。编码器首先将文件划分为多个每个中有10个块的条带，并为其计算出4个RS奇偶校验块。其中，最后一个条带中可能包含少于10个块，这取决于文件的大小。对奇偶计算来说，不完整的条带会被看作是被0填充了的满的条带。 HDFS-Xorbas为每个条带的总计16个块计算2个额外的奇偶校验块（即10个数据块，4个RS奇偶校验块和2个局部XOR奇偶校验块），如图2所示。与RS奇偶校验块的计算类似，Xorbas通过分布式的方式计算所有的奇偶校验块，即MapReduce编码任务。所有的块会根据Hadoop中配置的块放置策略（block placement policy）被分散到集群中。默认的策略为随机将块放置到DataNode上，并避免同一条带上的块分配到同一个DataNode上。 3.1.2 解码与修复 当检测到损坏的文件时，RaidNode会启动一个解码进程。Xorbas使用了两个解码器：轻量级的解码器用来处理每个条带中单个块的故障，重量级的解码器会在轻量级解码器处理失败时被使用。 当BlockFixer检测到丢失（或损坏）的块时，它会根据LRC的结构决定使用哪5个块来重建该块。之后会分配一个特殊的MapReduce任务来进行轻量级的解码：单个map任务向包含所需的块的节点打开并发的流，并下载这些块，再执行一个简单的XOR操作。对于多块故障，所需的5个块可能不可用。在这种情况下，轻量级解码器会失败并启动重量级解码器。重量级解码器使用与RS相同的操作：打开对该条带所有块的流，并通过等价于解一个线性方程组的方式解码。RS线性系统具有Vandermonde（范德蒙）结构[31]，这可以减少对CPU的利用。被恢复的块会按照集群块放置策略最终被发送并存储到一个Datanode中。 在当前部署的HDFS-RS的实现中，及时当仅有一个块损坏时，BlockFixer也会打开到该条带中其他的所有13个块的流（在更高效的实现中也可以将这个数量减少到10个）。因此，Xorbas的优势十分明显：对于所有单块故障和许多两个块故障（即两个丢失的块属于不同的局部XOR组中时）的情况，网络和磁盘I/O的开销会小的多。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:5:1","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"4. 可靠性分析 在本章中，我们会通过标准的马尔科夫模型估算平均数据丢失时间（MTTDL）来提供可靠性分析。我们通过上述的指标和模型将RS、LRCs与副本的方式进行了对比。影响MTTDL的主要因素有两个： $i)$在数据丢失前我们能够容忍的故障块的数量和 $ii)$修复块的速度。容错能力越强，MTTDL越高，修复块所需的时间越短。接下来，我们将探索这些因素的相互影响及它们对MTTDL的影响。 在不同策略的对比中，副本策略能够在较低的容错开销下提供最快的修复速度。另一方面，RS码和LRCs能够容忍更多故障，但相比需要更长的修复时间，其中LRC比RS需要的修复时间短。在[9]中，作者展示了Google集群中的数据，并报告说在他们的参数下，$(9,4)$-RS码能够提供比3副本策略高出约6个数量级的可靠性。同样，在这我们也将看到编码的方式如何在我们关注的可靠性方面由于副本策略。 正如[9]中所述，目前有大量分析了副本、RAID存储[32]和纠删码[11]的可靠性的工作。这些文献的主要部分采用了标准的马尔科夫模型分析推导各种存储设置的MTTDL。和这些文献一样，我们也采用了一个类似的方法估算我们对比的策略的可靠性。这里得到的数据孤立地看可能没有意义，但在对比不同策略时非常有用（参见[12]）。 在我们的分析中，$C$表示集群中总数据量，$S$表示条带大小。我们设磁盘节点数量$N=3000$，数据存储总量$C=30PB$。每个磁盘节点的平均故障时间为4年（$=1/ \\lambda $），块大小$B=256MB$（Facebook数据仓库的默认值）。基于对Facebook集群的测量，我们限制修复时跨机架的通信速率$ \\gamma = 1Gbps $。添加这一限的目的是模拟现实中Facebook集群跨机架通信的带宽限制。在我们的条件下，跨机架通信来自于同一个条带所有不同的编码块都被放置在了不同的机架上，以提高容错能力。这意味着当修复单个块时，参与修复的所有的块都会被从不同的机架下载。 在3副本策略下，每个条带由这三个副本的对应的三个块组成，因此系统中条带的总数量为$C/nB$，其中$n=3$。当使用RS码或LRC码时，条带的大小会根据编码的参数$k$和$n-k$变化。为了进行比较，我们认为每个数据条带大小$k$=10。因此，条带的数量为$C/nB$，其中对$(10,4)$RS来说$n=14$，对$(10,6,5)$-LRC来说$n=16$。对于以上值，我们计算单个条带的MTTDL（$MTTDL_{stripe}$）。随后，我们通过之前计算的总条带数对其归一化，得到系统的MTTDL，其计算方式如下。 $$ MTTDL = \\frac{MTTDL_{stripe}}{C/nB} \\tag {3} $$ 接下来，我们解释如何计算一个条带的MTTDL，对其我们可以使用标准的马尔科夫模型。每次失去的块的数量用来表示马尔科夫链的不同状态。故障和修复率对应两个状态间的正向转移和反向转移概率。当我们使用3副本策略时，在3个块被擦除后会发生数据丢失。对于$(10,4)$-RS和$(10,6,5)$-LRC策略，5个块被擦除后会导致数据丢失。因此，以上存储场景的马尔科夫链分别总计有3、5、5个状态。在图3中，我们展示了$(10,4)$-RS和$(10,6,5)$-LRC对应的马尔科夫链。我们注意到，尽管这两个链有相同的状态数，但是状态转移概率会是不同的，其取决于编码策略。 图3 用来计算$(10,4)$RS和$(10,6,5)$LRC的$MTTDL_{stripe}$的马尔科夫模型。图3 用来计算$(10,4)$RS和$(10,6,5)$LRC的$MTTDL_{stripe}$的马尔科夫模型。 \" 图3 用来计算$(10,4)$RS和$(10,6,5)$LRC的$MTTDL_{stripe}$的马尔科夫模型。 我们接下来计算状态转移概率。假设发生故障的间隔时间呈指数分布。修复时间也是如此。通常，修复时间可能不服从指数分布，然而，这样假设可以简化我们的分析。当条带中还有$i$个块时（即，当状态为$n-1$时），失去一个块的概率$ \\lambda _{i} = i \\lambda $，因为这$i$个块分布在不同节点上，且每个节点故障事件是独立的，其概率为$\\lambda$。块被修复的概率取决于修复需要下载多少个块、块大小和下载速率$\\gamma$。例如，对于3副本策略，修复单个块需要下载一个块，因此我们假设$\\rho _{i} = \\gamma / B $，其中$i=1,2$。对于编码策略，我们需要额外考虑使用轻量级和重量级编码器的影响。以LRC为例，如果两个相同条带的块丢失，我们决定调用轻量级和重量级编码器的概率，然后计算需要下载的块数的期望。受篇幅所限，我们跳过详细的推导。相似的做法可参见[9]。条带的MTTDL等于其从状态0到数据丢失状态的平均时间。在以上的假设和状态转移概率下，我们来计算条带的MTTDL，这样就可以通过**公式(3)**计算系统的MTTDL。 表1 对比三种策略的总结。MTTDL假设节点故障是独立事件表1 对比三种策略的总结。MTTDL假设节点故障是独立事件 \" 表1 对比三种策略的总结。MTTDL假设节点故障是独立事件 我们在马尔可夫模型下计算得到的副本策略、HDFS-RS和Xorbas的MTTDL的值如表1所示。我们观察到，在可靠性方面，LRC较高的修复速度弥补了需要额外存储的不足。这让Xorbas LRC$(10,6,5)$比$(10,6)$RS码的可靠性多出了两个0。三副本策略的可靠性比两种编码策略的可靠性低得多，这与相关工作[9]中观测到的结果相似。 另一个有趣的指标是数据可用性。可用性是数据可用时间的比例。需要注意的是，在三副本策略中，如果一个块丢失，该块的其它副本之一会立刻变得可用。相反，无论对于RS还是LRC来说，需要丢失了块的任务必须等修复任务执行完成。因为LRCs在读取降级后读取速率相对更快，所以能更快地完成这些任务，因此它们有更高的可用性。对编码存储系统权衡可用性的详细研究仍是未来中有趣的研究方向。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:6:0","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"5. 性能评估 在本章中，我们提供了我们为了在两个环境下（Amazon’s Elastic Compute Cloud (EC2)[1] 和Facebook中的一个测试集群）评估HDFS-Xorbas的性能的而开展的一系列实验的详细情况。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:7:0","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"5.1 评估指标 我们主要依赖以下指标来评估HDFS-Xorbas和HDFS-RS：HDFS读取字节数、网络流量和修复时间。HDFS读取字节数对应修复任务发起的总数据读取量。该值通过聚合故障事件后的任务分析报告的部分的测量值得到。网络流量代表了集群中节点间数据通信总量（按GB测量）。因为集群不处理任何额外的流量，所以网络流量等于了节点数据移动的总量。修复时间通过修复任务的开始时间和结束时间简单计算得到。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:7:1","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"5.2 Amazon EC2 在EC2我们创建了两个Hadoop集群，其中一个运行HDFS-RS，另一个运行HDFS-Xorbas。每个集群由51个m1.small类型的实例组成，每个实例对应了一个32位机器，有1.7GB内存、1个计算单元、160GB的存储，运行着Ubuntu/Linux-2.6.32。每个集群中的1个实例作为master，运行着Hadoop的NameNode、JobTracker和RaidNode守护进程；剩下的50个实例作为HDFS和MapReduce的slave，每个实例上运行着一个DataNode和一个TaskTracker守护进程，从而形成了一个总容量约为7.4TB的Hadoop集群。不幸的是，EC2没有提供集群的拓扑信息。两个集群初始时装载了等量的逻辑数据。随后在两个集群中手动触发相同的故障模式，一眼就数据恢复的动态情况。实验目标是测量修复时如HDFS读取字节数、真实的网络流量等关键属性。 实验使用的所有文件大小都为640MB。块大小被配置为64MB，在HDFS-RS和HDFS-Xorbas中每个文件产生的每个条带中分别有14和16个满大小的块。这种选择代表了Hadoop生产集群中大部分条带的情况：非常大的文件被分成多个条带，因此大小较小的条带只占了总数中很小的比例。另外，这允许我们更好地预测为了重建丢失的块所需的总数据读取量，从而解释我们的实验结果。最后，因为快的修复仅依赖同一条带，使用会生成多个条带的大文件不会影响我们的结果。章节5.3中讨论了一个关于任意文件大小的实验。 在一次实验期间，所有的文件都使用了RAID，每个集群中都出发了8次故障事件。故障事件包括杀死一个或多个DataNode运行。在我们的故障模式中，前4个故障事件中杀死了1个DataNode，接下来两个故障事件杀死了3个DataNode，最后两次杀死了两个DataNode。当故障事件发生时，RaidNode通过发起的MapReduce修复任务来恢复丢失的块。我们为两个集群提供了足够的时间来完成修复进程，以能对每个独立的事件进行测量。例如，图4中的事件是独立的。需要注意的是在两个集群中被选取杀死的DataNode种大概存储了相同数量的块。实验的目标是对比两个系统修复每个丢失的块的开销。然而，因为Xorbas有着额外的存储开销，一个随机的故障事件可能会导致Xorbas失去块的数量比RS多出14.3%。在任何情况下，对于考虑了这个开销的情形结果仍然适用，在我们的实验中不会观察到显著的影响。 为了了解数据量对系统性能的影响，我们在上述设置下进行了三个实验，依次增加存储的文件数量（50、100和200个文件）。图4中给出了最后一种条件下的测量结果，另外两次实验产生了类似的结果。所有实验的测量结果被合并在了图6中，其绘制了EC2中所有三次实验中的HDFS字节读取量、网络流量、修复时间与丢失的块的数量。我们还绘制了这些测量值的线性最小二乘拟合曲线。 图4 200次文件实验中的测量指标。由于网络输入和网络输出相似，因此没在图中显示。在实验期间，我们模拟了8次故障事件，x轴给出了每次故障事件中杀死的DataNode数量，括号中显示了丢失的块的数量。图4 200个文件的实验中的测量指标。由于网络输入和网络输出相似，因此没在图中显示。在实验期间，我们模拟了8次故障事件，x轴给出了每次故障事件中杀死的DataNode数量，括号中显示了丢失的块的数量。 \" 图4 200次文件实验中的测量指标。由于网络输入和网络输出相似，因此没在图中显示。在实验期间，我们模拟了8次故障事件，x轴给出了每次故障事件中杀死的DataNode数量，括号中显示了丢失的块的数量。 图5 两个EC2集群中故障事件序列按时间的测量值。图5 两个EC2集群中故障事件序列按时间的测量值。 \" 图5 两个EC2集群中故障事件序列按时间的测量值。 图6 故障事件测量点与相应事件中丢失的锁的总数。测量值来自所有三次实验。图6 故障事件测量点与相应事件中丢失的锁的总数。测量值来自所有三次实验。 \" 图6 故障事件测量点与相应事件中丢失的锁的总数。测量值来自所有三次实验。 5.2.1 HDFS字节读取数 图4a描述了在每次故障发生时由BlockFixer发起的HDFS字节读取总量。条形图表明，在重构相同数量的丢失的块时，HDFS-Xorbar的数据读取量为RS的41%~52%。考虑到每个块上很少有超过1个块丢失，这些测量值符合理论上的期望值（$12.14/5=41%$）。图6a展示了HDFS字节读取数与块丢失数的线性关系，这正与我们预期的一样。其斜率即为Xorbas和HDFS-RS中平均每个块读取的HDFS字节数。平均每个丢失的块读取的字节数估算的值分别为11.5和5.8，这表明了HDFS-Xorbas有2倍的优势。 5.2.2 网络流量 图4b描述了在整个修复过程中BlockFixer任务产生了网络流量。特别地，其展示了集群中所有实例的网络出流量的聚合。因为集群仅在内部通信，因此网络的出流量和入流量相似。在图5a中，我们展示了在200个文件的实验期间每5分钟的网络流量图。故障事件序列清晰可见。在我们整个实验中，我们始终都能观察到网络流量大概等于读取字节数的两倍。因此，正如我们预期的那样，HDFS读取的字节数的增加会转化为网络流量的增加。 5.2.3 修复时间 图4c描述了整个恢复过程的时间（即从第一个块的修复任务开始到最后一个块修复任务终止的时间 ）。图6c结合了所有实验中的测量值结果，其展示了修复时间和修复的块的数量的关系。这些图显示Xorbas完成时间比HDFS-RS快了25%~45%。 这两个系统的流量峰值不同的这一事实，表明了实验中可用的带宽没有完全饱和。在大规模MapReduce任务中[5, 14, 15]，网络通常是瓶颈。在Facebook的生产集群中大规模修复发生时，也会观测到相似的情况。这是因为数百台机器可能共享同一个顶层交换机，且该交换机饱和了。由于LRC传输的数据量少得多，我们预计网络瓶颈会导致大规模的RS修复完成时间进一步推迟，也因此LRC在恢复时间上有比RS更大的优势。 从CPU利用率的图表中我们可以得出结论：HDFS-RS和Xorbas有着相似的CPU需求，且这似乎不会影响修复时间。 5.2.4 修复下的工作负载 为了掩饰修复的性能对集群负载的影响，我们在执行着其他任务的集群中模拟了块的丢失。我们创建了两个集群，每个包含了15个slave。由人工提交的工作负载包含5个在同一个3GB文本文件上运行的word-count工作（job）。每个工作（job）包含了许多任务（task），这些任务足以占据所有的计算槽（slot）。Hadoop的FairScheduler将任务分给TaskTracker使各个工作的计算时间能被公平地分享。图7描述了两种情形下每个工作的执行时间： $i)$所有被请求的块都可用，和 $ii)$请求的块的最多20%丢失。不可用的块必须在重建后才能被访问，这增加了任务完成的延时。在HDFS-Xorbas中，这个延时要小得多。在已进行过的实验中，RS中由于块丢失而导致的额外的延时比LRC的二倍还多（LRC中为9分钟，RS中为23分钟）。 图7 在没有块丢失和最多20%块丢失时两个集群上完成10个WordCound的时间。虚线表示任务平均完成时间。图7 在没有块丢失和最多20%块丢失时两个集群上完成10个WordCound的时间。虚线表示任务平均完成时间。 \" 图7 在没有块丢失和最多20%块丢失时两个集群上完成10个WordCound的时间。虚线表示任务平均完成时间。 我们注意到，这些优势主要取决于Hadoop FairScheduler的配置方式。如果并发的工作被阻塞，但是调度器仍为他们分配了槽，那么延时会大幅增加。而且，修复时间超过阈值，那么需要读取块的工作可能会失败。在这些实验中，调度配置的选项是按照对RS最有利的方式设置的。最后，像之前讨论过的那样，由于网络饱和，我们预期在大规模的实验中，LRCs会比RS快的更多。 表2 修复对工作负载的影响表2 修复对工作负载的影响 \" 表2 修复对工作负载的影响 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:7:2","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"5.3 Facebook的集群 除了一系列在EC2上执行的受控的实验外，我们还在Facebook的测试机群上进行了另一个实验。测试集群由35个总容量为370TB的节点组成。我没没有像在EC2中那样在集群中预先放置确定大小的文件，而是直接利用了集群中现有的文件集：共3262个文件，总计约2.7TB的逻辑数据。块大小采用了256MB（与Facebook的生产集群相同）。大概94%的文件由3个块组成，其余的文件由10个块组成，即平均每个文件由3.4个块组成。 表3 Facebook集群中的实验结果表3 Facebook集群中的实验结果 \" 表3 Facebook集群中的实验结果 对我们的实验来说，集群上部署了HDFS-RS，且一旦数据的RAID操作完成后，就会有一台随机的DataNode被杀死。对HDFS字节读取量和修复时间的测量值会被采集起来。不幸的是，我们没有对网络流量的测量。实验会在以相同配置部署的HDFS-Xorbas上再次执行一遍。结果在表3中显示。需要注意的是，在本次实验中，HDFS-Xorbas比HDFS-RS的存储多出了27%（理想情况下，额外开销应为13%），这是由于集群中存储的主要是小文件。正如我们之前提到的那样，被存储在HDFS上的文件通常很大（小文件通常会被归档为较大的HAR文件）。此外，需要强调的是，本次实验中使用的特定的数据集接不代表Facebook生产集群中存储的数据集。 在本实验中，第二次运行中丢失的块数超过了第一次运行时丢失的数量，因为HDFS-Xorbas引入了额外的存储开销。然而，我们还是能够观测到其在整体数据读取和修复时间上的优势，当将丢失的块的数量归一化之后，这些收益会更加明显。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:7:3","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"6. 相关工作 为了实现高效修复而需要的编码优化问题是最近备受关注的主题，因为其与分布式系统有关。这里面有大量的工作要做，这里我们只视图给出一个高层的高数。感兴趣的读者可以参考[7]和其中的参考文献。 本文与文献[7]的第一个重要区别是功能性（functional）和精确（exact）修复的区别。功能性修复的意思是，当一个块丢失时，会创建一个不同的块来维持编码中指定的$(n,k)$的容错能力。函数性修复的主要问题在于，当一个系统块丢失时，它会被一个奇偶校验块替代。当纠错码的全局容错能力仍为$n-k$时，读取一个块的操作会变为需要访问$k$个块。尽管这样可能对很少被读取的归档系统很有用，但是这不适合我们的工作负载。因此，我们仅对能精确修复的编码感兴趣，这样我们可以系统地维护编码。 Dimakis等人[6]表明了通过比朴素的策略（读并传输$k$个块）更小的网络流量来修复是可行的。最初的再生成（regenerating）编码[6]仅提供了功能性修复，匹配信息论边界点值的精确再生成编码的存在性仍是一个开放的问题。 随后，大量的工作（如[7, 25, 30]和其中的参考文献）表明，精确修复是可行的，其符合信息论[6]的边界限制。精确修复编码被分为低比例的（$k/n\u003c1/2$）和高比例的（$k/n\u003e1/2$）。对于低比例的精确修复编码（例如存储额外开销大于2倍的），最近已经发现了合并了再生成编码的优美的结构实现[26, 29]。副本策略有三倍的存储开销，而我们的应用程序最感兴趣的是存储负载在1.4~1.8的策略，这与使用低比例的精确再生成编码相违背。 目前，我们对于高比例的精确修复编码的理解还不够完整。这一编码是否存在性曾一直是开放问题，直到两组团队[3]分别独立使用了干扰对齐（Interference Alignment）（一种为无线信息论开发的渐进技术，其表明比例高于$1/2$的精确再生成编码时存在的）。不幸的是，这一构造只具有理论意义，因为其需要指数的字段大小却仅在渐进状态下才能表现良好。显式的高比例再生成编码是研究热点课题，但是目前我们还不知道有可实际构建的方法。这些编码的第二个问题是这些编码中很多都减小了修复时的网络流量，但是有更高的磁盘I/O开销。目前还不知道高磁盘I/O是否是必需的，也不知道是否存在可实现的同时有较小的磁盘I/O和修复流量的编码存在。 另一族对修复做出优化的编码致力于放宽MDS的要求来改进修复的磁盘I/O和网络带宽（如[17, 20, 10]）。这些结构中使用了局部性（locality）这一指标，即重建一个丢失的块时需要读取的块的数量。我们介绍的这些编码在局部性方面是最优的，它们符合在[10]中给出的边界。在我们最近的先做出的工作中[23]，我们推广了这一边界，并证明了它符合信息论（例如，其也适用于线性和非线性的向量编码）。我们发现最优的局部性对于最优磁盘I/O或最优修复时网络流量来说不是必须的，这些量的基本联系仍是开放的问题。 本文的主要理论进步是：一个依赖RS全局奇偶校验的有最优局部性的心得编码结构。我们展示了隐式奇偶校验的概念如何节约存储，并展示了如果全局奇偶校验法是RS时，如何显式地实现奇偶校验校准。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:8:0","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"7. 结论 现代存储系统转向了纠删码技术。我们介绍了一个叫做Locally Repairable Codes（LRCs）的新的编码族，其在存储方面略逊于最佳水平，但修复时磁盘I/O和网络带宽需求明显更小。在我们的实现中，我们观测到其减少了2倍的磁盘I/O，并仅需14%的额外存储开销，这一代价在很多场景下是合理的。 我们认为局部可修复编码能产生重大影响的相关领域是纯归档集群。在这一情况下，我们可以部署大型LRCs（即条带大小为50或100个块），其会同时提高容错能力并减小存储额外开销。因为修复所需的流量随条带的大小线性增长，因此在这一情况下使用RS码是不现实的。局部修复还会减少磁盘旋转[21]，因为很少需要修复单个块。 总之，我们相信LRCs创建了一个新的操作点，其与大规模存储系统息息相关，特别是当网络带宽成为主要性能瓶颈时。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:9:0","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"8. 参考文献 [1] Amazon EC2. http://aws.amazon.com/ec2/. [2] HDFS-RAID wiki. http://wiki.apache.org/hadoop/HDFS-RAID. [3] V. Cadambe, S. Jafar, H. Maleki, K. Ramchandran, and C. Suh. Asymptotic interference alignment for optimal repair of mds codes in distributed storage. Submitted to IEEE Transactions on Information Theory, Sep. 2011 (consolidated paper of arXiv:1004.4299 and arXiv:1004.4663). [4] B. Calder, J. Wang, A. Ogus, N. Nilakantan, A. Skjolsvold, S. McKelvie, Y. Xu, S. Srivastav, J. Wu, H. Simitci, et al. Windows azure storage: A highly available cloud storage service with strong consistency. In Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles, pages 143–157, 2011. [5] M. Chowdhury, M. Zaharia, J. Ma, M. I. Jordan, and I. Stoica. Managing data transfers in computer clusters with orchestra. In SIGCOMM-Computer Communication Review, pages 98–109, 2011. [6] A. Dimakis, P. Godfrey, Y. Wu, M. Wainwright, and K. Ramchandran. Network coding for distributed storage systems. IEEE Transactions on Information Theory, pages 4539–4551, 2010. [7] A. Dimakis, K. Ramchandran, Y. Wu, and C. Suh. A survey on network codes for distributed storage. Proceedings of the IEEE, 99(3):476–489, 2011. [8] B. Fan, W. Tantisiriroj, L. Xiao, and G. Gibson. Diskreduce: Raid for data-intensive scalable computing. In Proceedings of the 4th Annual Workshop on Petascale Data Storage, pages 6–10. ACM, 2009. [9] D. Ford, F. Labelle, F. Popovici, M. Stokely, V. Truong, L. Barroso, C. Grimes, and S. Quinlan. Availability in globally distributed storage systems. In Proceedings of the 9th USENIX conference on Operating systems design and implementation, pages 1–7, 2010. [10] P. Gopalan, C. Huang, H. Simitci, and S. Yekhanin. On the locality of codeword symbols. CoRR, abs/1106.3625, 2011. [11] K. Greenan. Reliability and power-efficiency in erasure-coded storage systems. PhD thesis, University of California, Santa Cruz, December 2009. [12] K. Greenan, J. Plank, and J. Wylie. Mean time to meaningless: MTTDL, Markov models, and storage system reliability. In HotStorage, 2010. [13] A. Greenberg, J. Hamilton, D. A. Maltz, and P. Patel. The cost of a cloud: Research problems in data center networks. Computer Communications Review (CCR), pages 68–73, 2009. [14] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: A scalable and flexible data center network. SIGCOMM Comput. Commun. Rev., 39:51–62, Aug. 2009. [15] C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, and S. Lu. DCell: a scalable and fault-tolerant network structure for data centers. SIGCOMM Comput. Commun. Rev., 38:75–86, August 2008. [16] T. Ho, M. M´edard, R. Koetter, D. Karger, M. Effros, J. Shi, and B. Leong. A random linear network coding approach to multicast. IEEE Transactions on Information Theory, pages 4413–4430, October 2006. [17] C. Huang, M. Chen, and J. Li. Pyramid codes: Flexible schemes to trade space for access efficiency in reliable data storage systems. NCA, 2007. [18] S. Jaggi, P. Sanders, P. A. Chou, M. Effros, S. Egner, K. Jain, and L. Tolhuizen. Polynomial time algorithms for multicast network code construction. Information Theory, IEEE Transactions on, 51(6):1973–1982, 2005. [19] O. Khan, R. Burns, J. Plank, W. Pierce, and C. Huang. Rethinking erasure codes for cloud file systems: Minimizing I/O for recovery and degraded reads. In FAST 2012. [20] O. Khan, R. Burns, J. S. Plank, and C. Huang. In search of I/O-optimal recovery from disk failures. In HotStorage ’11: 3rd Workshop on Hot Topics in Storage and File Systems, Portland, June 2011. USENIX. [21] D. Narayanan, A. Donnelly, and A. Rowstron. Write off-loading: Practical power management for enterprise storage. ACM Transactions on Storage (TOS), 4(3):10, 2008. [22] F. Oggier and A. Datta. Self-repairing homomorphic codes for distributed storage systems. In INFOCOM, 2011 Proceedings IEEE, pages 1215 –1223, april 2011. [23] D. Papailiopoulos and A. G. Dim","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:10:0","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"附录 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:11:0","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"附录A 通过熵的距离和局部性 接下来，我们将使用一个基于熵函数的长为$n$的编码的编码距离$d$的特征。这一特征非常普遍，其覆盖了任何线性和非线性的编码设计场景。 设我们希望将大小为$M$的文件划分为$x$份，并与$\\frac{k}{n}$的冗余一起存在$n$个块中，每个块的大小为$\\frac{M}{k}$。在不损失普遍性的情况下，我们假设文件被划分为了$k$个有相同大小$x \\triangleq [X_1,…,X_k] \\in \\mathbb{F} ^{1 \\times k}$的块，其中$\\mathbb{F}$是所有被执行的运算的有限域。每个文件块的熵为$H(X_i)= \\frac{M}{k}$，其中$i \\in [k]$，$[n]= \\lbrace 1,…,n \\rbrace$注2。接下来，我们定义编码（生成器）映射$G : \\mathbb{F} ^ { 1 \\times k} \\mapsto \\mathbb{F} ^ {1 \\times n} $，其输入为$k$个文件块，输出$n$个编码块$G(x)=y=[Y_1,…,Y_n]$，其中对于所有$i \\in [n]$，$H(Y_i) = \\frac{M}{k}$。编码函数$G$在向量空间$\\mathbb{F} ^{1 \\times n}$上定义了一个$(k,n-k)$的编码$\\mathcal{C}$。我们可以使用文件块的熵和$n$个编码块的熵之和的比值来计算编码的有效比例。 注2：换句话说，每个块被视作熵为$\\frac{M}{k}$的随机变量。 $$ R = \\frac{ H(X_1,…,X_k) }{ \\sum _{i=1} ^{n} H(Y_i) } = \\frac{k}{n} \\tag {4} $$ 编码$\\mathcal{C}$的距离$d$等于擦除后剩余块的熵严格小于$M$的条件下的最小块擦除数量$y$。 $$ d= \\mathop{min} \\limits _{ H ( \\lbrace Y_1,…,Y_n \\rbrace \\backslash \\varepsilon ) \u003c M } | \\varepsilon | = n - \\mathop{max} \\limits _{ H(S) \u003c M } | \\mathcal{S} | \\tag {5} $$ 其中$ \\varepsilon \\in 2 ^{ \\lbrace Y_1,…,Y_n \\rbrace } $是块擦除模式的集合，且$ 2 ^{ \\lbrace Y_1,…,Y_n \\rbrace } $表示$\\lbrace Y_1,…,Y_n \\rbrace$的幂集，即该集合由所有的子集$\\lbrace Y_1,…,Y_n \\rbrace$组成。因此，对于一个长度为$n$，距离为$d$的编码$\\mathcal{C}$，任意$n-d+1$个编码块都能重新构建该文件，即联合熵至少等于$M$。由此可知，当$d$给定时，$n-d$是使熵小于$M$的编码块最大数量。 编码的局部性$r$同样可以从编码块的熵的角度定义。当编码块$Y_i$，$i \\in [n]$有局部性$r$时，那么对于另外$r$个编码块的变量有函数$Y_i=f_i(Y_{\\mathcal{R}(i)})$，其中$\\mathcal{R}(i)$索引了能重构$Y_i$的$r$个块的集合$Y_j$，$j \\in \\mathcal{R}(i) $，$f_i$是这$r$个编码块上的某个(线性或非线性)函数。因此，$Y_i$在其修复组$\\mathcal{R}(i)$上的熵恒等于零，$H(Y_i|f_i(Y_{ \\mathcal{R}(i) }))=0$，其中$i \\in [n] $。$Y_i$对$\\mathcal{R}(i)$的函数依赖基本上是在我们的推导中假设的唯一编码结构注3。这种普遍性是为具有局部性$r$的线性或非线性的$(k,n-k)$编码提供通用的信息论边界的关键。接下来，在考虑局部性时，边界可以看做是编码距离的统一的辛格尔顿界（Singleton bound）。 注3：接下来，我们考虑有相同局部性的编码。即，$(k,n-k)$编码中所有编码块都有局部性$r$。这些编码被称为非规范化的编码[10]。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:11:1","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"附录B 才疏学浅… 后面实在翻不动了… 感兴趣的小伙伴请参考原文XORing Elephants: Novel Erasure Codes for Big Data。 ","date":"2020-09-03","objectID":"/posts/paper-reading/xoring-elephants/:11:2","tags":["Erasure Code","Translation"],"title":"《XORing Elephants: Novel Erasure Codes for Big Data》论文翻译（arXiv:1301.3791v1）","uri":"/posts/paper-reading/xoring-elephants/"},{"categories":["Paper Reading"],"content":"本篇文章是对论文Bigtable-OSDI06的原创翻译，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:0:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"作者 Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach Mike Burrows, Tushar Chandra, Andrew Fikes, Robert E. Gruber {fay,jeff,sanjay,wilsonh,kerr,m3b,tushar,fikes,gruber}@google.com Google, Inc. ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:1:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"摘要 Bigtable是一个为管理大规模可伸缩的结构化数据而设计的的分布式存储系统，它可以跨上千台商用服务器管理PB级的数据。Google中很多项目将数据存储在Bigtable中，包括web索引、Google Earth和Google Finance。这些应用程序对Bigtable提出了非常不同的需求，这些不同包括数据大小不同（从URL到web页面再到卫星图像）和延迟要求不同（从后端批处理任务到实时数据服务）。尽管需求是多变的，Bigtable还是成功地为Google的所有这些产品提供了灵活的、高性能的解决方案。在本文中，我们描述了Bigtable提供的允许客户端动态控制数据布局和格式的简单数据模型，以及Bigtable的设计与实现。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:2:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"1. 引言 在过去的两年半的时间里，我们在Google设计、实现并部署了一个用来管理结构化数据的分布式存储系统——Bigtable。Bigtable为可靠地适用于PB级数据和上千台机器而设计。Bigtable已经完成了几个目标：适用性广、可伸缩、高性能和高可用。Bigtable在超过60余个Google的产品和项目中被使用，包括Google Analytics、Google Finance、Orkut、个性化搜索、Writely和Google Earth。这些产品使用Bigtable以应对变化多样的负载需求，从吞吐量敏感的批处理程序到面相终端用户的延迟敏感的数据服务。这些产品使用的Bigtable集群配置也变化多样，从几台服务器到数千台服务器，最多的可以存储几百TB的数据。 Bigtable在很多方面都很像一个数据库：Bigtable和数据库的很多实现策略都是相同的。并行数据库[14]和内存数据库[13]已经做到了可伸缩和高性能，但是Bigtable提供了与这类系统不同的接口。Bigtable不支持完整的关系数据模型，取而代之的是，Bigtable提供了一个简单地数据模型，该模型允许客户端动态控制数据布局和格式，且允许客户端参与决策层数据在下层存储中的位置属性。数据通过可用任意字符串命名的行名和列名来索引。Bigtable将数据视为普通字符串且不关注其内容。客户端可以将不同格式的结构化或半结构化数据序列化为字符串。客户端可以小心地选择数据的schema来控制数据的位置。最后欧，Bigtable的schema参数允许客户端动态控制将数据放在内存中还是磁盘中使用。 第二章更详细地介绍了数据模型。第三章给出了客户端API的概览。第四章简要描述了Bigtable依赖的Google的下层基础设施。第五章描述了Bigtable的基本实现。第六章我们为Bigtable的性能做出的改进。第七章提供了Bigtable的性能测试。在第八章中，我们描述了一些关于Bigtable在Google中被如何使用的例子。在第九章中，我们讨论了我们在设计和支持Bigtable时认识到的一些问题。最后，第十章讨论了相关工作，第十一章给出了我们的结论。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:3:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"2. 数据模型 Bigtable是一个稀疏的、分布式的、持久化的多维排序字典（map）。该字典通过行键（row key）、列键（column）和时间戳（timestamp）索引，字典中的每个值都是字节数组。 $$(row:string, column:string, time:int64) \\rightarrow string$$ 在调研了类Bigtable系统的各种潜在用途后，我们决定采用这种数据模型。驱动我们做出部分设计决策的案例是：假设我们想要持有一份可在很多项目中使用的大量web页面和相关信息的副本，我们称这个副本构成的特殊的表为Webtable。在Webtable中，我们使用URL作为行键，网页的不同性质作为列名，并将网页的内容和其被抓取的时间戳存储在“contentes:”列中，如图1中个所示。 图1 存储网页的样例表的一个分片。行名是反转后的URL。“contents（内容）”列族包含网页内容，“anchor（锚）”列族包含所有引用了该网页的锚的文本。CNN的主页同时被Sports Illustrated和MY-look的主页引用，因此该行中包含名为“anchor:cnnsi.com”和“anchor:my.look.ca”的列。每个锚定的单元格都有一个版本号。“contents有3个版本，时间戳分别为$t_{3}$、$t_{5}$和$t_{6}$”。图1 存储网页的样例表的一个分片。行名是反转后的URL。“contents（内容）”列族包含网页内容，“anchor（锚）”列族包含所有引用了该网页的锚的文本。CNN的主页同时被Sports Illustrated和the MY-look的主页引用，因此该行中包含名为“anchor:cnnsi.com”和“anchor:my.look.ca”的列。每个锚定的单元格都有一个版本号。“contents有3个版本，时间戳分别为$t_{3}$、$t_{5}$和$t_{6}$”。 \" 图1 存储网页的样例表的一个分片。行名是反转后的URL。“contents（内容）”列族包含网页内容，“anchor（锚）”列族包含所有引用了该网页的锚的文本。CNN的主页同时被Sports Illustrated和MY-look的主页引用，因此该行中包含名为“anchor:cnnsi.com”和“anchor:my.look.ca”的列。每个锚定的单元格都有一个版本号。“contents有3个版本，时间戳分别为$t_{3}$、$t_{5}$和$t_{6}$”。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:4:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"2.1 行 表中行键可以使任意的字符串（目前不能超过64KB，对大部分用户来说通常大小仅为10~100字节）。在单个行键下的读写是原子性的（无论该行中有多少不同的列正在被读写），这种设计决策使客户端对同一行并发更新时能够更容易地判断系统的行为。 Bigtable按照行键以字典序维护数据。表会对行区间动态分区。每个行区间被称为一个tablet，其为分布式和负载均衡的基本单位。这样做在读取较短的行区间时非常高效，且通常仅需要与较少的机器交互。客户端可以通过选择行键来利用这一性质，为数据访问提供更好的数据位置分布。例如，在Webtable中，通过将URL中hostname的各部分反转，可以将域名相同的网页被分组到连续的行中。如，我们会将“maps.google.com/index.thml”的数据使用键“com.google.maps/index。html”存储。将相同域名下的网页存储在相邻的位置可以使一些对主机和域名的分析更加高效。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:4:1","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"2.2 列族 列键被为一系列的组，这些组被称为“列族（column family）”，列族是访问控制（access control）的基本单位。被存储在同一列族下的数据通常为同一类型的（我们会将同一列族下的数据压缩在一起）。列族必须在数据被存储到该列族中任何列键下之前创建；在列族创建好后，该列族下任何列键都可以使用。我们希望减少表中不同列族的数量（最多在几百以内），且列族在操作期间很少被改变。相反，一个表中可以有无限多的列。 列键使用如下的格式命名：列族名:限定符。列族名必须是可打印（printable）的，但限定符可以是任何字符串。Webtable中的一个列族样例名为“language”。在“language”列族下仅使用了一个列键，在列键下存储的是网页的语言ID。该表中另一个很有用的列族为“anchor”。该列族下每个列键都表示一个单独的锚点（译注：若有一个网页引用了该网页，那么引用该网页的网站域名即为该网页的锚点），如图1所示。该列族中列键的限定符以引用该网页的网站名来命名，单元格的内容是该锚点的链接。 访问控制以及磁盘和内存统计都在列族级别执行。在我们的Webtable例子中，这些控制让我们能够对不同类型的应用程序做出不同的管理：有些应用程序被允许新增基本数据，有些应用程序被允许读取基本数据并创建派生的列族，有些应用程序仅被允许查看以后数据（且很有可能为隐私考虑无法阅读所有的列族）。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:4:2","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"2.3 时间戳 Bigtable中的单元格可以包含相同数据的不同版本，这些版本使用时间戳索引。Bigtable的时间戳是64位整型。时间戳可以被Bigtable分配，这样其可以以毫秒来表示当前时间；时间戳还可以通过客户端程序指定。如果应用程序需要避免时间戳碰撞，那么其必须自己生成唯一的时间戳。单元格的不同版本会按照时间戳倒序存储，这样最新的版本可以被最早读取。 为了使管理不同版本的数据更加简便，每个列族支持两种设置。通过设置可以使Bigtable能够自动地对单元格的版本进行垃圾回收。客户端可以指定Bigtable仅保留单元格的最后$n$个版本，或者仅保留足够新的版本（例如，仅保留最近七天内写入的值）。 在我们的Webtable例子中，我们为“content：”列中存储的爬取到的页面设置的时间戳为：该版本的页面被爬取到的实际时间。上文中描述的垃圾回收机制允许我们仅保留每个页面的最近3个版本。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:4:3","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"3. API Bigtable的API提供了用于创建、删除表和列族的函数。还提供了修改集群、表和列族的元数据（如访问控制权限）的函数。 客户端程序可以写入或删除Bigtable中的值、从个别行中查找值或者遍历表的子集中的数据。图2展示了在c++代码中使用RowMutation抽象来执行一系列更新。（省略了不相关的细节以保证示例简洁。）Apply调用会对Webtable执行一个原子性的变更：向“www.cnn.com”中增加一个anchor并删除另一个anchor。 图2 向Bigtable写入图2 向Bigtable写入 \" 图2 向Bigtable写入 图3展示了在c++代码中使用Scanner抽象遍历特定行的所有anchor。客户端可以遍历多个列族。客户端有几种限制扫描获取的行、列和时间戳的机制。例如，我们可以限制扫描仅获取列名匹配正则表达式anchor:*.cnn.com的anchor，或者仅匹配时间戳在当前时间的十天内的anchor。 图3 从Bigtable读取图3 从Bigtable读取 \" 图3 从Bigtable读取 Bigtable支持其他的一些允许用户通过更复杂的方式操作数据的特性。第一，Bigtable支持单行事务（single-row transaction），该特性可用作原子性地对一个行键下的数据串行地读、改、写。尽管Bigtable的客户端提供了跨行键的批量写入的接口，但是Bigtable目前不支持跨行键的事务。第二，Bigtable允许单元格被用作整型计数器。最后，Bigtable支持服务器的地址空间中执行用户提供的脚本。这些脚本通过Google开发的用于数据处理的Sawzall语言[28]编写。目前，基于Sawzall的API不允许客户端脚本将数据写回Bigtable，但支持多种形式的数据转换、基于任意表达式的数据过滤、使用多种操作符运算。 Bigtable可在MapReduce[12]中使用。MapReduce是一个Google开发的运行大规模并行计算的框架。我们已经编写了一系列的封装，来使Bigtable可以作为MapReduce任务的输入或输出。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:5:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"4. 块的构建 Bigtable构建一些Google的其他基础架构之上。Bigtable使用了分布式的Google File System（GFS）[17]来存储日志和数据文件。Bigtable集群通常在运行着各式各样的分布式程序的共享的主机池上运行，且Bigtable进程经常与其他程序的进程在同一机器上运行。Bigtable依赖集群管理系统来调度任务、管理共享机器的资源、处理机器故障和监控机器状态。 我们内部使用Google SSTable文件格式来存储Bigtable的数据。SSTable提供了持久化的、按照键-值的顺序排序的不可变字典，其键值可以使任意的字节型字符串。SSTable提供了按照指定的键查找值和在指定键的范围内遍历键值对的操作。每个SSTable内部都包含一个块（block）的序列（块大小可通过配置修改，通常为64KB）。SSTable通过块索引（block index，存储在SSTable的结尾）来定位块，当SSTable被打开时，块索引会被载入到内存中。查找可通过一次磁盘seek操作实现：首先对内存中的块索引使用二分查找来查找指定块的位置，接着从磁盘读取该块。SSTable还可以可选地被完全映射的内存，这可以使查找和扫描不需要访问磁盘。 Bigtable依赖高可用、持久化的锁——[8]。一个Chubby服务包含5个活动的副本，这些副本中的一份被选举为master并处理请求。当这些副本中的大部分副本可以相互通信时，该服务即为可用的。Chubby使用Paxos算法[9, 23]维护副本一致性，以应对故障情况。Chubby提供了由目录和小文件组成的命名空间机制。每个目录或文件都可以用作锁，对文件的读写都是原子性的。Chubby的client库提供了对Chubby文件的一致性缓存。如果client在租约过期时间内没能更新session的租约，那么该session会过期。当session过期时，client会失去所有的锁和已经打开的句柄（handle）。Chubby的client还可以对Chubby的文件和目录注册回调（callback），当其被修改或session过期时会通知client。 Bigtable在很多任务中使用了Chubby，如：确保忍一时客服最多只有一个活动的master、存储Bigtable数据引导（bootstrap）位置（章节5.1）、发现tablet服务器并认定tablet服务器挂掉（章节5.2）、存储Bigtable的schema信息（每张表的列族信息）、存储访问控制列表。如果Chubby在较长的一段时间内不可用，那么Bigtable也会变得不可用。我们最近测量了跨11个Chubby实例的14个Bigtable集群中的效果。其中，由于Chubby（因Chubby停机或网络问题导致）不可用而导致的某些Bigtable中的数据不可用的时间平均占0.0047%。单个集群因Chubby不可用受影响占比为0.0326。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:6:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"5. 实现 Bigtable的实现包含了三个主要的组件：链接到每个client中的库、一个master server、若干tablet server。tablet server可随着负载的变化动态被添加或删除到集群。 master负责将tablet分配到tablet server、检测tablet server的加入或过期、均衡tablet server的负载、回收GFS中的文件。除此之外，master还处理shcema变化，如表和列族的创建。 每个tablet server都管理一系列的tablet（通常每个tablet server管理大概十到一千个tablet）。tablet server处理对其加载的tablet读写请求，并在tablet增长得过大时分割tablet。 与其他单master的分布式存储系统[17, 21]类似，client的数据不直接发送到master，而是由client直接与tablet server通信来读写数据。因为Bigtable的client不依赖master查找tablet的位置信息，大部分的client从不与master通信。这样，master在实际环境中的负载非常低。 Bigtable集群可以存储大量的表。每个表都由一系列tablet组成。当表增长时会被自动分割成多个tablet，每个tablet默认大小约为100~200MB。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:7:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"5.1 tablet位置 我们采用类似B+树[10]的三层的数据结构存储tablet位置信息（如图4所示）。 图4 tablet位置层级图4 tablet位置层级 \" 图4 tablet位置层级 第一层是一个存储在Chubby中的文件，其包含了root tablet的位置信息。root tablet中特殊的METADATA表包含了所有tablet的位置信息。每个METADATA tablet包含了一系列用户tablet的位置信息。虽然root tablet只是METADATA表的第一个tablet，但其被处理的方式比较特殊：root tablet永远不会被分割，这样可以保证tablet位置层级不超过三层。 METADATA表在一个行键下存储一个tablet的位置信息，该行键由这个tablet的标识符和其末行编码而得。METADATA表中每行在内存中大约占1KB。METADATA表大小限制为128MB，该三层位置信息结构能够提供$2^{34}$个tablet的寻址能力。 client库会缓存tablet位置信息。如果client不知道tablet的位置或者其发现缓存的位置信息不正确，其会递归地向上查询。如果client的缓存为空，那么位置算法需要3轮网络交互，其中包括一次从Chubby中读取数据的网络交互。如果client的缓存数据较旧，那么其需要最多6轮网络交互，因为陈旧的缓存条目仅在失配时才会被发现（假设METADATA tablet不会频繁移动）。尽管因tablet位置信息被存储在内存中而不需要访问GFS，我们还是通过令client的库预拉取tablet位置信息的方式进一步削减了大多数场景下的开销。当client的读取METADATA表时，其会读取不止1个tablet的元数据。 我们还在METADATA表中存储了次要的信息，包括每个tablet的相关事件（如服务器为其提供服务的时间等）。这些信息对调试和性能分析非常有帮助。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:7:1","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"5.2 tablet分配 每个tablet在同一时刻仅会被分配到一个tablet server。master会持续记录存活的tablet server和该tablet server中当前的tablet分配情况。当一个tablet未被分配且有空间足以容纳该tablet的tablet server可用时，master会通过向该tablet server发送一个tablet装载请求来分配tablet。 Bigtable使用Chubby来跟踪记录tablet server。当tablet server启动时，其会在指定Chubby目录下创建一个唯一命名的文件，并在该文件上获取排他锁。master监控这个目录（服务器目录）来发现tablet server。如果tablet server失去了其排他锁（例如因网络分区导致服务器失去了其访问Chubby的session），该tablet server会停止提供其tablet的服务。（Chubby提供了一个高效的机制使tablet server能够检查其是否仍持有持有锁且不会导致网络拥堵。）只要tablet server创建的文件还存在，tablet server就会试图新获取其文件的排他锁。如果这个文件不再存在，那么tablet server永远不会再次提供服务，因此其会杀死自己的进程。当一个tablet server终止时（例如由于集群管理系统将该tablet server所在的机器移出了集群），其会试图释放它持有的锁，这样master可以更快地重新分配tablet。 master需要检测到tablet server不再对其tablet提供服务的情况，并尽快地重新分配那些tablet。为了检测tablet server不再对其tablet提供服务的情况，master会间歇地询问每个tablet server的锁的状态。如果tablet server报告其失去了它的锁或者master在几次重试后仍无法访问tablet server，那么master会试图在该tablet server创建的文件上获取排他锁。如果master能够获取到锁，那么说明Chubby存活且tablet server可能挂掉或无法访问Chubby，master会删除该tablet server的文件以确保该tablet server永远无法再次提供服务。一旦tablet server创建的文件被删除，master便可以将之前分配到该tablet server上的tablet转变为一系列未分配的tablet。为了确保Bigtable集群在master和Chubby间网络出现问题的情况下的健壮性，master会在其Chubby session过期时杀死自己的进程。然而，如上文所述，master故障不会改变tablet在tablet server中的分配情况。 当master被集群管理系统启动时，它需要在对tablet的分配进行修改前发现当前tablet的分配情况。master会在启动时执行以下步骤：（1）master在Chubby中取得一个唯一的master锁以防止并发的master实例化。（2）master扫描Chubby中tablet server目录来寻找存活的tablet server。（3）master与每个存活的tablet server通信来发现每个tablet server中已分配的tablet情况。（4）master扫描METADATA表以了解tablet的状态。一旦扫描时遇到了未分配的tablet，master会将其加入到未分配的tablet的集合，使其符合tablet分配的条件。 这样，只有当METADATA的tablet被分配完成后才能扫描METADATA表。因此，在扫描开始前（步骤（4）），如果master在步骤（3）中没有找到root tablet的分配情况，master先将root tablet加入到未分配的tablet的集合中。这保证了root tablet会被分配。因为root tablet包含所有METADATA的tablet的名称，master会在扫描root tablet后获取到所有METADATA的tablet的信息。 已存在的tablet集合仅当有tablet被创建或删除、两个已存在的tablet合并为一个更大的tablet、或一个已存在的tablet被分割为来两个小tablet时被修改。除了最后一种修改，其他均由master启动，因此master可以追踪这些修改。而由于tablet的分割是由tablet server启动的，因此其处理方式不同。tablet server通过在METADATA表中记录新的tablet的信息的方式提交tablet分割。当分割被提交后，其会通知master。如果分割通知丢失（可能因tablet server或master挂掉造成），master会在其要求tablet server加载已经被分割的tablet时检测到新的tablet。此时，tablet server会将tablet分割信息告知master，因为master在METADATA表中找到的tablet条目仅为该tablet中被要求加载的部分（译注：master无法在METADATA表中找到tablet被分割的新的部分）。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:7:2","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"5.3 tablet服务 如图5所示，tablet的持久化状态被存储在GFS中。更新会被提交到存储着redo记录的commit log。其中，最近提交的更新会被存储在内存中被称为memtable的缓冲区中，较旧的更新会被存储在SSTable文件序列中。为了恢复一个tablet，tablet server会从METADATA表中读取其元数据。元数据包含了由tablet和一系列redo point（指向任何可能包括该tablet数据的指针）组成的SSTable列表。tablet server会将SSTable的索引读入内存，并通过应用所有redo point后的更新的方式重建memtable。 图5 tablet的表示图5 tablet的表示 \" 图5 tablet的表示 当写操作到达tablet server时，tablet server会检查其是否格式正确且其sender是否被授权执行该变更。鉴权通过从一个Chubby文件（大多数情况下总是会命中Chubby client的缓存）中读取被允许的writer列表来实现。合法的变更会被写入到commit log中。tablet server使用了分组提交的方式来提高多个小变更[13, 16]的吞吐量。在写入操作被提交后，其内容会被插入到memtable中。 当读操作到达tablet server时，同样会检查格式是否和权限是否正确。合法的读操作会在SSTable序列和memtable的合并的视图上执行。因为SSTable和memtable是按照字典序排序的数据结构，所以可以高效地生成合并视图。 在tablet分割或合并时，到达的读写操作仍可继续执行。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:7:3","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"5.4 精简数据 执行写操作时，memtable的大小会增加。当memtable的大小达到临界值时，该memtable会被冻结，并创建一个新的memtable，被冻结的memtable会被转换成一个SSTable并写入到GFS中。该minor compaction进程有两个目标：其会较小tablet server的内存占用，并减小当server挂掉后恢复时需要读取的commit log的总数据量。当触发minor compaction时，到达的读写操作可以继续执行。 每次minor compaction会创建一个新的SSTable。如果该行为不受约束地持续执行，读操作可能需要合并来自任意数量的SSTable中的数据更新以获取数据。因此，通过间歇性地在后台执行merging compaction以限制这种文件的数量。merging compaction时会读取一些SSTable和memtable中的内容，并将其写入到一个新的SSTable中。一旦mergin compaction完成后即可丢弃输入的SSTable和memtable。 将所有的SSTable写入到恰好一个SSTable中的merging compaction被称为major compaction。非major compaction生产的SSTable可能包含特殊的删除操作项，删除操作项用来阻止对被删除的但仍在活动的数据的操作。（译注：当删除的数据正在活动时，Bigtable不会立刻删除这些数据，而是写入这个删除操作项。这样，使用了这些待删除的数据的活动可以继续正常执行，而后续的活动无法再访问这些待删除的数据。）而major compaction则相反，其创建的SSTable中不包含删除操作的信息或被删除的数据。Bigtable会循环遍历其tablet并周期性地对它们执行major compaction。major compaction允许Bigtable回收被删除的数据占用的资源，并使Bigtable能够确保被删除的数据能够及时地从系统中移除，这对存储敏感型数据服务来说十分重要。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:7:4","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"6 改进 在上一章中描述的数显需要很多改进才能满足我们的用户需要的高性能、高可用、高可靠性。本章将更详细地讲述各部分实现的改进。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:8:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"6.1 局部组 client可以将多个列族组合为一个局部组（locality group）。每个tablet中的每个局部组会生成一个独立的SSTable。通常，可以将不在一起访问的列族放到不同的局部组中，以提高读取效率。例如Webtable中的页面元数据（如语言和校验和）可以放在一个局部组中，页面的内容可以放在不同的局部组中。这样，想要读取元数据的应用程序就不需要读取所有页面内容了。 除此之外，还可以为每个局部组指定不同的调优参数。例如，局部组可被声明为“内存型（in-memory）”。仅内存的局部组的SSTable会被懒式加载到tablet server的内存中。一旦加载完成，对这种局部组中的列族的访问就不需要访问磁盘。这个特性对被频繁访问的小规模数据非常有用。我们的METADATA表的位置列族的内部就使用了这一特性。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:8:1","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"6.2 压缩 client可以控制是否要压缩局部组的SSTable及使用哪种压缩格式。用户指定的压缩格式会被应用到SSTable的每个块（其大小可通过局部组调参控制）。尽管分别压缩每个block会损失一些空间，但是当我们需要读取一个SSTable的一小部分时不需要解压缩整个文件。许多client采用自定义的二次压缩（two-pass compression）策略。第一次压缩使用Bentley and McIlroy[6]算法，其会压缩跨大窗口的相同的长字符串。第二次压缩使用更快的压缩算法，在16KB的小窗口周查找重复的数据。两次压缩都非常快，在现在机器上，可以以100~200MB/s的速度编码，以400~1000MB/s的速度解码。 尽管我们在选择压缩算法时强调速度而不是空间的减少，这种二次压缩的策略实际表现还是出奇的好。例如，在Webtable中，我们使用这种压缩策略来存储网页的内容。在一次实验中，我们在一个局部组中存储了大量的文档。为了达到实验目的，我们限制仅对每个文档存储一个版本而不是所有可用的版本。通过这种策略压缩后仅占用原来的十分之一的空间。而通常使用的Gzip仅能将空间压缩到原来的三分之一到四分之一。对于HTML页面，二次压缩策略比Gzip的表现好很多，这归功于Webtable的行的布局：所有来自同一个主机的页面被就近存储。这使Bentley-McIlroy算法能够在同一主机下识别到大量的相同的模式。不只是Webtable，对很多应用程序来说，都可以通过挑选它们的行名的方式来使相似的数据聚堆，这样可以得到非常好的压缩比例。当我们在Bigtable中存储同一个值的多个版本时，压缩比例甚至会更好。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:8:2","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"6.3 读取缓存 为了提高读取性能，tablet server使用了二级缓存。Scan Cache是高层缓存，其将SSTable接口返回的键值对缓存到tablet server的代码中。Block Cache是低层缓存，其缓存从GFS读取的SSTable的块。对于更倾向于反复读取相同数据的应用程序来说，Scan Cache的作用更大。对更倾向于读取其最近读取的位置附近数据的应用程序来说，Block Cache的作用更大（例如，顺序读取、某个局部组的热点行中对不同列的随机读取）。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:8:3","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"6.4 布隆过滤器 正如章节5.3中描述的那样，读操作必须读取所有组成了tablet状态的SSTable。如果这些SSTable不在内存中，会造成大量的磁盘访问。为了减少磁盘访问，我们允许client为特定的局部组创建布隆过滤器（Bloom filter）[7]。布隆过滤器让我们能够询问SSTable是否可能包含指定行或列的数据。对特定的应用程序来说，在tablet server中仅使用少量内存来存储布隆过滤器即可大大减少读操作所需的磁盘寻道次数。使用布隆过滤器意味着大多数对不存在的行或列的查找不需要访问磁盘。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:8:4","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"6.5 commit log的实现 如果我们为每个tablet单独保存一个commit log，将会有大量的文件在GFS中并发写入。由于GFS服务器的下层存储系统实现方式，这些写入操作会导致大量的磁盘寻道次数以写入不同的物理上的日志文件。此外，因为局部组经常很小，为每个tablet分别存储日志文件会削弱分组提交的优化效果。为了解决这些问题，我们将对每个tablet server上的tablet的变更追加到同一个commit log中，同一个物理日志文件中包含了来自不同tablet的变更[18, 20]。 使用同一个日志文件在执行一般操作时能够提供大幅的性能提高，但是复杂化了恢复操作。当一个tablet server挂掉时，其提供服务的tablet将会被移动到很多其他的tablet server上，每个tablet server通常仅加载原tablet server中少量的tablet。为了恢复tablet的状态，新的tablet server需要重新应用原tablet server上该tablet的commit log中的变更。然而，这些tablet的变更在同一个物理日志文件中。恢复的其中一种方法是，每个tablet server读取完整的commit log并进应用其需要恢复的tablet的日志条目。然而，在这种策略下，如果100台机器中每台机器都分到了一个来自故障tablet server的tablet，那么日志文件将要被读取100次（每台tablet server一次）。 为了避免多次读取commit log，首先会对commit log中的条目按照$\u003c表, 行名, 日志序号\u003e$的键排序。在排序的输出中，每个特定的tablet的变更条目是连续的，这样就可以通过一次寻道和随后的顺序读取来高效地读取日志。为了并行化排序过程，我们将日志文件划分为64MB的段，并将每个段在不同的tablet server上并行地排序。排序进程由master协调，并在tablet server表名其需要从某个commit log文件中恢复变更时启动。 在将commit log写入到GFS是会因很多种原因导致性能波动（例如，涉及写操作的GFS机器崩溃，或者到涉及写操作的特定三台GFS服务器的网络拥塞、或者负载过高）。为了笔辩变更受GFS峰值时延的影响，每个tablet server实际上有两个日志写入线程，每个线程写各自的日志文件，在同一时刻二者中仅有一个线程被激活使用。如果写入到活动的日志文件的性能表现较差，那么日志的写入会切换到另一个线程，且在commit log队列中的变更会被新激活的日志写入线程写入。日志条目包含一个序号，这使恢复进程可以忽略因切换线程而产生的重复的日志条目。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:8:5","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"6.6 加速tablet恢复 如果master将一个tablet从一个tablet server移动到了另一个tablet server，源tablet server首先会对该tablet应用一次minor compaction。该操作会通过减少tablet server中的commit log中未压缩状态的的总量来减少恢复时间。当minor compaction完成后，tablet server会停止对该tablet提供服务。在其实际卸载该tablet之前，tablet server还会再进行一次minor compaction（通常很快）来消除任何在执行第一次minor compaction时到来的操作造成的剩余的未压缩的状态。在第二次minor compaction完成后，tablet可以被另一台tablet server装载且不需要恢复任何的日志条目。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:8:6","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"6.7 不变性的利用 因为我们生成的SSTable是不变的，所以除了SSTable的缓存，Bigtable系统的各种其他部分都可以被简化。例如，在我们为读取SSTable而访问文件系统时，不需要做任何的同步。这样，行的并发控制可以被高效实现。唯一的会同时被读写操作访问的可变数据结构是memtable。为了减少读取memtable的竞态，我们使memtable的每一行都在写入时复制（copy-on-write），并允许读写操作并行执行。 因为SSTable是不可变的，永久移除已删除的数据问题被转化成了对过时的SSTable的垃圾回收问题。每个tablet的SSTable都被会注册到METADATA表中。master对METADATA表的root tablet中记录的SSTable集合中的过时的SSTable集合应用“标记-清除（mark-and-sweep）”算法[25]进行垃圾回收。 最后，SSTable的不可变性可以让我们快速分割tablet。我们让子tablet共享父tablet，而不是为每个子tablet生成新的tablet。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:8:7","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"7. 性能评估 我们构建了一个有$N$个tablet server的Bigtable集群来测量Bigtable的性能和伸缩性，其中N有多种取值。tablet server的配置采用了1GB内存，数据通过一个由每台有两块400G IDE硬盘的1786台机器组成的GFS单元写入。Bigtable的测试负载由$N$个client机器产生。（我们使用了与tablet server数量相同的client以确保client不会成为瓶颈。）每台机器有两块双核2GHz皓龙处理器、足以支撑所有的工作进程负载的物理内存、和一个1Gbps的以太网连接。这些机器被安排在二层树状交换机网络中，根节点总带宽约100~200Gbps。所有机器都在同一个托管设施中，因此任意一对机器间RTT时间小于1ms。 所有tablet server、master、测试client、GFS server都在同一组机器上运行。每台机器上都运行着一个GFS server。一些机器还运行着一个tablet server、或一个client进程、或与这些实验的同时运行的其他任务的进程。 在测试中，Bigtable使用了$R$个不同的行键。$R$选取的目的是使每个benchmark都会对每个tablet server读或写约1GB的数据。 顺序写入的benchmark使用了被命名为$0$~$R-1$的行键。行键的空间被拆分为了10N个大小相同的区间。这些区间被中心调度器分配给了N个client，当client完成对一个区间的处理后，该调度器会将下一个可用的区间分配给该client。这种动态分配的策略能够帮助减少在client机器上运行的其他进程造成的性能变化的影响。我们在每个行键下写入了一个字符串。每个字符串都是随机生成的，因此无法被压缩。另外，不同行键下的字符串是不同的，因此也无法跨行压缩。随机写入的benchmark与顺序写入的benchmark类似，除了行键在写入前采用了对$R$取模的哈希算法，因此在整个benchmark期间，写入负载能够大致均匀地分布到整个行空间中。 顺序读取的benchmark生成行键的方法与顺序写入benchmark中的方法完全一致，但其在（在之前的顺序写入benchmark中已经生成好的）行键下读取而不是写入。与顺序读取benchmark相似，随机读取的benchmark也隐藏了随机写入benchmark中的操作。 扫描（scan）的benchmark与顺序读取的benchmark类似，但是其使用了Bigtable提供支持的API来扫描一个行键区间下的所有值。使用扫描的方式可以减少benchmark执行的RPC的数量，因为一个RPC即可获取tablet server中大量连续的值。 内存式随机读取的benchmark与随机读取的benchmark类似，但是去包含benchmark数据的局部组被标记为“内存型”。因此，读取由tablet server的内存满足，而不需要读取GFS。对于内存式读取的benchmark，我们将每个tabler的数据量从1GB减少到了100MB，以适配tablet server的内存大小。 图6 每秒中1000字节的值读写量。表中展示了每个tablet server的速率，图中展示了总速率。图6 每秒中1000字节的值读写量。表中展示了每个tablet server的速率，图中展示了总速率。 \" 图6 每秒中1000字节的值读写量。表中展示了每个tablet server的速率，图中展示了总速率。 图6中展示了在Bigtable中读写1000字节的值的性能表现的两种视图。表中展示了每个tablet server每秒的操作数，图中展示了整体的每秒操作数。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:9:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"7.1 单tablet server性能表现 让我们首先考虑单tablet server的性能表现。随机读取比所有其他操作要慢一个数量级或更多。每次随机读取中包含了将64KB的SSTable块通过网络从GFS发送到tablet server的操作，但其中仅有1000字节的值被使用。tablet server每秒执行约1200次读操作，读操作会从GFS以大概75MB/s的速率传输数据。由于网络栈的开销，这个带宽足以使tablet server的CPU饱和。SSTable的解析和Bigtable的代码，同样几乎是我们系统的网络连接饱和。大多数采用这种访问模式的Bigtable应用程序通常会将块大小减小到8KB。 内存式随机读取相比之下要快得多，因为tablet server的本地内存可以满足1000字节的读取，而不需要从GFS获取64KB的块。 随机写入和顺序写入的性能表现要biubiu随机读取更好，因为每个tablet server将所有到达的操作追加到到一个commit log中，并使用分组提交的方式，高效地将数据流式写入到GFS中。随机写入和顺序写入的性能表现没有太大的差距，在这两种亲情况下，tablet server所有的写入操作都会被记录到同一个commit log中。 顺序读取的性能表现比随机读取要好，因为每次从GFS获取的64KB的SSTable的块都会被存入块缓存中，在接下来的64次读取请求中都可以被使用。 扫描操作的性能甚至更高，因为tablet server可以在一次client的RPC中返回更多的值，因此RPC本身的开销可以被分摊到大量的值中。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:9:1","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"7.2 伸缩性 当我们将系统中的tablet server数量从1个增加到500个时，整体的吞吐量增长非常显著，增长了100倍以上。例如，在tablet server的数量增长了500倍时，内存式随机读取的性能表现几乎增长了300倍。其原因在于该benchmark的性能瓶颈在于每台tablet server的CPU。 然而，性能表现并不是线性增长的。对于大多数的benchmark，当tablet server的数量从1个增加到50个时，每台tablet server的吞吐量会明显地下降。性能下降是由于多服务器配置的负载不均衡导致的，这通常由进程争夺CPU和网络导致。我们的负载均衡算法视图解决这种不均衡的问题，但是由于两个主要原因而无法完美解决：为了减少tablet的移动次数，我们减少了重均衡（当tablet被移动时，其会在通常小于1秒的短时间内不可用），且benchmark生成的负载会随着benchmark的进度变化。 随机读取benchmark的伸缩性最差（当服务器数量增加了500倍时整体吞吐量仅提升了100倍）。其原因在于每次读取1000字节的数据时都要传输较大的64KB的块。这种传输方式使网络中很多共享的1Gbps的网络链路饱和，这也导致了当增加机器数时每台server的吞吐量下降非常明显。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:9:2","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"8. 真实应用 直到2006年8月，在Google的不同机器集群中共运行了388个非测试Bigtable集群，总计有24500台tablet server。表1中展示了每个cluster中tablet server的大致数量。其中许多集群被用作开发使用，因此在很长一段时间都是闲置的。一组由总计8069台tablet server组成的14个繁忙的集群每秒请求总量超过120万次，其以约741MB/s的速率收到RPC流量，以约16GB/s的速率发出RPC流量。 表1 Bigtable集群中tablet server的数量分布表1 Bigtable集群中tablet server的数量分布 \" 表1 Bigtable集群中tablet server的数量分布 表2提供了一些目前正在使用的表的相关数据。其中一些表被用作存储用户数据，其他的一些表用于皮肤处。表的总大小、平均单元大小、服务中内存中数据使用百分比和表schema的复杂度丰富多变。在本章后续章节中，我们将简要描述三个生产团队如何使用Bigtable。 表2 一些生产用的表的特征。Table size（压缩前测量）和#Cells可用来大致衡量大小。禁用了压缩的标的压缩率没有给出表2 一些生产用的表的特征。Table size（压缩前测量）和#Cells可用来大致衡量大小。禁用了压缩的标的压缩率没有给出 \" 表2 一些生产用的表的特征。Table size（压缩前测量）和#Cells可用来大致衡量大小。禁用了压缩的标的压缩率没有给出 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:10:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"8.1 Google Analytics Google Analytics（analytics.google.com）是一个用来帮助站长分析他们的流量模式的服务。其提供了整体分析，如每天里不同的访问者的数量、每个URL的网页每天的浏览量、和网站追踪分析报告等（如百分之多少的用户在浏览了指定的页面后购买了）。 为了启用该服务，站长需要将一小段JavaScript程序嵌入其前端页面。该程序会在网页被浏览时被调用。其记录了Google Analytics请求的各种信息，例如用户标识符和获取的网页信息。Google Analytics会汇总数据并让使其能被站长使用。 这里简要描述一下Google Analytic使用的两张表。raw click表（~200TB）的每一行都维护了一个终端用户的session。行名是包含网站名和session创建时间的元组。该schema确保了浏览相同网站的session是连续且按字典序排序的。该表被压缩到了其原始大小的14%。 summary表（~20TB）包含了各种网站预定义的总结。每隔一段时间，被调度的MapReduce任务会从raw click表计算生成summary表。每个MapReduce任务从raw click表中提取最近的session数据。整个系统的吞吐量受GFS的吞吐量限制。该表被压缩到了其原始大小的29%。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:10:1","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"8.2 Google Earth Google运营者一系列的服务。通过Google Maps接口（maps.google.com）和Google Earth（earth.google.com）自定义的client软件，这些服务让用户能够访问整个世界表面的高分辨率卫星图像。这些产品可以为用户提供整个世界表面的导航：它们可以在多种不同分辨率上计划、查看并注释卫星图像。该系统使用了一个表来预处理数据，并使用了另外一系列表为client提供数据服务。 预处理流水线使用了一张表来存储原始图像。在预处理中，图像会被清洗并合成为最终提供服务的数据。该表包含了约70TB的数据，因此其使用硬盘提供服务。这些图像已经被压缩过了，因此禁用了Bigtable的压缩。 图像表的每一行都对应一个地理段。行的命名保证了相邻的地理段被就近存储。表中包含了一个用来追踪每个段的数据源的列族。该列族中有大量的列，基本上每个原始数据图像都有一个列。由于每个地理段仅由少量几个图片构建出，因此该列族非常稀疏。 预处理流水线很大程度依赖MapReduce而不是Bigtable来传输数据。在一些MapReduce任务中，整个系统的每台tablet server数据处理速度超过1MB/s。 服务系统使用了一张表来索引存储在GFS中的数据。该表相对比较小（~500GB），但是其必须为每个数据中心的每秒数万次的查询提供低延时的服务。因此，该表分布在了上百台tablet server且使用了内存型列族。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:10:2","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"8.3 个性化搜索 个性化搜索（www.google.com/psearch）是一个可选的服务，其记录了用户在Google的各种产品如网页搜索、图片和新闻等上的的查询和点击。用户可以浏览他们的搜索历史以重新访问他们的历史查询和点击，同时他们还可以根据其Google历史使用模式进行个性化搜索。 个性化搜索在Bigtable中存储了每个用户的数据。每个用户都有一个唯一的userid，并有一个以这个userid命名的行。所有的用户行为都被存储在表中。每个用户的行为类型有一个独立的列族（例如，有一个列族存储了所有的web查询）。每个数据项在Bigtable中的时间戳为用户行为发生时间。个性化搜索会在Bigtable上使用MapReduce生成用户配置。这些用户配置文件被被用作实现个性化实时搜索结果。 个性化搜索的数据被被分到多个Bigtable集群中以增强可用性并减少因到client距离而带来的时延。个性化搜索团队原本在client侧建立了一个在Bigtable上层的副本机制以保证副本的最终一致性。而现在的系统目前使用的是构建在服务器端的副本子系统。 个性化搜索的存储系统的设计允许其他组在自己的列中添加新的用户的信息，目前Google的很多产品中都使用了该系统，它们需要存储每个用户的配置和设置。再多个组中共享同一张表导致该表中有不同寻常的大量的列族。为了支持这种共享，我们为Bigtable添加了一个简单的配额（quote）机制来限制任意一个client对共享表的存储消费。该机制为不同产品的组使用该系统存储每个用户的信息提供了一些隔离性。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:10:3","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"9. 经验 在设计、实现、维护和支持Bigtable的过程中，我们收获了很多很有帮助的经验并学到了一些有趣的知识。 我们学到的一个知识是：大型分布式系统在很多种故障下是非常脆弱的，这些故障不仅仅包括标准的网络分区和很多分布式协议中假设的故障停机（fail-stop）问题。例如，我们遇到过所有以下原因导致的问题：内存和网络老化、较大的时钟偏差、机器挂起、扩展的和不对称的网络分区、我们使用的其他系统中的bug（如Chubby）、GFS配额溢出、有计划或无计划的硬件维护等。随着我们在这些问题上的经验越来越多，我们通过修改不同的协议来解决了这些问题。例如，我们在我们的RPC机制上增加了校验和。我们还通过移除系统中一个部分对另一个部分的假设解决了一些问题。例如，我们不再假设一个给定的Chubby操作仅能返回一组固定的错误中的一个。 我们学到的另一个知识是：等弄清楚一个新的特性会被怎样使用后再添加这个特性是非常重要的。例如，我们最初计划在我们的API中加入通用的事务控制。因为我们不需要立刻使用这一特性，所以我们没实现。现在，我们有很多真实的应用程序运行在Bigtable上，我们能够检验这些程序的实际需求。我们发现大部分应用程序仅需要单行的事务。当用户需要分布式事务时，最重要的用途是维护二级索引，我们计划添加一种装门用来满足这一需求的机制。新的机制将比分布式事务的通用性差，但是会更高效（特别是对于跨几百或更多行更新的时候）且好会和我们的乐观的（optimistic）跨数据中心备份方案配合的更好。 在我们为Bigtable提供支持时学到的重要的知识是：合适的系统级监控的重要性（即同时监控Bigtable自身和通过Bigtable监控client进程）。例如，我们扩展了我们的RPC系统，使我们可以记录样例RPC的一个样例在整个RPC过程中的重要行为。这一特性让我们能够检测并修复很多问题，如tablet数据结构中的锁争用、提交Bigtable变更时GFS的写入慢、当METADATA tablet不可用时对METADATA表的访问会被卡主等。另一个体现了监控的用处的例子是：每个Bigtable都会在Chubby中注册。这让我们能够追踪所有的集群、发现集群大小、查看集群中运行的软件版本、查看集群正在接受的流量、有没有出现意外的长延时等。 我们学到的最重要的知识是简单的设计的价值。考虑到我们系统的大小（不包括测试大概有10万行代码）和代码会随着时间无法预料的变化，我们发现代码和设计的清晰对代码维护和调试有巨大的帮助。其中一个例子是我们的tablet server成员协议。我们的最初的协议非常简单：master每隔一段时间会向tablet server发出租约，如果tablet server的租约过期，它们会杀死自己的进程。不幸的是，这个协议会在出现网络故障时大大削弱系统的可用性，同时该协议对master的恢复时间非常敏感。我们重新设计了几次协议，知道我们得到了一个表现良好的协议。然而，最终的协议太过复杂并依赖Chubby中很少被其他应用程序使用的特性。我们发现我们花费了过多的时间在Bigtable的代码中甚至在Chubby的代码中调试模糊的情况。最终，我们不再使用该协议并转向了新的更简单的协议，新的协议仅依赖Chubby中被广泛应用的的特性。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:11:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"10. 相关工作 项目Boxwood[24]中有与Chubby、GFS和Bigtable在某些方面功能重叠的组件，其提供了分布式协议、锁、分布式块（chunk）存储、和分布式B树存储。虽然Boxwood在这些方面都有重叠，但其组件的目标定位似乎比Google的服务要较底层。Boxwood项目的目标是提供构建高层服务（如文件系统或数据库）的基础设施，而Bigtable的目标是直接支持client应用程序的数据存储需求。 许多近期的项目都解决了提供分布式存储或在广域网下（通常在Internet的范围下）提供高层服务的问题。包括从CAN[29]、Chord[32]、Tapestry[37]和Pastry[30]之类的项目开始的分布式哈希表的工作。这些系统解决了在Bigtable中不会出现的问题，如高度可变的带宽、不受信的参与者、频繁地修改配置等。去中心化的控制和拜占庭容错不是Bigtable的目标。 我们认为对于应用程序开发者来说，分布式的B树或分布式哈希表的模型过于受限。键值对的模型是十分有用的构建模块，但我们也不应该仅向开发者提供这一种模块。我们选择的模型比普通的键值对更加丰富，且支持稀疏的半结构化数据。而且该模型仍保持的足够的简单性，可用于非常高效的扁平化的文件表示；且其足够透明（通过局部组），用户可以对系统的重要行为进行调优。 一些数据库的供应商已经开发了能够存储大量数据的并行数据库。Oracle的Real Application Cluster数据库[27]使用了共享磁盘来存储数据（Bigtable使用GFS）并使用一个分布式的锁管理器（Bigtable使用Chubby）。IBM的DB2 Parallel Edition[4]基于类似Bigtable的shared-nothing[33]架构。每个DB2服务器负责管理一张表的行的一个子集，这个子集被存储在本地的关系型数据库中。这两个产品都提供了带事务的完整的关系模型。 相比于其他在磁盘上基于列而不是基于行组织数据的存储系统（包括C-Store[1, 34]和商业产品如Sybase IQ[15, 36]、Sensage[31]、KDB+[22]和MonetDB/X100的ColumnBM存储层[38]），Bigtable的局部组实现了类似的压缩和磁盘读优化。另一个可以将数据横向或纵向分区到扁平化的文件中并有良好压缩率的系统是AT\u0026T的Daytona数据库[19]。Bigtable的局部组不支持CPU缓存级别的优化，如Ailamaki[2]中描述的那种优化。 Bigtable使用memtable和SSTable将更新存储到tablet的方式与Log-Structured Merge Tree[26]中将更新存储到索引数据中的方式类似。在二者中，排好序的数据在写入磁盘前都会在内存中缓冲，读操作必须合并内存和磁盘中的数据。 C-Store和Bigtable有很多相同的特性：二者都使用shared-nothing架构，且都有两种不同的数据结构，其中之一被最近的写入使用，另一个用来长期存储数据，并有将数据从一种格式转移到另一种格式的方法。但这连个系统的API有着很大的区别：C-Store像是一个关系型数据库，而Bigtable提供了较为底层的读写接口且被设计支持每台服务器每秒钟处理几千个这种请求。C-Store还是一个有读优化的关系型DBMS，而Bigtable为读敏感和写敏感的应用程序都提供了很好的性能。 Bigtable的负载均衡器解决了一些在同类型的shared-nothing数据库的负载均衡和内存均衡问题（例如参考文献[11, 35]）。我们的问题与之相比更简单一些：（1）我们不需要考虑可能由于视图或索引导致的相同数据有多个副本的情况；（2）我们让用户界定数据应该存储在内存中还是在磁盘中昂，而不是试图自动地为其做决策；（3）我们不需要执行或优化复杂的查询。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:12:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"11. 结论 我们描述了Bigtale，其是一个Google中用来存储结构化数据的分布式系统。Bigtable集群从2005年4月开始一直使用至今，我们在那之前大概花费了7人年的时间来设计并实现它。知道2006年8月，已经有超过60个项目使用了Bigtable。我们的用户喜欢Bigtable的实现提供了性能和高可用性，也因此用户可以在资源需求随时间变化时能够简单地通过在系统中添加更多机器的方式来提高集群的容量。 考虑到Bigtable的接口不太常见，我们的用户适配该系统的难度也是一个有趣的问题。新用户有时不确定怎样使用Bigtable的接口才能获得最好的效果，特别是当他们习惯使用支持通用事务的关系型数据库时。不过，事实上Google中使用Bigtable的很多产品的成功印证了在实际环境中我们的设计非常良好。 我们正在实现一些Bigtable的额外的特性，比如支持二级索引和构建具有多master副本的跨数据中心备份的Bigtable的基础设施。我们还开始将Bigtable作为产品服务部署，这样独立的团队就不需要维护自己的集群。随着我们的服务集群变大，我们需要解决更多Bigtable自身的资源共享问题[3, 5]。 最后，我们发现在Google构建自己的存储方案有很大的优势。在为Bigtable设计自己的数据模型时，我们有很大的灵活性。此外，我们对Bigtable实现的控制权和对Bigtable依赖的其他Google的基础设施的控制权意味着当效率低下时我们能够移除性能瓶颈。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:13:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"致谢 感谢匿名的审稿者、David Nagle和我们的领导者Brad Calder为本篇论文提供的反馈。Bigtable系统收到了很多Google内用户的反馈。另外，感谢以下的人为Bigtable做出的贡献：Dan Aguayo、Sameer Ajmani、Zhifeng Chen、Bill Coughran、Mike Epstein、Healfdene Goguen、Robert Griesemer、Jeremy Hylton、Josh Hyman、Alex Khesin、Joanna Kulik、Alberto Lerner、Sherry Listgarten、Mike Maloney、Eduardo Pinheiro、Kathy Polizzi、Frank Yellin、Arthur Zwiegincew。 ","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:14:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"参考文献 [1] ABADI, D. J., MADDEN, S. R., AND FERREIRA, M. C. Integrating compression and execution in columnoriented database systems. Proc. of SIGMOD (2006). [2] AILAMAKI, A., DEWITT, D. J., HILL, M. D., AND SKOUNAKIS, M. Weaving relations for cache performance. In The VLDB Journal (2001), pp. 169–180. [3] BANGA, G., DRUSCHEL, P., AND MOGUL, J. C. Resource containers: A new facility for resource management in server systems. In Proc. of the 3rd OSDI (Feb. 1999), pp. 45–58. [4] BARU, C. K., FECTEAU, G., GOYAL, A., HSIAO, H., JHINGRAN, A., PADMANABHAN, S., COPELAND, G. P., AND WILSON, W. G. DB2 parallel edition. IBM Systems Journal 34, 2 (1995), 292–322. [5] BAVIER, A., BOWMAN, M., CHUN, B., CULLER, D., KARLIN, S., PETERSON, L., ROSCOE, T., SPALINK, T., AND WAWRZONIAK, M. Operating system support for planetary-scale network services. In Proc. of the 1st NSDI (Mar. 2004), pp. 253–266. [6] BENTLEY, J. L., AND MCILROY, M. D. Data compression using long common strings. In Data Compression Conference (1999), pp. 287–295. [7] BLOOM, B. H. Space/time trade-offs in hash coding with allowable errors. CACM 13, 7 (1970), 422–426. [8] BURROWS, M. The Chubby lock service for looselycoupled distributed systems. In Proc. of the 7th OSDI (Nov. 2006). [9] CHANDRA, T., GRIESEMER, R., AND REDSTONE, J. Paxos made live — An engineering perspective. In Proc. of PODC (2007). [10] COMER, D. Ubiquitous B-tree. Computing Surveys 11, 2 (June 1979), 121–137. [11] COPELAND, G. P., ALEXANDER, W., BOUGHTER, E. E., AND KELLER, T. W. Data placement in Bubba. In Proc. of SIGMOD (1988), pp. 99–108. [12] DEAN, J., AND GHEMAWAT, S. MapReduce: Simplified data processing on large clusters. In Proc. of the 6th OSDI (Dec. 2004), pp. 137–150. [13] DEWITT, D., KATZ, R., OLKEN, F., SHAPIRO, L., STONEBRAKER, M., AND WOOD, D. Implementation techniques for main memory database systems. In Proc. of SIGMOD (June 1984), pp. 1–8. [14] DEWITT, D. J., AND GRAY, J. Parallel database systems: The future of high performance database systems. CACM 35, 6 (June 1992), 85–98. [15] FRENCH, C. D. One size fits all database architectures do not work for DSS. In Proc. of SIGMOD (May 1995), pp. 449–450. [16] GAWLICK, D., AND KINKADE, D. Varieties of concurrency control in IMS/VS fast path. Database Engineering Bulletin 8, 2 (1985), 3–10. [17] GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In Proc. of the 19th ACM SOSP (Dec. 2003), pp. 29–43. [18] GRAY, J. Notes on database operating systems. In Operating Systems — An Advanced Course, vol. 60 of Lecture Notes in Computer Science. Springer-Verlag, 1978. [19] GREER, R. Daytona and the fourth-generation language Cymbal. In Proc. of SIGMOD (1999), pp. 525–526. [20] HAGMANN, R. Reimplementing the Cedar file system using logging and group commit. In Proc. of the 11th SOSP (Dec. 1987), pp. 155–162. [21] HARTMAN, J. H., AND OUSTERHOUT, J. K. The Zebra striped network file system. In Proc. of the 14th SOSP (Asheville, NC, 1993), pp. 29–43. [22] KX.COM. kx.com/products/database.php. Product page. [23] LAMPORT, L. The part-time parliament. ACM TOCS 16, 2 (1998), 133–169. [24] MACCORMICK, J., MURPHY, N., NAJORK, M., THEKKATH, C. A., AND ZHOU, L. Boxwood: Abstractions as the foundation for storage infrastructure. In Proc. of the 6th OSDI (Dec. 2004), pp. 105–120. [25] MCCARTHY, J. Recursive functions of symbolic expressions and their computation by machine. CACM 3, 4 (Apr. 1960), 184–195. [26] O’NEIL, P., CHENG, E., GAWLICK, D., AND O’NEIL, E. The log-structured merge-tree (LSM-tree). Acta Inf. 33, 4 (1996), 351–385. [27] ORACLE.COM. www.oracle.com/technology/products/- database/clustering/index.html. Product page. [28] PIKE, R., DORWARD, S., GRIESEMER, R., AND QUINLAN, S. Interpreting the data: Parallel analysis with Sawzall. Scientific Programming Journal 13, 4 (2005), 227–298. [29] RATNASAMY, S., FRANCIS, P., HANDLEY, M., KARP, R., AND SHENKER, S. A scalable content-addressable network. In Proc. of SIGCOMM (Aug. 2001), pp. 161–172. [3","date":"2020-08-15","objectID":"/posts/paper-reading/bigtable-osdi06/:15:0","tags":["Bigtable","Translation"],"title":"《Bigtable: A Distributed Storage System for Structured Data》论文翻译（BigTable-OSDI06）","uri":"/posts/paper-reading/bigtable-osdi06/"},{"categories":["Paper Reading"],"content":"本篇文章是对论文MapReduce-OSDI04的原创翻译，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:0:0","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"作者 Jeffrey Dean and Sanjay Ghemawat jeff@google.com, sanjay@google.com Google, Inc ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:1:0","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"摘要 MapReduce是一个用来处理和生成大型数据集的编程模型和相关实现。用户需要指定map函数和reduce函数。map函数处理键值对并生成一组由键值对组成的中间值，reduce函数将所有键相同的中间值合并。就像本文中展示的那样，现实世界中的很多任务都可以通过这个模型表示。 以这种函数式风格编写的程序可以自动地作为并行程序在大型商用机集群上执行，运行时（run-time）系统负责对输入数据分区、在一系列机器间调度程序执行、处理机器故障、管理必要的机器间的通信。这让没有任何并行程序和分布式系统开发经验的编程人员能够轻松利用一个大型分布式系统的资源。 我们的MapReduce实现是高度可伸缩的，其运行在一个由商用机器组成的大型分布式集群上。通常，一个MapReduce计算会处理上千台机器上数TB的数据。每天都有数百个MapReduce程序提交的高达上千个MapReduce任务在Google集群上执行。开发人员认为这个系统非常易用。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:2:0","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"1. 引言 在过去的五年中，本文作者和其他在Google的开发者实现了数以百计的计算程序，以计算处理不同来源的大规模原始数据（如爬取到的文档、web请求日志等）。这些程序可能用来计算倒排索引（inverted index）、web文档在图论中的各种表示、每个主机爬取到的页面数量之和、给定的某天中查询最频繁的集合等等。虽然大部分的计算程序逻辑非常简单，但是由于其输入数据的规模通常很大，所以这些程序必须在成百上千台机器上分布式执行以在可可接受的时间内完成。解决并行计算、数据分布、故障处理等问题需要大量复杂的代码，让原本简单的问题不再简单。 为了应对这种复杂性，我们设计了一个新的程序抽象。其允许我们通过简单的描述表达我们要执行的计算，同时将并行化、容错、数据分布、负载均衡等细节隐藏在库中。我们的抽象收到了Lisp和许多其他函数式语言中的map和reduce原语的启发。我们意识到，我们大部分的计算都设计map操作和reduce操作。首先对输入数据中每条逻辑记录应用map操作以计算出一系列的中间键值对，然后对所有键相同的值应用reduce操作以合理地整合这些派生数据。用户可以自定义map和reduce操作，这让大型计算的并行化更为简单，且可以使用“重跑（re-execution）”的方法作为主要容错机制。 本工作的主要贡献为一个简单且功能强大的能实现自动并行化、高伸缩性分布式计算的的接口，和该接口在大型商用PC集群上的高性能的实现。 第二章描述了基本编程模型，并给出了几个例子。第三章描述了为我们基于集群的计算环境定制的MapReduce接口实现。第四章描述了该编程模型中我们认为有帮助的细节。第五章我们的实现在各种任务重的性能测试。第六章探究了MapReduce在Google中的使用，其中包括了我们以MapReduce为基础重写我们产品索引系统的经历。第七章探讨了相关工作与未来的工作。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:3:0","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"2. 编程模型 计算任务以一系列输入键值对作为输入，并产出一系列输出键值对作为输出。MapReduce库的用户将计算表示为两个函数：map和reduce。 用户编写的map函数将输入键值对处理为一系列中间键值对。MapReduce库将键相同的所有中间键值对的值与其对应的键$I$传递给reduce函数。 用户编写的reduce函数接收中间键值对的键$I$和该键对应的一系列值。它将这些值合并，并生产一个可能更小的一系列值。每个reduce函数调用通常产出0个或1个输出值。中间键值对中的值通过一个迭代器（iterator）供用户编写的reduce函数使用。这让我们能够处理因过大而无法放入内存中的值列表。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:4:0","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"2.1 示例 考虑如下一个问题：统计一个大量文档集合中每个单词出现的次数。用户会编写如下的伪代码。 map(String key, String value): // key: document name // value: document contents for each word w in value: EmitIntermediate(w, \"1\"); reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result)); map计算出每个单词与其（译注：在每个文档中的）出现的次数（在本例中为“1”）。reduce函数会求出每个单词出现次数的和。 另外，用户编写代码来一个mapreduce specification（规格/规范）对象，填写输入输出文件名和可选的调节参数。随后，用户调用MapReduce函数，将mapreduce specification对象作为参数传入。用户代码会被与MapReduce库（C++实现）链接到一起。附录A包含本示例的完整程序。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:4:1","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"2.2 类型 尽管前面的伪代码中使用了字符串作为输入输出类型，但理论上用户提供的map和reduce函数可以使用相关联的类型： map (k1,v1) -\u003e list(k2,v2) reduce (k2,list(v2)) -\u003e list(v2) 即输入的键和值与输出的键和值的类型域不同，而中间键与值和输出键域值的类型与相同。 在我们的C++实现中，我们通过字符串将接受或传入用户定义的函数的参数，将字符串与适当类型的转换留给用户代码去实现。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:4:2","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"2.3 更多示例 本节中，我们给出了一些简单的示例。这些示例是可以简单地通过MapReduce计算表示的有趣的程序。 分布式“grep”：如果一行文本匹配给定的模板，那么map函数会输出该行。reduce作为一个恒等函数，它仅将提供的中间数据复制到输出。 URL访问频率计数：map函数处理web网页请求日志，并按照$\u003cURL,1\u003e$输出。reduce函数对$URL$相同的值求和，并输出$\u003cURL,总数\u003e$键值对。 反转web链接拓扑图：map函数对名为$source$的页面中每个名为$target$的URL链接输出一个$\u003ctarget,source\u003e$键值对。reduce函数按照所有$target$相同的$source$合并为一个列表，并与其相应的URL关联，输出$\u003ctarget,list(source)\u003e$键值对。 每个主机的词向量统计：词向量是对是对一个或一系列文档中最重要的词的总结，其形式为$\u003c词,词频\u003e$键值对列表。map函数为每篇输入文档输出一个$\u003c主机名,词向量\u003e$键值对（其中$主机名$由文档到的URL解析而来）。reduce函数会受到对于给定的主机上每篇文章的所有的词向量。其将这些词向量加在一起，丢弃掉低频词，并最终输出$\u003c主机名,词向量\u003e$键值对。 倒排索引：map函数对每篇文档进行提取，输出一个$\u003c词,文档ID\u003e$的序列。reduce函数接受给定词的所有键值对，并按照$文档ID$排序。输出一个$\u003c词,list(文档ID)\u003e$键值对。所有输出的键值对的集合组成了一个简单的倒排索引。如果需要持续跟踪词的位置，仅需简单的增量计算。 分布式排序：map提取每条记录中的键，输出一个$\u003c键,记录\u003e$的键值对。reduce函数不对中间变量作修改直接输出所有的键值对。排序计算依赖章节4.1中介绍的分区机制和章节4.2介绍的排序属性。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:4:3","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"3. 实现 MapReduce接口可能有很多不同的实现。如何作出正确的选择取决于环境。例如，一种实现可能适合小型的共享内存的机器，一种实现可能适合大型NUMA多处理器主机，或者一种实现可能适合更大型的通过网络连接的机器集群。 本节中，我们将介绍一个中面向Google中常用的计算环境的实现。Google的常用计算环境为彼此通过交换机以太网[4]连接的大型商用PC集群。在我们的环境中： 机器通常使用双核x86处理器，2-4GB内存，运行Linux系统。 使用商用网络硬件：每台机器带宽通常为100Mbps或1Gbps，但平均分到的带宽要小得多。（译注：可能受交换机间带宽限制，每台机器平均分到的带宽远小于其单机带宽。） 一个集群由成百上千的机器组成，因此机器故障是常态。 存储由直接连接到独立的机器上IDE（译注：本文IDE指集成设备电路Intergated Drive Electronics）磁盘提供。我们为了管理这些磁盘上的数据，开发了一个内部的分布式文件系统[8]。该文件系统使用副本的方式在不可靠的硬件上提供了可用性和可靠性。 用户将工作（job）提交到一个调度系统中。每个工作由一系列的任务（task）组成，这些任务被*scheduler（调度器）*映射到集群中一系列可用的机器上。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:5:0","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"3.1 执行概览 输入数据会自动被分割为$M$个分片（split），这样，map函数调用可以在多个机器上分布式执行，每个输入的分片可以在不同机器上并行处理。中间键值对的键空间会通过被分区函数(例如，$hash(key) mod R$)分割为$R$个分区，这样，reduce函数也可以分布式执行。其中分区的数量（$R$）和分区函数由用户指定。 图1 执行概览图1 执行概览 \" 图1 执行概览 图1展示了在我们的实现中，MapReduce操作的完整工作流。当用户程序调用MapReduce函数时会发生如下的操作（下列序号与图1中序号对应）： 用户程序中的MapReduce库首先将输入文件划分为$M$个分片，通常每个分片为16MB到64MB（用户可通过可选参数控制）。随后，库会在集群中的机器上启动程序的一些副本。 这些程序的副本中，有一份很特殊，它是master副本。其他的副本是被master分配了任务的worker副本。总计要分配$M$个map任务和$R$个reduce任务。master选取闲置的worker并为每个选取的worker分配map或reduce任务。 被分配map任务的worker从输入数据分片中读取内容。其解析输入数据中的键值对，并将每个键值对传给用户定义的map函数。map函数输出的中间键值对在内存中缓存。 内存中缓存的键值对会定期地写入本地磁盘，写入的数据会被分区函数划分为$R$个区域。这些在磁盘中缓存的键值对的位置会被发送给master，master会将这些位置信息进一步传递给reduce worker。 当master通知reduce worker中间键值对的位置信息后，reduce worker会通过远程过程调用（译注：即RPC。）的方式从map worker的本地磁盘中读取缓存的数据。当reduce worker读取完所有中间数据后，它会对中间数据按照键进行排序，以便将所有键相同的键值对分为一组。因为通常来说，需对键不同的数据会被映射到同一个reduce任务中，所以需要对数据排序。如果中间数据总量过大以至于无法放入内存中，则会使用外排序算法（external sort）。 reduce worker遍历每一个遇到的中间键值对的，它会将键和该键对应的一系列值传递给用户定义的reduce函数。reduce函数的输出会被追加（append）到该reduce分区的最终输出文件中。 当所有的map和reduce任务都执行完毕后，master会唤醒用户程序。此时，调用MapReduce的调应用序会返回到用户代码中。 在成功执行完毕后，MapReduce的输出可在通过$R$个输出文件访问（每个reduce任务一个文件，文件名由用户指定）。通常情况下，用户不需要将这$R$个输出文件合并到一个文件中，用户经常将这些文件作为另一次MapReduce调用的输入，或者在另一个能够从多个分区的文件输入的分布式程序中使用这些文件。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:5:1","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"3.2 master数据结构 master中保存着多种数据类型。对每个map和reduce任务，master会存储其状态（状态包括等待中（idle）、执行中（in-progress）和完成（conpleted））和非等待中的任务对应的worker的标识符。 master是将中间文件区域的位置从map任务传递到reduce任务的管道。因此，对于每个已完成的map任务，master会存储其输出的$R$个中间文件区域的位置。当map任务完成后，master会收到其对中间文件区域位置和大小信息的更新。这些信息会被增量地推送到有执行中的reduce任务的worker中。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:5:2","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"3.3 容错 因为MapReduce库是为使用成百上千台机器处理大规模数据提供帮助而设计的，所以必须能够优雅地对机器故障进行容错。 3.3.1 worker故障 master会定期ping每个worker。如果在一定时间内没有收到worker的响应，master会将该worker标记为故障。被故障的worker处理的已完成的map任务会被重设为其初始的“等待中”的状态，因此其符合被调度到其他worker的条件。同样，在故障的worker上任何执行中的map或reduce任务也会被重设为“等待中”的状态，符合重新调度的条件。 当worker故障发生时，该worker完成的map任务也需要被重新执行，因为map任务的输出被存储在故障的机器的本地磁盘上，无法被访问。故障worker完成的reduce任务则不需要被重新执行，因为他们的输出被存储在全局的文件系统中 当一个起初被worker A执行的map任务因A发生故障而随后被worker B执行时，所有正在执行reduce任务的worker会被告知这个map任务被重新执行。任何没从worker A中读取完数据的reduce任务将会从worker B中读取数据。 MapReduce可以弹性处理大规模worker故障。例如，在MapReduce操作中，由于在正在运行的集群中的网络维护工作导致了80台机器在几分钟内同时变得不可访问。MapReduce的master会简单地重新执行不可访问的worker的机器上已完成的工作，并继续执行后续任务，最终完成整个MapReduce操作。 3.3.2 master故障 我们让master简单地周期性地为之前提到的master中的数据结构设置检查点。如果master 任务挂掉，一份新的master的拷贝会从最后一次检查点的状态重启。尽管只有一个master，发生故障的可能性也很小。因此，目前我们的实现方式为：如果master故障，则终止MapReduce计算。client可以检测到该状态，如果有需要可以重试MapReduce操作。 3.3.3 故障出现时的语义 对于相同的输入数据，当用户提供的map和reduce操作是确定性函数时（译注：确定性函数指在任何时候，当函数输入相同时，总会得到相同的输出。），分布式的MapReduce输出的数据和一个没发生故障的顺序执行的程序输出的数据是一样的。 我们通过原子性地提交map任务和reduce任务输出的方式来实现这一性质。每个执行中的任务将其输出写入到私有的临时文件中。每个reduce任务会创建一个这样的临时文件，每个map任务会创建$R$个这样的临时文件（每有一个reduce任务就创建一个）。当有一个map任务完成时，该worker会向master发送一条带有$R$个临时文件名的消息。如果master收到了一个已经完成过的map任务的完成消息，master会忽略该消息。否则，master会在其数据结构中记录这$R$个文件的文件名。 当有一个reduce任务完成时，该worker会自动地将其临时输出文件重命名为一个永久的文件名。如果同一个reduce任务被在多台机器中执行，会出现多个重命名调用将文件重命名同一个永久文件名的情况。我们依赖下层文件系统提供了原子性重命名操作，来保证最终的文件系统中仅包含来自一次reduce任务输出的数据。 我们绝大多数map和reduce操作是确定性的。因此，分布式的MapReduce语义等同于顺序执行的语义。这使得编程人员可以很容易地理解程序行为。当map和（或）reduce为非确定性函数时，我们提供了较弱但仍合理的语义。当非确定性的操作出现时，一次特定的reduce任务的输出$R_{1}$等同于这个非确定性操作顺序执行的输出$R_{1}$。但是，不同次*reduce*任务的输出$R_{2}$可能对应这个非确定性操作顺序不同次执行的输出$R_{2}$。 考虑这样一种情况，有map任务$M$和reduce任务$R_{1}$和$R_{2}$。$e(R_{i})$表示被提交的任务$R_{i}$的执行过程（有且仅有一个该执行过程）。因为$e(R_{1})$与$e(R_{2})$可能读取了任务$M$的不同次执行后的输出文件，因此会出现较弱的语义。（译注：即如果$M$因故障等原因被多次执行，因为$M$多次执行的输出不一致，所以$R_{1}$和$R_{2}$读取的输入可能不一致。） ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:5:3","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"3.4 位置分配 在我们的计算环境中，网络带宽是相对稀缺的资源。为了节约网络带宽，我们将输入数据（由GFS管理[8]）存储在集群中机器的本地磁盘中。GFS将每个文件分割为若干个64MB的块，并为每个块存储在不同机器上若干个副本（通常为3个）。MapReduce的master会考虑输入文件的位置信息，并试图在持有输入文件的副本的机器上分配相应的map任务。如果分配失败，master会试图将map任务分配在离其输入文件的副本较近的机器上（例如，在与持有输入数据副本的机器在相同交换机下的机器上分配）。在集群中较大比例的机器上运行大型MapReduce操作时，大部分输入数据都是从本地读取，不消耗网络带宽。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:5:4","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"3.5 任务粒度 如前文所述，我们将map阶段进一步划分为$M$份，将reduce阶段进一步划分为$R$份。在理想状态下，$M$和$R$应远大于worker的机器数。让每个worker执行多个不同的任务可以提高动态负载均衡能力，也可以在一个worker故障时提高恢复速度：该worker完成的多个map任务可以被分散到所有其他的worker机器上执行（译注：否则，考虑$M$小于worker机器数的情况，每个worker上只有一个任务，如果一个worker故障，那么该worker中完成的任务只能在另一台worker机器上重跑，无法充分利用并行的性能）。 在我们的MapReduce实现的实际情况中，对$M$和$R$的上限进行了限制。如前文所述，master必须做出$O(M+R)$个调度决策，并在内存中保存$O(M \\times R)$个状态。（内存占用的常数因子比较小：$O(M \\times R)$条状态由大约每个map/reduce任务仅一字节的数据组成。） 此外，$R$还经常受用户限制，因为每个reduce任务会生成一个单独的输出文件。在实际情况下，我们更倾向于自定义参数$M$，这样可以使每个单独的任务的输入数据大概在16MB到64MB（这样可以使前面提到的局部性优化最有效），同时，我们使$R$是期望使用的worker机器的较小的倍数。我们经常在$2,000$台机器上选择$M=200,000$、$R=5,000$的参数执行MapReduce计算。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:5:5","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"3.6 任务副本 延长MapReduce操作总时间的常见原因之一为“离群问题”：一个机器花费了不寻常的长时间完成计算中最后的几个map任务或reduce任务。吃线离群问题的原因有很多。例如，一台磁盘情况不良的机器可能频繁修正磁盘错误，导致其读取速度从$30MB/s$降低到$1MB/s$。集群的调度系统可能已经将其他任务调度到了该机器上，导致其因CPU、内存、本地磁盘或网络带宽等因素执行MapReduce代码更慢。我们最近遇到的问题是在机器初始化代码中的一个bug，其导致了处理器缓存被禁用，受影响的机器上的计算慢了超过100倍。 我们有一个通用的机制来避免离群问题。当MapReduce操作将要完成时，master会通过调度对仍在执行中的任务创建副本并执行。当原任务和其副本之一执行完成时，该任务会被标记为已完成。我们对这个机制进行了一些调优，使它通常情况下对计算资源的占用仅提高几个百分点。我们发现这个机制显著地减少了完成大型MapReduce操作的时间。例如，章节5.3中的排序程序在禁用任务副本机制时，完成时间延长了44%。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:5:6","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"4. 改进 尽管编写map和reduce函数提供的基本功能已经能够满足大多数场景下的需求，有一些扩展功能还是会提供很大帮助。我们将在本节中讨论这些扩展。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:6:0","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"4.1 分区函数 MapReduce的用户可以自定义其需要的reduce的任务或输出文件数（$R$）。分区函数（partitioning function）通过中间键值对的键将数据为每个任务分区。我们提供了使用哈希函数（例如，$hash(key) mod R$）的默认分区函数。使用该函数往往会得到比较平衡的分区。然而，在有些情况下，通过某些其他的函数按照键分区很有用。例如，有时输出的键为URL，我们希望所有来自同一个主机的条目最终会被输出到相同的文件中。为了支持类似情况，MapReduce库的用户可以提供一个自定义的分区函数。例如，使用”$hash(hostname(urlkey)) mod R$“作为分区函数可以使来自同一个主机的所有URL最终输出到同一个文件中。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:6:1","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"4.2 有序性保证 我们保证在一个给定的分区中，中间键值对是按照键的升序进行处理的。这种有序性保证使每个分区生成有序的输出变得非常简单。这对于输出文件格式需要支持按照键进行高效的随机访问等情况时十分有用。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:6:2","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"4.3 合并函数 在一些情况下，map任务会产出很多键相同的中间键值对，且用户定义的reduce函数服从交换律和结合律。章节2.1中的单词计数就是一个很好的例子。因为词频往往服从Zipf分布（齐夫定律），每个map任务会产出成百上千条$\u003cthe,1\u003e$的记录。所有的这些计数记录会被通过网络发送到同一个reduce任务，并随后被reduce函数加在一起得到一个总数。我们允许用户自定义一个可选的合并函数（combiner function），在数据通过网络发送前对这部分数据进行合并。 合并函数会在每个执行map任务的机器上执行。通常，实现合并函数和reduce函数的代码是相同的。合并函数和reduce函数唯一的区别是MapReduce库处理函数输出的方式。reduce函数的输出会被写入最终输出文件。合并函数的输出会被写入中间文件，随后中间文件会被发送给reduce任务。 部分数据的合并显著地提高了某些类型的MapReduce操作的速度。附录A包含了一个使用了合并函数的例子。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:6:3","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"4.4 输入输出类型 MapReduce库提供了以多种格式读取输入数据的支持。例如，“text”模式将每一行作为一个键值对输入：其中键为行号，值为行的内容。另一种支持的常见的格式被存储为按键排序的键值对序列。每个输入类型的实现都知道如何将输入划分为有意义的区间，以便分开交给map任务处理（例如，“text”模式的区间划分保证仅在行分隔符处划分区间）。用户可以通过提供类似reader接口的实现的方式来增加对新的输入类型的支持，虽然大部分用户仅使用了预支持输入类型中的一小部分。 reader并非必须从文件读取数据。例如，我们可以很容易地实现一个从数据库或从内存中映射的数据结构中读取记录的reader。 类似地，我们也提供了一系列可以将数据输出位不同格式的输出类型，且也可以很容易地通过哦用户代码添加新的输出类型支持。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:6:4","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"4.5 附属输出 在一些情况下，MapReduce的用户可以很方便地通过map和（或）reduce操作生成附属输出文件作为额外的输出。我们依赖应用程序的writer来使这种操作具有原子性（atomic）与幂等性（idempotent）。通常，应用程序将数据写入到一个临时文件，并在该文件完全生成完成后原子性地将该文件重命名。 我们没有对一个任务生产多个输出文件提供原子性的两段提交协议（two-phase commits，2PC）支持。因此，产生多个输出文件且有跨文件一致性需求的任务应该具有“确定性（译注，如章节3.3.3）”。但在实际环境中，这一限制并不是什么问题。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:6:5","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"4.6 跳过损坏的记录 有时，用户代码中的bug会导致map或reduce函数在处理某些记录时会发成确定性地崩溃。这种bug导致MapReduce操作无法完成。这种情况下，通常的处理方式是修复这个bug，但有时这样并不可行，因为bug可能在无法访问源码的第三方库中。同时，有些时候忽略一些记录时可以接受的，例如在大规模数据集中进行统计分析时。为此，我们提供了一种可选的执行模式，该模式下MapReduce库可以检测会导致确定性崩溃的记录并跳过这些记录，以让处理进程能够继续执行。 每个worker进程会安装一个捕捉段违规（segmentation violation）和总线错误（bus error）的处理器。再调用用户的map或reduce操作之前，MapReduce库会在全局变量中存储参数的编号。如果用户代码产生了一个信号，信号处理器会向master发送一个含有该编号的“last gasp（奄奄一息）”UDP包。当master在同一条记录上收到超过一个故障时，master会在下一次重新执行相关map任务或reduce任务时指示跳过该记录。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:6:6","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"4.7 本地执行 调试map或reduce函数中的bug是非常棘手的，因为它们实际运行在分布式系统中，且其经常运行在几千台机器上并由master动态地决定任务的分配。为了帮助开发者debug、分析和小规模测试，我们开发了一个MapReduce库的替代实现，其可以在一台本地机器上顺序的执行所有MapReduce操作。用户可以仅执行计算任务中的几个特定的map任务。用户仅需使用一个特殊的标识符调用程序，就可以轻松地使用任何调试工具或测试工具（如gdb）。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:6:7","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"4.8 状态信息 master会运行一个内部的HTTP服务器，并将一系列的状态信息页面暴露给用户。这些页面会展示计算的进度，如多少个任务已经完成、多少个任务正在执行、输入的字节数、中间数据的字节数、输出的字节数、处理速度等。这些页面中还包含展示每个任务输出的标准错误和标准输出文件的页面链接。用户可通过这些数据预测计算需要消耗多长时间、需不需要为计算任务增加额外的资源。这些页面也可以用来发现计算是否比预期慢很多。 另外，顶级的状态页面展示了哪些worker执行失败了与它们失败时运行的map任务和reduce任务。这些信息对诊断用户代码中的bug十分有帮助。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:6:8","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"4.9 计数器 MapReduce库提供了用来计数不同事件发生次数的计数器。例如，用户代码可能希望对处理的词数或者索引的德语文档计数等。 若使用计数器，用户代码需要创建一个命名的计数器对象并在map和（或）reduce函数中适当地增加计数器计数。例如： Counter* uppercase; uppercase = GetCounter(\"uppercase\"); map(String name, String contents): for each word w in contents: if (IsCapitalized(w)): uppercase-\u003eIncrement(); EmitIntermediate(w, \"1\"); 每个worker机器会周期性地将计数器的值传给master（通过ping的响应报文携带）。master将成功执行的map和reduce任务中的计数器的值加在一起，并在MapReduce操作完成时将其返回给用户代码。计数器当前的值同样在master的分析页面中显示，这样用户就可以查看实时的计算进度。master在对计数器求和时，会对多次执行的相同的map或reduce任务中的计数器值去重，以避免重复计数。（任务副本和因故障被重新执行的任务都会导致重复执行。） 有些计数器的值被MapReduce库自动维护，如处理过的输入键值对的数量或生成的输出键值对的数量。 对用户而言，计数器对检查MapReduce操作是否完成非常有帮助。例如，在有些MapReduce操作中，用户代码可能希望保证输出键值对的数量和输入键值对的数量正好相等，或者想保证处理过的德语文档在总数中占的比例是否在允许的范围内。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:6:9","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"5. 性能 在本章中，我们将测量大规模集群中的两个MapReduce计算的性能。其中一个计算任务是在大约1TB的数据中按照一个模板（pattern）搜索。另一个计算任务时排序大约1TB的数据。 这两个程序都代表了用户编写的真实程序中占比很大的两类子集。其中一类程序是将数据从一种表示变换到另一种表示，另一类程序是从大规模数据集中提取少量感兴趣的数据。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:7:0","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"5.1 集群配置 所有的程序都在一个由大约1800台机器的集群中执行。每台机器有两个开启了超线程的$GHz的Intel至强Xeon处理器、4GB内存、2个160GB的IDE硬盘和1Gbps的以太网连接。这些机器组成了双层树状的交换机网络，根节点总带宽约100~200Gbps。所有机器都在同一个中心托管，因此任何两个机器间往返时延（RTT）小于1ms。 在4GB内存中，有大约1~1.5GB内存被集群为了运行其他任务保留。这些程序是在一个周末的下午执行的，那时CPU、磁盘和网络几乎都处于空闲状态。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:7:1","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"5.2 grep grep程序会扫描$10^{10}$条100B的记录，以搜索匹配一个相对较少的三个字母的模板（92,337条记录命中）。输入数据被分割为约64MB的分片（$M=15000$），所有的输出被放置在一个文件中（$R=1$）。 图2展示了计算进度随时间的变化。Y轴展示了输入数据被扫描的速率。随着分配给MapReduce计算的机器越来越多，其速度也逐渐提高。当有1764个worker被分配到该任务时，速率峰值超过了30GB/s。当map任务完成时，速率开始逐渐下降并在整个计算时间的大概第80s时下降到0。整个计算从开始到结束大概消耗了150s。这包括了大概一分钟的启动时间开销。这一开销的原因是程序需要传播到所有worker机器与打开1000个输入文件并获取局部优化所需的信息时与GFS交互的时延。 图2 数据传输速率随时间变化图图2 数据传输速率随时间变化图 \" 图2 数据传输速率随时间变化图 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:7:2","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"5.3 sort sort程序会对$10^{10}$条100B的记录进行排序（大约1TB的数据）。这个程序是模仿TeraSort的benchmark程序[10]构建的。 排序程序的用户代码少于50行。三行的map函数从一行文本中提取一个10字节的排序用的键，并将这个键与原始文本作为中间键值对输出。我们使用了一个内建的恒等函数作为reduce操作。这个函数不对中间键值对就行修改，直接作为输出键值对传递。最终排序的输出被写入一系列2副本的GFS文件中（即，程序输出总计写入了2TB）。 与前者相同，输入数据被分割为64MB的分片（$M=15000$）。我们将排序的输出分区到4000个文件中（$R=4000$）。分区函数根据键的首字节将其划分到$R$个分区之一中。 该benchmark的分区函数内建了键的分布情况。在通常的排序程序中，我们会增加一个提前执行的MapReduce操作，该操作会采集一些键的样本，并通过这些样本来计算最终排序时的分割点。 **图3(a)**展示了以普通方式执行时程序的进度。左上角的图表展示了输入数据读取的速率。速率的峰值达到大概13GB/s，随后快速下降，因为所有哦map任务都在大概第200秒前完成。需要注意的是该程序数据输入速率比grep低。这是因为sort的map任务消耗了大概一半的时间和I/O带宽用于将中间数据写入到本地磁盘，而grep的中间数据大小几乎可以忽略不计。 左侧中间的图表展示了数据通过网络从map任务发送到reduce任务的速率。该数据转移（shuffle）在第一个map任务完成时便开始。图表中第一个峰中的数据转移是为了第一批约1700个reduce任务（整个MapReduce被分配到1700台机器上，每台机器同时最多执行1个reduce任务）。在整个计算任务的大概第300秒时，部分第一批reduce任务完成了，并开始为剩余的reduce任务转移数据。所有的数据转移在整个计算的大概第600秒是完成。 左下角的图表展示了排好序的数据被reduce任务写入最终输出文件的速率。在第一个数据转移阶段和数据开始被reduce任务写入到最终文件间有一段延时，这是因为这期间机器都在忙于排序中间数据。写入操作以2~4GB/s的速率持续了一段时间，在整个计算过程的大概第850秒时完成了数据写入。算上启动的开销，整个计算过程消耗了891秒。这与目前在TeraSort benchmark中报道的最佳结果1057秒非常接近[18]。 这有一些需要注意的点：由于我们的局部性优化，大部分数据直接从本地磁盘读取，绕过了带宽相对受限的玩过，所以数据输入速率比数据转移速率高。由于数据输出阶段写入了两份排好序的数据的副本，所以数据转移的速率比输出的速率高（为了可靠性和可用性，我们为输出数据设置了两份副本）。我们的下层文件系统为了可靠性和可用性的考虑而写入了两份副本。如果我们使用擦除编码（erasure code）[14]的方式而不是副本的方式，写入数据时网络带宽的需求会减少。 图3 排序程序不同种执行方式中数据传输速率随时间的变化图图3 排序程序不同种执行方式中数据传输速率随时间的变化图 \" 图3 排序程序不同种执行方式中数据传输速率随时间的变化图 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:7:3","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"5.4 任务副本的影响 **图3(b)展示了禁用了任务副本后的sort程序执行情况。其执行流程与图3(a)**中的类似，除了最后有撑场一段时间几乎没有写入发生。在960秒后，除了5个reduce任务外其他所有任务都已经完成了。然而，最后这些离群的任务在300秒后才执行完毕。整个计算过程消耗了1283秒，增加了44%的运行时间。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:7:4","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"5.5 机器故障 图3(c)中，我们展示了在执行sort程序时，我们故意在计算开始的几分钟后里杀死了1746个worker中的200个时，程序的执行进度情况。下层的集群调度器立刻在这些机器上重启了新的worker进程（因为仅杀死了进程，机器还在正常运行）。 因为当worker被杀死后，一些之前已经完成了的map任务消失且需要被重新执行，所以对输入速率有负面影响。重新执行的map任务相对比较快。算上启动的开销，整个计算过程在993秒内完成（仅比正常执行时增加了5%）。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:7:5","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"6. 研发经历 我们在2003年2月编写了第一个版本的MapReduce库，并在2003年8月对其进行了大幅增强，包括局部性优化、跨worker机器的动态负载均衡等。从那时起，我们便惊喜的发现在处理各种问题时MapReduce库的应用之广。MapReduce库在Google内部被广泛应用于各种领域，包括： 大规模机器学习问题； Google News和Froogle产品的聚类问题； 提取数据用于生成热门查询报告（例如，Google Zeitgeist）； 为了新的实验和产品提取网页属性（例如，从大量的网页语料库中提取地理位置信息，用于本地化搜索）； 大规模图运算。 图4 MapReduce实例数随时间变化图图4 MapReduce实例数随时间变化图 \" 图4 MapReduce实例数随时间变化图 图4中可见，在我们的主源代码管理系统中，独立的MapReduce程序随时间大幅增长。其数量从2003年初的0个增长到2004年9月末的几乎800个独立实例。MapReduce取得了很大的成功，它可以让用户仅编写简单的代码即可在半小时内在上千台机器上高效运行，这大大的提高了开发和设计周期。此外，MapReduce让没有任何分布式和（或）并行系统编程经验的开发者能够轻松利用大量资源。 在每个工作的最后，MapReduce库会记录该工作使用的计算资源的统计数据。表1展示了Google在2004年8月运行的MapReduce工作的子集的统计数据。 表1 2004年8月运行的MapReduce工作情况 工作数 平均工作完成时间 使用的机器工作量 29,423 634 secs 79,186 days 读取的输入数据量 生成的中间数据量 写入的输出数据量 3,288 TB 758 TB 193 TB 平均每个工作使用的worker机器数 平均每个工作故障机器数 平均每个工作map任务数 平均每个工作reduce任务数 157 1.2 3,351 55 不同的map实现数量 不同reduce实现数量 不同map/reduce组合数量 395 269 426 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:8:0","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"6.1 大规模索引 目前，我们使用MapReduce做的最重要的工作之一是完全重写了一个索引系统，该系统被用作生成用于Google web搜索服务的数据结构。该索引系统将大量被我们爬虫系统检索到的文档（作为GFS文件存储）作为输入。这些文档的原始内容的数据大小超过20TB。索引进程会运行一系列5~10个MapReduce操作。使用MapReduce（而不是旧版索引系统中ad-hoc分布式传递方案）提供了很多好处： 索引代码更简单、短、便于理解，因为处理容错、分布式和并行的代码被隐藏在了MapReduce库中。例如，计算中的有一个阶段的代码量从3800行C++代码所见到了700行使用MapReduce的代码。 MapReduce库的性能足够好，这让我们可以将概念上不相关的计算分离开，而不是将它们混合在一起，这样可以避免传递过多额外的数据。这使改变索引程序变得非常简单。例如，在我们旧的索引系统中，一处修改会花费几个月的时间，而新的系统仅需要几天就能实现。 索引系统变得更容易操作。大部分因机器故障、缓慢的机器、网络不稳定等引起的问题都被MapReduce库自动处理了，不需要引入额外的操作。此外，向索引集群添加新机器以获得更好的性能变得更加简单。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:8:1","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"7. 相关工作 许多系统提供了受限制的编程模型，并通过这些限制来进行自动化并行计算。例如，使用并行前缀和计算（parallel prefix computation）[6, 9, 13]，可以使用$N$个处理器上在$O(logN)$的时间内计算有$N$个元素的数组中所有前缀和。MapReduce可被看做是对一些这类模型基于我们在现实世界中对大型计算的经验做出的简化和升华。更重要的是，我们提供了适用于大规模的数千个处理器的带有容错机制的实现。相反，大部分并行处理系统仅被小规模使用，且将处理机器故障的细节留给了开发者。 BSP模型（Bulk Synchronous Programming）[17]和一些MPI（Message Passing Interface，消息传递接口）[11]原语提供了让开发者编写并行程序更简单的高层抽象。这些系统和MapReduce的关键区别在于MapReduce提供了一个受限的编程模型，以自动地并行化用户程序，并提供了透明的容错机制。 我们的局部性优化的灵感来自于如活动磁盘（active disk）[12, 15]技术，即计算程序被推送到靠近本地磁盘的处理设备中，这减少了I/O子系统或者网络的总数据发送量。我们在直连少量磁盘的商用处理器上运行程序，而不是直接在磁盘控制处理器上运行，但最终目的都是一样的。 我们的任务副本机制类似Charlotte System[3]中使用的Eager调度机制。简单的Eager调度的一个缺点是，当一个任务反复故障时，整个计算都无法完成。我们通过跳过损坏记录的方式来解决导致该问题的一些情况。 MapReduce的实现依赖了一个内部的集群管理系统，该系统负责在大量共享的机器上分配并运行用户任务。该系统比较神似如Condor[16]的其他系统，但这并不是本文的重点。 MapReduce中的排序机制在操作上类似NOW-Sort[1]。源机器（map worker）将待排序的数据分区，并将其发送到$R$个reduce worker之一。每个reduce worker将其数据在本地排序（如果可以，会在内存中执行）。当然，NOW-Sort不支持用户自定义map和reduce函数，这让我们的库适用范围更广。 River[2]提供了一个通过分布式队列发送数据来处理程序间交互的编程模型。就像MapReduce，River系统试图在存在由异构硬件或系统干扰导致的性能不均匀的情况下提供良好的平均性能。River通过小心地调度磁盘和网络传输以使计算时间平衡的方式实现这一点。而MapReduce框架通过对编程模型进行限制，将问题划分为大量更细致的任务。这些任务在可用的worker间动态调度，以让更快的worker处理更多任务。这种受限的编程模型还允许在工作末期调度冗余执行的任务，这样可以大大缩减离群机器（如慢速或者卡死的worker）中的计算时间。 BAD-FS[5]采用了和MapReduce区别非常大的编程模型。与MapReduce不同，BAD-FS的目标是在广域网中执行工作。然而，有两个基本点很相似。（1）二者都使用了冗余执行的方式恢复因故障丢失的数据。（2）二者都使用了有位置感知（locality-aware）调度方式来减少拥堵的网络连接中数据发送的总量。 TACC[7]是一个为简化高可用网络服务设计的系统。像MapReduce一样，TACC依赖重新执行的方式作为容错机制。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:9:0","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"8. 结论 MapReduce编程模型被成功应用于Google中的很多目标。我们将这种成功归结于几个原因。第一，因为该模型隐藏了并行化、容错、本地优化和复杂均衡的细节，所以甚至没有相关经验的程序员都可以轻松使用。第二，很多不同的问题都可以被表示为MapReduce计算。例如，MapReduce在Google的生产系统的web搜索服务、排序、数据挖掘、机器学习和很多其他系统中被作为数据生成工具使用。第三，我们开发了一个适用于由上千台机器组成的大型集群的MapReduce实现。该实现可以高效利用这些机器的资源，因此其非常适用于Google中的大型计算问题。 我们从这项工作中学习到了很多事。第一，对编程模型进行限制可以让并行化、分布式计算、容错等更加简单。第二，网络带宽是非常稀缺的资源。我们系统中的大量优化都是为了减少网络发送的数据量：局部性优化允许我们从本地磁盘读取数据，在本地磁盘中写单个中间数据的副本同样节约了网络带宽。第三，冗余执行可以用来减少缓慢的机器带俩的影响，并可以用来处理机器故障和数据丢失。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:10:0","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"致谢 Josh Levenberg在修订和扩展用户级MapReduce API方面提供了很大帮助，他根据自己对MapReduce的使用经验和其他人对功能增强的建议，提供了很多新特性。MapReduce从GFS[8]读取输入并写入输出。感谢Mohit Aron, Howard Gobioff, Markus Gutschke, David Kramer, Shun-Tak Leung和Josh Redstone在开发GFS中做出做出的工作。同样感谢Percy Liang和Olcan Sercinoglu在MapReduce使用的集群管理系统中做出的工作。Mike Burrows, Wilson Hsieh, Josh Levenberg, Sharon Perl, Rob Pike和Debby Wallach为本文的早期草稿提供了有帮助的评论。OSDI的匿名审稿者和我们的领导者Eric Brewer对本文的改进提供了帮助。最后，我们希望感谢来自Google工程师的MapReduce使用者，他们给出了很多有帮助的反馈、建议和bug报告。 ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:11:0","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"参考文献 [1] Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, David E. Culler, Joseph M. Hellerstein, and David A. Patterson. High-performance sorting on networks of workstations. In Proceedings of the 1997 ACM SIGMOD International Conference on Management of Data, Tucson, Arizona, May 1997. [2] Remzi H. Arpaci-Dusseau, Eric Anderson, Noah Treuhaft, David E. Culler, Joseph M. Hellerstein, David Patterson, and Kathy Yelick. Cluster I/O with River: Making the fast case common. In Proceedings of the Sixth Workshop on Input/Output in Parallel and Distributed Systems (IOPADS ’99), pages 10–22, Atlanta, Georgia, May 1999. [3] Arash Baratloo, Mehmet Karaul, Zvi Kedem, and Peter Wyckoff. Charlotte: Metacomputing on the web. In Proceedings of the 9th International Conference on Parallel and Distributed Computing Systems, 1996. [4] Luiz A. Barroso, Jeffrey Dean, and Urs Holzle. ¨ Web search for a planet: The Google cluster architecture. IEEE Micro, 23(2):22–28, April 2003. [5] John Bent, Douglas Thain, Andrea C.Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, and Miron Livny. Explicit control in a batch-aware distributed file system. In Proceedings of the 1st USENIX Symposium on Networked Systems Design and Implementation NSDI, March 2004. [6] Guy E. Blelloch. Scans as primitive parallel operations. IEEE Transactions on Computers, C-38(11), November 1989. [7] Armando Fox, Steven D. Gribble, Yatin Chawathe, Eric A. Brewer, and Paul Gauthier. Cluster-based scalable network services. In Proceedings of the 16th ACM Symposium on Operating System Principles, pages 78–91, Saint-Malo, France, 1997. [8] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. The Google file system. In 19th Symposium on Operating Systems Principles, pages 29–43, Lake George, New York, 2003. [9] S. Gorlatch. Systematic efficient parallelization of scan and other list homomorphisms. In L. Bouge, P. Fraigniaud, A. Mignotte, and Y. Robert, editors, Euro-Par’96. Parallel Processing, Lecture Notes in Computer Science 1124, pages 401–408. Springer-Verlag, 1996. [10] Jim Gray. Sort benchmark home page. http://research.microsoft.com/barc/SortBenchmark/. [11] William Gropp, Ewing Lusk, and Anthony Skjellum. Using MPI: Portable Parallel Programming with the Message-Passing Interface. MIT Press, Cambridge, MA, 1999. [12] L. Huston, R. Sukthankar, R. Wickremesinghe, M. Satyanarayanan, G. R. Ganger, E. Riedel, and A. Ailamaki. Diamond: A storage architecture for early discard in interactive search. In Proceedings of the 2004 USENIX File and Storage Technologies FAST Conference, April 2004. [13] Richard E. Ladner and Michael J. Fischer. Parallel prefix computation. Journal of the ACM, 27(4):831–838, 1980. [14] Michael O. Rabin. Efficient dispersal of information for security, load balancing and fault tolerance. Journal of the ACM, 36(2):335–348, 1989. [15] Erik Riedel, Christos Faloutsos, Garth A. Gibson, and David Nagle. Active disks for large-scale data processing. IEEE Computer, pages 68–74, June 2001. [16] Douglas Thain, Todd Tannenbaum, and Miron Livny. Distributed computing in practice: The Condor experience. Concurrency and Computation: Practice and Experience, 2004. [17] L. G. Valiant. A bridging model for parallel computation. Communications of the ACM, 33(8):103–111, 1997. [18] Jim Wyllie. Spsort: How to sort a terabyte quickly. http://alme1.almaden.ibm.com/cs/spsort.pdf. ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:12:0","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"附录A 词频统计 本节包含了一个对通过命令行指定的一系列输入文件中每个单词出现次数技术的程序。 #include \"mapreduce/mapreduce.h\" // User’s map function class WordCounter : public Mapper { public: virtual void Map(const MapInput\u0026 input) { const string\u0026 text = input.value(); const int n = text.size(); for (int i = 0; i \u003c n; ) { // Skip past leading whitespace while ((i \u003c n) \u0026\u0026 isspace(text[i])) i++; // Find word end int start = i; while ((i \u003c n) \u0026\u0026 !isspace(text[i])) i++; if (start \u003c i) Emit(text.substr(start,i-start),\"1\"); } } }; REGISTER_MAPPER(WordCounter); // User’s reduce function class Adder : public Reducer { virtual void Reduce(ReduceInput* input) { // Iterate over all entries with the // same key and add the values int64 value = 0; while (!input-\u003edone()) { value += StringToInt(input-\u003evalue()); input-\u003eNextValue(); } // Emit sum for input-\u003ekey() Emit(IntToString(value)); } }; REGISTER_REDUCER(Adder); int main(int argc, char** argv) { ParseCommandLineFlags(argc, argv); MapReduceSpecification spec; // Store list of input files into \"spec\" for (int i = 1; i \u003c argc; i++) { MapReduceInput* input = spec.add_input(); input-\u003eset_format(\"text\"); input-\u003eset_filepattern(argv[i]); input-\u003eset_mapper_class(\"WordCounter\"); } // Specify the output files: // /gfs/test/freq-00000-of-00100 // /gfs/test/freq-00001-of-00100 // ... MapReduceOutput* out = spec.output(); out-\u003eset_filebase(\"/gfs/test/freq\"); out-\u003eset_num_tasks(100); out-\u003eset_format(\"text\"); out-\u003eset_reducer_class(\"Adder\"); // Optional: do partial sums within map // tasks to save network bandwidth out-\u003eset_combiner_class(\"Adder\"); // Tuning parameters: use at most 2000 // machines and 100 MB of memory per task spec.set_machines(2000); spec.set_map_megabytes(100); spec.set_reduce_megabytes(100); // Now run it MapReduceResult result; if (!MapReduce(spec, \u0026result)) abort(); // Done: ’result’ structure contains info // about counters, time taken, number of // machines used, etc. return 0; } ","date":"2020-08-08","objectID":"/posts/paper-reading/mapreduce-osdi04/:13:0","tags":["MapReduce","Translation"],"title":"《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）","uri":"/posts/paper-reading/mapreduce-osdi04/"},{"categories":["Paper Reading"],"content":"本篇文章是对论文GFS-SOSP2003的原创翻译，转载请严格遵守CC BY-NC-SA协议。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:0:0","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"作者 Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung Google ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:1:0","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"摘要 GFS（Google File System）是由我们设计并实现的为大规模分布式数据密集型应用程序设计的可伸缩（scalable）的分布式文件系统。GFS为在廉价商用设备上运行提供了容错能力，并可以在有大量客户端的情况下提供较高的整体性能。 GFS的设计来自于我们对我们的应用负载与技术环境的观察。虽然GFS与过去的分布式文件系统有着共同的目标，但是根据我们的观察，我们的应用负载和技术环境与过去的分布式系统所做的假设有明显的不同。这让我们重新审视了传统的选择并去探索完全不同的设计。 GFS很好地满足了我们的存储需求。GFS在Google被广泛地作为存储平台部署，用于生成、处理我们服务所使用的数据或用于需要大规模数据集的研发工作。到目前为止，最大的GFS集群有上千台机器、上千块磁盘，并提供了上百TB的存储能力。 在本文中，我们介绍了为支持分布式应用程序而设计的文件系统接口的扩展，还从多方面讨论了我们的设计，并给出了小批量的benchmark与在现实场景中的使用表现。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:2:0","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"1. 引言 为了满足Google快速增长的数据处理需求，我们设计并实现了GFS。GFS与过去的分布式系统有着很多相同的目标，如性能（performance）、可伸缩性（scalability）、可靠性（reliability）和可用性（availability）。但是我们的设计来自于我们对我们的应用负载与技术环境的观察。这些观察反映了与过去的分布式系统所做的假设明显不同的结果。因此，我们重新审视的传统的选择并探索了完全不同的设计。 第一，我们认为设备故常经常发生。GFS由成百上千台由廉价设备组成的存储节点组成，并被与其数量相当的客户端访问。设备的数量和质量决定了几乎在任何时间都会有部分设备无法正常工作，甚至部分设备无法从当前故障中分恢复。我们遇到过的问题包括：应用程序bug、操作系统bug、人为错误和硬盘、内存、插头、网络、电源等设备故障。因此，系统必须具有持续监控、错误检测、容错与自动恢复的能力。 第二，文件比传统标准更大。数GB大小的文件是十分常见的。每个文件一般包含很多引用程序使用的对象，如Web文档等。因为我们的数据集由数十亿个总计数TB的对象组成，且这个数字还在快速增长，所以管理数十亿个几KG大小的文件是非常不明智的，即使操作系统支持这种操作。因此，我们需要重新考虑像I/O操作和chunk大小等设计和参数。 第三，大部分文件会以“追加”（append）的方式变更（mutate），而非“覆写”（overwrite）。在实际场景中，几乎不存在对文件的随机写入。文件一旦被写入，即为只读的，且通常仅被顺序读取。很多数据都有这样的特征。如数据分析程序扫描的大型数据集、流式程序持续生成的数据、归档数据、由一台机器生产并同时或稍后在另一台机器上处理的数据等。鉴于这种对大文件的访问模式，追加成了为了性能优化和原子性保证的重点关注目标，而客户端中对chunk数据的缓存则不再重要。 第四，同时设计应用程序和文件系统API便于提高整个系统的灵活性。例如，我们放宽了GFS的一致性协议，从而大幅简化了系统，减少了应用程序的负担。我们还引入了一种在不需要额外同步操作的条件下允许多个客户端并发将数据追加到同一个文件的原子性操作。我们将在后文中讨论更多的细节。 目前，我们部署了多个GFS集群并用于不同的目的。其中最大的集群有超过1000个存储节点、超过300TB的磁盘存储，并被数百台客户端连续不断地访问。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:3:0","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"2. 设计概述 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:4:0","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"2.1 假设 在设计能够满足我们需求的文件系统时，我们提出并遵循了一些挑战与机遇并存的假设。之前我们已经提到了一些，现在我们将更详细地阐述我们的假设。 系统有许多可能经常发生故障的廉价的商用设备组成。它必须具有持续监控自身并检测故障、容错、及时从设备故障中恢复的能力。 系统存储一定数量的大文件。我们的期望是能够存储几百万个大小为100MB左右或更大的文件。系统中经常有几GB的文件，且这些文件需要被高效管理。系统同样必须支持小文件，但是不需要对其进行优化。 系统负载主要来自两种读操作：大规模的流式读取和小规模的随机读取。在大规模的流式读取中，每次读取通常会读几百KB、1MB或更多。来自同一个客户端的连续的读操作通常会连续读文件的一个区域。小规模的随机读取通常会在文件的某个任意偏移位置读几KB。性能敏感的应用程序通常会将排序并批量进行小规模的随机读取，这样可以顺序遍历文件而不是来回遍历。 系统负载还来自很多对文件的大规模追加写入。一般来说，写入的规模与读取的规模相似。文件一旦被写入就几乎不会被再次修改。系统同样支持小规模随机写入，但并不需要高效执行。 系统必须良好地定义并实现多个客户端并发向同一个文件追加数据的语义。我们的文件通常在生产者-消费者队列中或多路归并中使用。来自不同机器的数百个生产者会并发地向同一个文件追加写入数据。因此，最小化原子性需要的同步开销是非常重要的。文件在被生产后可能同时或稍后被消费者读取。 持续的高吞吐比低延迟更重要。我们的大多数应用程序更重视告诉处理大量数据，而很少有应用程序对单个读写操作有严格的响应时间的需求。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:4:1","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"2.2 接口 尽管GFS没有实现像POSIX那样的标准API，但还是提供了大家较为熟悉的文件接口。文件被路径名唯一标识，并在目录中被分层组织。GFS支持如创建（create）、删除（delete）、打开（open）、关闭（close）、读（read）、写（write）文件等常用操作。 此外，GFS还支持快照（snapshot）和追加记录（record append）操作。快照操作会以最小代价创建一个文件或一个目录树的拷贝。追加记录操作允许多个客户端在保证每个独立的客户端追加操作原子性的同时能够并发地向同一个文件追加数据。这对实现如多路归并、生产者-消费者队列等多个客户端不需要额外的锁即可同时向同一文件追加数据非常有益。我们发现这类文件对于构建大型分布式应用程序有极高的价值。快照和追加记录的操作将分别在章节3.4和章节3.3讨论。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:4:2","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"2.3 架构 如图1所示，一个GFS集群包括单个master（主服务器）和多个chunkserver（块服务器），并被多个client（客户端）访问。每个节点通常为一个运行着用户级服务进程的Linux主机。如果资源允许且可以接受不稳定的应用程序代码所带来的低可靠性，那么可以轻松地在一台机器上同时运行chunkserver和client。 图1 GFS架构图图1 GFS架构图 \" 图1 GFS架构图 文件被划分为若干个固定大小的chunk（块）。每个chunk被一个不可变的全局唯一的64位chunk handle（块标识符）唯一标识，chunk handle在chunk被创建时由主节点分配。chunkserver将chunk作为Linux文件存储到本地磁盘中，通过chunk handle和byte range（字节范围）来确定需要被读写的chunk和chunk中的数据。为了可靠性考虑，每个chunk会在多个chunkserver中有副本。我们默认存储三份副本，用户也可以为不同的命名空间的域指定不同的副本级别。 master维护系统所有的元数据。元数据包括命名空间（namespace）、访问控制（access control）信息、文件到chunk的映射和chunk当前的位置。master还控制系统级活动如chunk租约（chunk lease）管理、孤儿chunk垃圾回收（garbage collection of orphaned chunks）和chunkserver间的chunk迁移（migration）。master周期性地通过心跳（HeartBeat）消息与每个chunkserver通信，向其下达指令并采集其状态信息。 被链接到应用程序中的GFS client的代码实现了文件系统API并与master和chunkserver通信，代表应用程序来读写数据。进行元数据操作时，client与master交互。而所有的数据（译注：这里指存储的数据，不包括元数据）交互直接由client与chunkserver间进行。因为GFS不提供POXIS API，因此不会陷入到Linux vnode层。 无论client还是chunkserver都不需要缓存文件数据。在client中，因为大部分应用程序需要流式地处理大文件或者数据集过大以至于无法缓存，所以缓存几乎无用武之地。不使用缓存就消除了缓存一致性问题，简化了client和整个系统。（当然，client需要缓存元数据。）chunkserver中的chunk被作为本地文件存储，Linux系统已经在内存中对经常访问的数据在缓冲区缓存，因此也不需要额外地缓存文件数据。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:4:3","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"2.4 单master 采用单master节点大大简化了我们的设计，且让master可以通过全局的信息做复杂的chunk分配（chunk placement）和副本相关的决策。然而，我们必须最小化master节点在读写中的参与，以避免其成为系统瓶颈。client不会直接从master读取文件数据，而是询问master它需要与哪个chunkserver通信。client会在一定时间内缓存信息，并直接与对应的chunkserver通信以完成后续操作。 让我们结合图1来解释一个简单地“读”操作。首先，通过固定的chunk大小，client将应用程序指定的文件名和chunk偏移量翻译为该文件中的chunk index（块序号）。然后，client想master发送一个包含了文件名和chunk index的请求。master会返回其相应的chunk handle和副本所在的位置。client将这个信息以文件名和chunk index为键进行缓存。 client接着向最有可能为最近的副本所在的chunkserver发送请求。请求中指定了chunk handle和byte range。之后，client再次读取相同的chunk时不再需要与master交互，直到缓存过期或文件被重新打开。事实上，client通常会在同一个请求中请求多个chunk，master也可以返回包含多个chunk的响应。这种方式避免了client与master进一步的通信，在几乎不需要额外开销的情况下得到更多的信息。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:4:4","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"2.5 chunk大小 chunk大小是关键的设计参数之一。我们选择了64MB，其远大于通常的文件系统的块大小。每个chunk的副本被作为普通的Linux文件存储在chunkserver上，其仅在需要时扩展。懒式空间分配（lazy space allocation）避免了内部碎片（internal fragmentation）带来的空间浪费，而内部碎片可能是选择较大的chunk大小所带来的最大的不利因素。 选择较大的chunk大小提供了很多重要的优势。第一，减少了client与master交互的次数，因为对一个chunk的读写仅需要与master通信一次以请求其位置信息。因为我们的应用程序通常连续地读写大文件，所以减少了client与master交互的次数是尤为重要的。即使对于小规模的随机读取的情况，client也可以轻松地缓存一个数TB的数据集所有的chunk位置信息。第二，因为chunk较大，client更有可能在一个chunk上执行更多的操作，这可以通过与chunkserver保持更长时间的TCP连接来减少网络开销。第三，减少了master中保存的元数据大小。我们可以将元数据保存在master的内存中，这样做提供了更多的优势，这些优势将在章节2.6.1中讨论。 然而，即使有懒式空间分配，较大的chunk大小也存在着缺点。管理仅有几个chunk的小文件就是其中之一。如果多个client访问同一个文件，那么存储这这些文件的chunkserver会成为hot spot（热点）。在实际情况相爱，因为应用程序大部分都顺序地读取包含很多chunk的大文件，所以hot spot不是主要问题。 然而在GFS首次被批处理队列（batch-queue）系统使用时，确实出现了hot spot问题：一个可执行文件被以单个chunk文件的形式写入了GFS，然后在数百台机器上启动。存储这个可执行程序的几台chunkserver因几百个并发的请求超载。我们通过提高这种可执行文件的副本数（replication factor）并让批处理队列系统错开应用程序启动时间的方式修复了这个问题。一个潜在的长期解决方案是在让client在这种场景下从其他client读取数据。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:4:5","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"2.6 元数据 master主要存储三种元数据：文件和chunk的命名空间（namespace）、文件到chunk的映射和chunk的每个副本的位置。所有元数据被存储在master的内存中。前两种类型（文件和快的命名空间、文件到chunk的映射）还通过将变更（mutation）记录到一个操作日志（operation log）的方式持久化存储在master的磁盘上，并在远程机器上备份。通过日志，我们可以简单、可靠地更新master的状态，即使master故障也没有数据不一致的风险。master不会持久化存储chunk的位置信息，而是在启动时和当chunkserver加入集群时向chunkserver询问其存储的chunk信息。 2.6.1 内存数据结构 因为元数据被存储在内存中，master可以快速地对其进行操作。此外，在内存中存储元数据可以使master周期性扫描整个的状态变得简单高效。这种周期性的扫描被用作实现垃圾回收、chunkserver故障时重做副本、chunkserver间为了负载均衡和磁盘空间平衡的chunk迁移。章节4.3和章节4.4会进一步讨论这些活动。 这种仅使用内存的方法的一个潜在问题是chunk的数量及整个系统的容量受master的内存大小限制。在实际情况中，这并不会成为一个严重的限制。master为每个64MB的chunk维护少于64字节的元数据。因为大多数文件包含多个chunk，所以大部分chunk是满的，仅最后一个chunk被部分填充。并且因为采用了前缀压缩的方式紧凑地存储文件名，每个文件的命名空间数据通常需要少于64字节。 即使当有必要支持更大型的文件系统时，增加额外的内存的成本，远远低于通过内存存储元数据所带来的简单性、可靠性、性能和灵活性。 2.6.2 chunk位置 master不会持久化保存哪台chunkserver含有给定的chunk的副本的记录，而是简单地在启动时从chunkserver获取信息。随后，master就可以保证自己的记录是最新的，因为master控制着所有chunk的分配并通过周期性的心跳消息监控chunkserver状态。 最初我们试图让master持久化保存chunk位置信息，但是后来我们意识到在chunkserver启动时和启动后周期性请求数据要简单的多。这样做消除了当chunkserver加入或离开集群、更改名称、故障、重启等问题时，保持master和chunkserver同步的问题。在有着数百台服务器的集群中，这些事件都会经常发生。 另一种理解这种设计的方法是，chunkserver对其磁盘上有或没有哪些chunk有着最终决定权。因为chunkserver中的错误会导致chunk消失（例如磁盘可能损坏或被禁用）或一个操作者可能重命名一个chunkserver。因此，试图在master上维护一个持久化的快位置信息视图是没有以意义的。 2.6.3 操作日志 操作日志包含重要的元数据变更的历史记录。这是GFS的核心。它不仅是元数据中唯一被持久化的记录，还充当了定义并发操作顺序的逻辑时间线。带有版本号的文件和chunk都在他们被创建时由逻辑时间唯一、永久地确定。 操作日志是GFS至关重要的部分，其必须被可靠存储，且在元数据的变更被持久化前不能让client对变更可见。否则当故障发生时，即使chunk本身没有故障，但是整个文件系统或者client最近的操作会损坏。我们将操作日志备份到多台远程主机上，且只有当当前操作记录条目被本地和远程主机均写入到了磁盘后才能向客户端发出响应。master会在操作记录被写入前批量合并一些操作记录来减少写入和备份操作对整个系统吞吐量的影响。 master通过重放（replay）操作日志来恢复其文件系统的状态。操作日志要尽可能小以减少启动时间。当日志超过一定大小时，master会对其状态创建一个检查点（checkpoint），这样master就可以从磁盘加载最后一个检查点并重放该检查点后的日志来恢复状态。检查点的结构为一个紧凑的B树（B-tree）这样它就可以在内存中被直接映射，且在查找命名空间时不需要进行额外的解析。这进一步提高了恢复速度，并增强了系统的可用性。 因为创建一个检查点需要一段时间，所以master被设计为可以在不推迟新到来的变更的情况下创建检查点。创建检查点时，master会切换到一个新的日志文件并在一个独立的线程中创建检查点。这个新的检查点包含了在切换前的所有变更。一个有着几百万个文件的集群可以再一分钟左右创建一个检查点。当检查点被创建完成后，它会被写入master本地和远程主机的磁盘中。 恢复仅需要最后一个完整的检查点和后续的日志文件。旧的检查点和日志文件可以随意删除，不过我们会不保留一段时间以容灾。创建检查点时发生错误不会影响日志的正确性，因为恢复代码会检测并跳过不完整的检查点。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:4:6","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"2.7 一致性模型 GFS宽松的一致性模型可以很好地支持我们的高度分布式应用程序，且实现起来简单高效。我们将讨论GFS提供的保证和其对应用程序的意义。我们也会重点讨论GFS如何维持这些保证，但会将细节留给本论文的其他部分。 2.7.1 GFS提供的保证 文件命名空间的变更（例如创建文件）操作时原子性的。它们仅由master处理。命名空间锁保证了原子性和正确性（章节4.1）；master的操作日志定义了这些操作的全局总顺序（章节2.6.3）。 在数据变更后，无论变更的成功与否，一个文件区域（file region）的状态都取决于变更类型。表1总结了变更后文件区域的状态。如果一个文件区域的任意一个副本被任何client读取总能得到相同的数据，那么这个文件区域状态为consistent（一致的）。在一个文件区域的数据变更后，如果它是一致的，且client总能看到其写入的内容（译注：并发写等操作后文件区域虽然consistent，但是client不一定能够读到其写入的数据，后文会描述这种情况。），那么这个文件区域的状态为defined（确定的）（defined状态包含了consistent状态）。文件区域在并发变更执行后的状态为consistent but undefined（一致的但非确定的）：所有客户端能考到同样的数据，但数据可能并不反映任何一个变更写入的数据。通常，数据融合了多个变更的内容。文件区域在一个失败的变更后状态会变为inconsistent（不一致的）（且undefined）：不同client在不同时刻可能看到不同的数据。下面我摩恩将描述我们的应用程序如何区分defined和undefined的区域。应用程序不需要进一步区分不同种的undefined状态。 表1 变更后文件区域状态 Table 1: File Region State After Mutation Write（写入） Record Append（记录追加） 串行成功 （Secrial success） defined （确定的） defined interspersed with inconsistent 确定的，但部分不一致 并发成功 （Secrial success） consistent but undefined （一致的但非确定的） 失败（Failure） inconsistent （不一致的） 数据变更操作可能为write或record append（译注：record append操作与文件的append有所不同，下文中会有对record append的介绍）。write操作会在应用程序指定的文件与偏移处写入数据。record append会将数据至少一次（at least once）地原子性地写入文件，即使在record append的同时可能存在并发的变更，但是record append写入位置是由GFS选择的偏移量（章节3.3）。（与常规的append不同，append仅会在client认为的文件末尾处写入数据。）record append的偏移量会被返回到client，这个偏移量为record append写入的数据的起始位置。除此之外，GFS可能会在记录的中间插入填充（padding）和或重复的记录。它们占用的区域状态为inconsistent的，通常情况下，它们的数量远少于用户数据。 在一系列变更执行成功后，被变更的文件区域状态为defined的，且该区域中包含最后一次变更写入的数据。这一点是GFS通过以下方式实现的：（a）对chunk执行变更时，其所有副本按照相同的顺序应用变更（章节3.1）（b）使用chunk版本号（chunk version）来检测因chunkserver宕机而错过了变更的陈旧的chunk副本（章节4.5）。陈旧的chunk副本永远不会在执行变更时被使用，也不会在master返回client请求的chunk的位置时被使用。它们会尽早地被作为垃圾回收。 由于client会缓存chunk的位置，在缓存信息刷新前，client可能会访问陈旧的副本。这个时间窗口会受缓存过期时间和下一次打开文件限制（下一次打开文件会清除文件的所有chunk位置信息）。除此之外，由于我们大多数文件是仅追加的，陈旧的副本的通常会返回一个版本较早的结束位置处的数据，而不是陈旧的数据（译注：这里陈旧的数据指错过了write变更的数据）。当reader重试并与master通信时，它将立刻获取目前的chunk位置。 即使在变更被成功应用的很长时间后，设备故障仍然可以损坏（corrupt）会销毁（destroy）数据。GFS通过master和所有chunkserver周期性握手的方式来确定故障的chunkserver，并通过校验和（checksunmming）的方式检测数据损坏（章节5.2）。一旦出现问题，数据会尽快地从一个合法的副本恢复章节4.3）。一个chunk只有在GFS作出反应前（通常在几分钟内）失去了所有的副本，chunk才会不可逆地丢失。即使在这种情况下，chunk也仅变得不可用而非损坏，因为应用程序可以收到明确的错误而非损坏的数据。（译注：本节中的“损坏corrupt”指读到错误的数据，“销毁（destory）”指数据丢失。） 2.7.2 对应用程序的影响 GFS应用程序可以通过一些简单的技术来使用其宽松的一致性模型，且这些技术已经因其他目标而被使用，如：依赖append而不是overwrite、检查点、自验证写入（writing self-validating）、自标识记录（self-identifying records）。 在实际使用中，我们所有的应用程序都通过append而不是overwrite的方式对文件进行变更。其中一个典型的引用场景是：一个write从头到尾地生成一个文件。它会周期性地为已经写入的文件数据创建检查点，并在所有数据都被写入文件后自动将其重命名为一个永久的文件名。检查点可能包含应用程序级别的校验和。reader会验证文件仅处理跟上最新的检查点的文件区域，这些区域的状态一定的“defined”的。尽管这种方法有一致性和并发问题，它仍很好地满足了我们的需求。append的效率远高于随机写入，且在应用程序故障时更容易恢复。检查点机制允许writer在重启时增量写入，并能够防止reader处理那些虽然已经被成功写入文件但是从应用程序的角度看仍然不完整的文件数据。 另一种典型的用途是，许多write并发地向同一个文件append数据以获得合并后的结果或文件作为生产者-消费者队列使用。record append的“至少一次追加（append-at-least-once）”语义保证了每个write的输出。而reader偶尔需要处理填充和重复的数据，如下文所述。每条被writer准备好的记录包含如校验和的额外信息，这样，记录的合法性就可被校验。一个reader通过校验和来识别并丢弃额外的填充和记录。如果rearder无法容忍偶尔发生的重复（如果重复的记录可能触发非幂等（non-idempotent）运算），它可以使用记录中的唯一标识符来对齐进行过滤。通常，在命名应用程序相关的实体时（如web文档），总会使用唯一的标识符。数据记录的I/O的充能都在库代码中（除了去重），可以被我们的应用程序使用，且其还适应于Google实现的其他文件接口。通过这些库，带有极少的重复的记录，总会被以相同顺序交付给reader。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:4:7","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"3. 系统交互 在我们设计系统时，我们让master尽可能少地参与所有操作。在此背景下，我们将描述client、master和chunkserver如何交互来实现数据变更、原子地record append和快照操作。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:5:0","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"3.1 租约和变更顺序 改变chunk或元数据的操作被称为“变更”，如write或append。chunk变更时，其每个副本都会应用变更。我们使用租约（lease）来维护副本间变更顺序的一致性。master向其中一份副本授权一个变更的租约，我们称这个副本为primary（译注：有时也可代指primary副本所在的chunkserver）。primary为应用于该chunk的所有变更选取顺序。所有副本都会按照这个顺序来应用变更。因此，全局的变更顺序首先由master选取的租约授权顺序定义，接着在租约内由primary选取的顺序编号定义。 这种租约机制是为了最小化master管理负载而设计的。租约的初始超时时间为60秒。然而，一旦chunk被变更，primary就可以向master请求延长租约时间，或者（通常为）接受来自master的租约时间延长操作。这些租约延长请求和租约授权请求依赖master与chunkserver间周期性地心跳消息来实现。有时master可能会在租约过期前视图撤销租约（例如，当master想禁止对正在被重命名的文件进行变更时）。即使master与一个primary的通信丢失，master仍可以在旧租约过期后安全地向另一个副本授权新的租约。 在图2中，我们将通过带编号的控制流来讲解一次write的流程。 图2 写操作的控制与数据流图2 写操作的控制与数据流 \" 图2 写操作的控制与数据流 client向master询问哪个chunkserver持有指定chunk的租约及该chunk的其他副本的位置。如果没有chunkserver持有租约，那么master会选择一个副本对其授权（这一步在图中没有展示）。 master回复primary副本的标识符和其他副本（也称secondary）的位置。client为后续的变更缓存这些信息。client只有当primary不可访问或primary向client回复其不再持有租约时才需要再次与master通信。 client将数据推送到所有副本。client可以按任意顺序推送。每个chunkserver都会将数据在内部的LRU中缓存，直到数据被使用或缓存老化失效（age out）。通过将数据流和控制流解耦，我们可以使用基于网络拓扑的技术来提高开销高昂的数据流的性能，且与哪台chunkserver是primary无关。章节3.2将对此进一步讨论。 一旦所有副本都确认收到了数据，client会向primary发送一个write请求。这个请求标识了之前推送到所有副本的数据的作用。primary会为其收到的所有的变更（可能来自多个client）分配连续的编号，这一步提供了重要的顺序。primary对在本地按照该顺序应用变更。 primary将write请求继续传递给其他secondary副本。每个secondary副本都按照primary分配的顺序来应用变更。 所有的secondary副本通知primary其完成了变更操作。 primary回复client。任意副本遇到的任何错误都会被报告给client。即使错误发生，write操作可能已经在primary或secondary的任意子集中被成功执行。（如果错误在primary中发生，那么操作将不会被分配顺序，也不会被继续下发到其他副本。）只要错误发生，该请求都会被认为是失败的，且被修改的区域的状态为inconsistent。client中的代码会通过重试失败的变更来处理这种错误。首先它会重试几次步骤（3）到步骤（7），如果还没有成功，再从write请求的初始操作开始重试。 如果应用程序发出的一次write请求过大或跨多个chunk，GFS的client代码会将其拆分成多个write操作。拆分后的write请求都按照上文中的控制流执行，但是可能存在与其他client的并发的请求交叉或被其他client的并发请求覆盖的情况。因此，共享的文件区域最终可能包含来自不同client的片段。但共享的文件区域中的内容最终是相同的，因为每个操作在所有副本上都会以相同的顺序被成功执行。正如章节2.7中所述，这会使文件区域变为consistent but undefined状态。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:5:1","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"3.2 数据流 为了高效地利用网络，我们对数据流与控制流进行了解耦。在控制流从client向primary再向所有secondary推送的同时，数据流沿着一条精心挑选的chunkserver链以流水线的方式线性推送。我们的目标是充分利用每台机器的网络带宽，避免网络瓶颈和高延迟的链路，并最小化推送完所有数据的时延。 为了充分利用机器的网络带宽，数据会沿着chunkserver链线性地推送，而不是通过其他拓扑结构（如树等）分配发送。因此，每台机器全部的出口带宽都被用来尽可能快地传输数据，而不是非给多个接受者。 为了尽可能地避免网络瓶颈和高延迟的数据链路（例如，交换机间链路（inter-switch）经常同时成为网络瓶颈和高延迟链路），每台机器会将数据传递给在网络拓扑中最近的的且还没有收到数据的机器。假设client正准备将数据推送给S1~S4。client会将数据发送给最近的chunkserver，比如S1。S1会将数据传递给S2~S4中离它最近的chunkserver，比如S2。同样，S2会将数据传递给S3~S4中离它最近的chunkserver，以此类推。由于我们的网络拓扑非常简单，所以可以通过IP地址来准确地估算出网络拓扑中的“距离”。 最后，我们通过流水线的方式通过TCP连接传输数据，以最小化时延。当chunkserver收到一部分数据时，它会立刻开始将数据传递给其他chunkserver。因为我们使用全双工的交换网络，所以流水线可以大幅减少时延。发送数据不会减少接受数据的速度。如果没有网络拥塞，理论上将$B$个字节传输给$R$个副本所需的时间为$B/T+RL$，其中$T$是网络的吞吐量，$L$是两台机器间的传输时延。通常，我们的网络连接吐吞量$T$为$100Mbps$，传输时延$L$远小于$1ms$。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:5:2","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"3.3 原子性record append GFS提供了一种叫做record append的原子性append操作。在传统的write操作中，client会指定数据写入的偏移量。对同一个文件区域的并发write操作不是串行的，可能会导致该区域中不同段的数据来自多个cllient。然而在record append中，client仅需指定待追加的数据。GFS会为其选择一个偏移量，在该偏移量处至少一次地原子性地将数据作为一个连续的字节序列追加到文件，并将该偏移量返回给client。这很像Unix系统中，在不存在多writer并发写入带来的竞态条件下，写入以O_APPEND模式打开的文件的情况。 record append被大量应用在我们的有多个来自不同机器的client向同一个文件并发append数据的分布式应用程序中。如果通过传统的write操作，那么client还需要额外的复杂且开销很高的同步操作（例如分布式锁管理）。这种文件在我们的工作环境下常被作为MPSC（multiple-producer/single-consumer，多生产者单消费者）队列使用，或是作为包含了来自多个client的数据合并后的结果被使用。 record append是变更的一种，也遵循章节3.1中的控制流，仅在primary端稍有点额外的逻辑。在client将数据推送到所有副本的最后一个chunk之后，client会向primary发送一个请求。primary会检查当新记录追加到该chunk之后，是否会导致该chunk超过最大的chunk大小限制（64MB）。如果会超出chunk大小限制，primary会将该chunk填充到最大的大小，并通知secondary也做相同的操作，再回复客户端，使其在下一个chunk上重试该操作。record append操作限制了每次最多写入最大chunk大小的四分之一的数据，以保证在最坏的情况下产生的碎片在可接受的范围内。（译注：过大的请求会被拆分成多个请求，如章节3.1中所述。）在一般情况下，记录大小都在最大限制以内，这样primary会向数据追加到它的副本中，并通知secondary在与其追加的偏移量相同的位置处写入数据，并将最终成功操作的结果返回给client。 如果record append操作在任何一个副本中失败，那么client会重试操作。这样会导致同一个chunk的不同副本中可能包含不同的数据，这些数据可能是同一条记录的部分或完整的副本。GFS不保证所有副本在字节级别一致，其只保证record append的数据作为一个单元被原子性地至少写入一次。这一点很容易证明，因为数据必须在某个chunk的所有副本的相同偏移位置处写入。此外，在record append之后，每个副本都至少与最后一条记录一样长。这样，任何未来的新记录都会被分配到一个更高的偏移位置或者一个新chunk，即使另一个副本成为了primary也能保证这个性质。这样，被record append操作成功写入的区域在一致性方面都将是defined状态（因此也是consistent的），而这些defined区域间的文件区域是inconsistent的（因此也是undefined的）。我们应用程序会通过章节2.7.2中讨论的方式处理inconsistent的区域。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:5:3","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"3.4 快照 快照操作几乎会在瞬间对一个文件或一个目录树（被称为源）完成拷贝，同时能够尽可能少地打断正在执行的变更。我们的用户使用快照操作来快速地对一个庞大的数据集的一个分支进行拷贝（或对其拷贝再进行拷贝等等），或者在实验前对当前状态创建检查点，这样就可以在试验后轻松地提交或回滚变更。 我们使用类似AFS[5]的标准的写入时复制技术来实现快照。当master收到快照请求的时候，它首先会撤销快照涉及到的文件的chunk上所有未完成的租约。这确保了对这些chunk在后续的写入时都需要与master交互以查找租约的持有者。这会给master优先拷贝这些chunk的机会。 在租约被收回或过期后，master会将快照操作记录到日志中，并写入到磁盘。随后，master会通过在内存中创建一个源文件或源目录树的元数据的副本的方式来进行快照操作。新创建的快照文件与源文件指向相同的chunk。 在快照操作后，首次想要对chunk$C$进行write操作的client会向master发送一个请求以找到当前的租约持有者。master会检测到chunk$C$的引用数超过1个。master会推迟对client的响应，并选取一个新的chunk handler $C'$。接着，master请求每个当前持有chunk$C$副本的chunkserver去创建一个新chunk$C'$。通过在与源chunk相同的chunkserver上创建新chunk，可以保证数据只在本地拷贝，而不会通过网络拷贝（我们的磁盘大概比$100Mb$的以太网连接快3倍左右）。在这之后，请求的处理逻辑就与处理任何其他chunk的请求一样了：master向新chunk$C'$的一个副本授权租约并将其响应client的请求。这样，client就可以像平常一样对chunk进行write操作，且client并不知道这个chunk是刚刚从一个已有的chunk创建来的。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:5:4","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"4. master操作 master执行所有命名空间操作。除此之外，master还管理整个系统中chunk的副本：master做chunk分配（placement）决策、创建新chunk与副本、协调各种系统范围的活动以保持chunk副本数饱和、平衡所有chunkserver的负载并回收未使用的存储。现在我们将讨论这些主题。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:6:0","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"4.1 命名空间管理与锁 master的很多操作可能消耗很长时间，例如：快照操作必须收回其涉及到的chunk所在的chunkserver的租约。当这些操作执行时，我们不希望推迟master的其他操作。因此，我们允许同时存在多个运行中的操作，并对命名空间的区域使用锁机制来保证操作正确地串行执行。 不像很多传统的文件系统，GFS没有用来记录目录中有哪些文件的数据结构。GFS也不支持对同一个文件或目录起别名（alias）（如Unix系统中的硬链接（hard link）或软链接（symbolic link））。GFS在逻辑上用一个完整路径名到元数据的查找表来表示命名空间。通过前缀压缩技术，这个查找表可在内存中高效地表示。在命名空间树上的每个节点（既可能是一个文件的绝对路径名，也可能是一个目录的绝对路径名）都有一个与之关联的读写锁（read-write lock）。 master的每个操作执行前都会请求一系列的锁。通常，如果master的操作包含命名空间$/d1/d2/…/dn/leaf$，master会在目录$/d1$、$/d1/d2$，…，$/d1/d2/…/dn$上请求读取锁，并在完整路径名$/d1/d2/…/dn/leaf$上请求读取锁或写入锁。其中，$leaf$可能是文件或者目录，这取决于执行的操作。 现在，我们将说明锁机制如何在$/home/user$正在被快照到$/save/user$时，防止$/home/user/foo$被创建。快照操作会在$/home$和$/save$上请求读取锁、在$/home/user$和$/save/user$上请求写入锁。文件创建操作需要在$/home$进和$/home/user$上请求读取锁，在$/home/user/foo$上请求写入锁。由于它们试图在$/home/user$上获取锁时发生冲突，因此这两个操作可以正确地串行执行。因为GFS中没有目录数据结果或像inode一样的数据结构，所以无需在修改时对其进行保护，因此在文件创建操作时不需要获取其父目录的写入锁。其父目录上的读取锁已经足够保护其父目录不会被删除。 这种锁机制提供了一个非常好的性质：允许在同一目录下并发地执行变更。例如，在同一目录下的多个文件创建操作可以并发执行：每个文件创建操作都获取其父目录的读取锁与被创建的文件的写入锁。目录名上的读取锁足够防止其被删除、重命名或快照。文件名上的写入锁可以防止相同同名文件被创建两次。 因为命名空间可能含有很多的结点，所以读写锁对象会在使用时被懒式创建，并一旦其不再被使用就会被删除。此外，为了防止死锁，锁的获取顺序总是一致的：首先按照命名空间树中的层级排序，在同一层级内按照字典顺序排序。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:6:1","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"4.2 副本分配 GFS集群在多个层级上都高度分布。GFS通常有数百个跨多个机架的chunkserver。这些chunkserver可能会被来自相同或不同机架上的数百个clienet访问。在不同机架上的两台机器的通信可能会跨一个或多个交换机。另外，一个机架的出入带宽可能小于这个机架上所有机器的出入带宽之和。多层级的分布为数据的可伸缩性、可靠性和可用性带来了特有的挑战。 chunk副本分配策略有两个目标：最大化数据可靠性和可用性、最大化网络带宽的利用。对于这两个目标，仅将副本分散在所有机器上是不够的，这样做只保证了容忍磁盘或机器故障且只充分利用了每台机器的网络带宽。我们必须在机架间分散chunk的副本。这样可以保证在一整个机架都被损坏或离线时（例如，由交换机、电源电路等共享资源问题引起的故障），chunk的一些副本仍存在并保持可用状态。除此之外，这样还使对chunk的流量（特别是读流量）能够充分利用多个机架的总带宽。而另一方面，写流量必须流经多个机架，这是我们资源做出的权衡。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:6:2","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"4.3 chunk创建、重做副本、重均衡 chunk副本的创建可能由三个原因引起：chunk创建、重做副本（re-replication）和重均衡（rebalance）。 当master创建一个chunk的时候，它会选择初始化空副本的位置。位置的选择会参考很多因素：（1）我们希望在磁盘利用率低于平均值的chunkserver上放置副本。随着时间推移，这样将平衡chunkserver间的磁盘利用率（2）我们希望限制每台chunkserver上最近创建的chunk的数量。尽管创建chunk本身开销很小，但是由于chunk时写入时创建的，且在我们的一次追加多次读取（append-once-read-many）的负载下chunk在写入完成后经常是只读的，所以master还要会可靠的预测即将到来的大量的写入流量。（3）对于以上讨论的因素，我们希望将chunk的副本跨机架分散。 当chunk可用的副本数少于用户设定的目标值时，master会重做副本副本。chunk副本数减少可能有很多种原因，比如：chunkserver可能变得不可用、chunkserver报告其副本被损坏、chunkserver的磁盘因为错误变得不可用、或者目标副本数增加。每个需要重做副本的chunk会参考一些因素按照优先级排序。这些因素之一是当前chunk副本数与目标副本数之差。例如，我们给失去两个副本的chunk比仅失去一个副本的chunk更高的优先级。另外，我们更倾向于优先为还存在的文件的chunk重做副本，而不是优先为最近被删除的文件（见章节4.4）重做。最后，为了最小化故障对正在运行的应用程序的影响，我们提高了所有正在阻塞client进程的chunk的优先级。 master选取优先级最高的chunk，并通过命令若干chunkserver直接从一个存在且合法的副本拷贝的方式来克隆这个chunk。新副本位置的选取与创建新chunk时位置选取的目标类似：均衡磁盘空间利用率、限制在单个chunkserver上活动的克隆操作数、在机架间分散副本。为了防止克隆操作的流量远高于client流量的情况发生，master需要对整个集群中活动的克隆操作数和每个chunkserver上活动的克隆操作数进行限制。除此之外，在克隆操作中，每个chunkserver还会限制对源chunkserver的读请求，以限制每个克隆操作占用的总带宽。 最后，每隔一段时间master会对副本进行重均衡：master会检测当前的副本分布并移动副本位置，使磁盘空间和负载更加均衡。同样，在这个过程中，master会逐渐填充一个新的chunkserver，而不会立刻让来自新chunk的高负荷的写入流量压垮新的chunkserver。新副本放置位置的选择方法与我们上文中讨论过的类似。此外，master必须删除一个已有副本。通常，master会选择删除空闲磁盘空间低于平均的chunkserver上的副本，以均衡磁盘空间的使用。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:6:3","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"4.4 垃圾回收 在文件被删除后，GFS不会立刻回收可用的物理存储空间。master仅在周期性执行懒式垃圾回收时回收物理存储空间，其中垃圾回收分为文件级垃圾回收和chunk级垃圾回收。我们发现这种方法可以让系统更为简单可靠。 4.4.1 垃圾回收机制 当一个文件被应用程序删除时，master会像执行其他操作时一样立刻将删除操作写入日志。但是master不会立刻对资源进行回收，而是将待删除的文件重命名为一个带有删除时间戳的隐藏文件名。当master周期性地扫描文件系统命名空间时，它会删除已经存在超过三天（用户可以配置这个间隔时间）的这种隐藏文件。在文件被彻底删除之前，仍可通过该文件被重命名后的特殊的新文件名对其进行访问，也可以通过将其重命名为正常文件的方式撤销删除。当隐藏文件被从命名空间中移除时，其在内存中的元数据也会被删除。这种方式可以有效地切断文件和其对应的chunk的链接。 和上文介绍的文件级垃圾回收类似，在进行chunk级垃圾回收时，master会周期性扫描chunk命名空间，并找出孤儿chunk（orphaned chunk）（例如哪些无法被任何文件访问到的chunk）并删除这些chunk的元数据。在chunkserver周期性地与master进行心跳消息交换时，chunkserver会报告其拥有的chunk的子集，而master会回复这些chunk中元数据已经不存在的chunk的标识。chunkserver可以自由地删除这些元数据已经不存在的chunk的副本。 4.4.2 关于垃圾回收的讨论 分布式系统垃圾回收通常是一个很困难的问题，其往往需要在编程时使用复杂的解决方案。但是在我们的场景下它非常简单。因为文件到chunk的映射由master专门管理，所以我们可以轻松地识别所有chunk的引用。同样，因为chunk的副本在每个chunkserver上都是Linux系统中指定目录下的文件，所以我们也可以轻松地识别所有chunk的副本。所有master中没有记录的副本都会被视为垃圾。 这种暂存待回收文件的垃圾回收方法相比饿汉式回收有很多优势。第一，这种方法在设备经常出现故障的大规模可伸缩分布式系统中非常简单可靠。chunk的创建可能仅在部分chunkserver上成功而在其他chunkserver上失败，这样会导致系统中出现master不知道的副本。且副本删除消息可能会丢失，这样master在其自身和chunkserver故障时都必须重新发送该消息。垃圾回收机制为清理那些不知道是否有用的副本提供了一个统一且可靠的方法。第二，垃圾回收机制将对存储空间的回收操作合并为master的后台活动，如周期性扫描命名空间和周期性地与chunkserver握手。因此，垃圾回收机制可以分批回收存储空间并平摊回收的开销。另外，垃圾回收仅在master相对空闲时执行。这样，master可以更迅速的相应需要及时响应的来自client的请求。第三，延迟回收存储空间可以防止意外的不可逆删除操作。 根据我们的实际经验，延迟回收的主要缺点是：当用户存储空间紧张时，延迟回收会让用户难以释放存储空间。快速创建并删除临时文件的应用程序可能无法立刻重用存储空间。为了解决这个问题，我们在用户再次显示删除已删除文件时，加快了对存储空间的回收。同时，我们允许用户对不同的命名空间应用不同的副本与回收策略。例如，用户可以指定某个目录树下的所有文件都不需要副本，且当这个目录树下的文件被删除时立刻且无法撤销地将其从文件系统中移除。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:6:4","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"4.5 陈旧副本检测 如果chunkserver因故障离线时错过了对其中的chunk的变更，那么该chunkserver中chunk的副本会变为陈旧的副本。master会为每一个chunk维护一个chunk版本号（chunk version number），用来区分最新的和陈旧的副本。 master每当为一个chunk授权新租约时，都会增加chunk的版本号并同时其最新的副本。master和这些副本都持久化保存这个新版本号。这一步发生在master响应任何client前，即在chunk可以被写入前。如果一个副本当前不可用，那么这个副本的chunk版本号不会增长。这样，当这个chunkserver重启时并向master报告其包含的chunk和chunk对应的版本号时，master会检测出这个chunkserver中的副本是陈旧的。如果master收到了比它的记录中更高的chunk版本号，master会认为其授权租约失败，并将更高的版本号视为最新的版本号。 master在周期性垃圾回收时会删除陈旧的副本。即使在master回收陈旧副本之前，当client向master请求该副本的chunk时，master仍会认为该陈旧的副本不存在。另一种保护措施是，当master通知client哪个chunkserver持有指定chunk的租约时，和当master在克隆操作中命令一个chunkserver从另一个chunkserver读取chunk时，其请求中需要带有chunk的版本号。client或者chunkserver会在执行操作时验证版本号以确保其始终在操作最新的数据。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:6:5","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"5. 错误容忍与诊断 在我们设计GFS时，最大的挑战之一就是处理经常发生的设备故障。设备的质量和数量让故障发生不再是异常事件，而是经常发生的事。我们既无法完全信任机器，也无法完全新人磁盘。设备故障可能导致系统不可用，甚至会导致数据损坏。我们将讨论我们是如何应对这些挑战的，以及系统内建的用来诊断系统中发生的不可避免的问题的工具。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:7:0","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"5.1 高可用 在由数百台服务器组成的GFS集群中，在任意时间总会有一些服务器不可用。我们通过两个简单但有效的策略保证整个系统高可用：快速恢复和副本。 5.1.1 快速恢复 在master和chunkserver的设计中，它们都会保存各自的状态，且无论它们以哪种方式终止运行，都可以在数秒内重新启动。事实上，我们并不区分正常终止和非正常的终止。通常，服务会直接被通过杀死进程的方式终止。当client和其他服务器的请求超时时，它们会在发生一个时间很短的故障，并随后重新连接到重启后的服务器并重试该请求。章节6.2.2中有启动时间相关的报告。 5.1.2 chunk副本 正如之前讨论的，每个chunk会在不同机架的多个chunkserver上存有副本。用户可以为不同命名空间的文件制定不同的副本级别。副本级别默认为3。当有chunkserver脱机或通过哦校验和（见章节5.2）检测到损坏的副本时，master根据需求克隆现有的副本以保证每个chunk的副本数都是饱和的。尽管副本策略可以很好地满足我们的需求，我们还是探索了其他形式的跨服务器的冗余策略以满足我们日益増长的只读数据存储需求，如：奇偶校验码（parity code）或擦除码（erasure code）。因为我们的流量主要来自append和读操作，而不是小规模的随机写操作，所以我们希望在松散耦合的系统中，既有挑战性又要可管理地去实现这些复杂的冗余策略。 5.1.3 master副本 为了保证可靠性，master的状态同样有副本。master的操作日志和检查点被在多台机器上复制。只有当变更在被日志记录并被写入，master本地和所有master副本的磁盘中后，这个变更才被认为是已提交的。为了简单期间，一个master进程既要负责处理所有变更又要负责处理后台活动，如垃圾回收等从内部改变系统的活动。当master故障时，其几乎可以立刻重启。如果运行master进程的机器故障或其磁盘故障，在GFS之外的负责监控的基础架构会在其它持有master的操作日志副本的机器上启动一个新的master进程。client仅通过一个规范的命名来访问master结点（例如gfs-test），这个规范的命名是一个DNS别名，其可以在master重新被分配到另一台机器时被修改为目标机器。 此外，“影子”master节点（“shadow” master）可以提供只读的文件系统访问，即使在主master结点脱机时它们也可以提供服务。因为这些服务器可能稍稍滞后于主master服务器（通常滞后几分之一秒），所以这些服务器是影子服务器而非镜像服务器。这些影子master服务器增强了那些非正在被变更的文件和不介意读到稍旧数据的应用程序的可用性。实际上，由于文件内容是从chunkserver上读取的，所以应用程序不会读取到陈旧的文件内容。能够在一个很短的时间窗口内被读取到的陈旧的数据只有文件元数据，如目录内容和访问控制信息。 为了让自己的元数据跟随主master变化，影子master服务器会持续读取不断增长的操作日志副本，并像主master一样按照相同的顺序对其数据结构应用变更。像主master一样，影子master服务器也会在启动时从chunkserver拉取数据来获取chunk副本的位置（启动后便很少拉取数据），并频繁地与chunkserver交换握手信息来监控它们的状态。只有因主master决定创建或删除副本时，影子master服务器上的副本位置才取决于主master服务器。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:7:1","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"5.2 数据完整性 每个chunkserver都使用校验和来检测存储的数据是否损坏。由于GFS集群通常在数百台机器上有数千chunk磁盘，所以集群中经常会出现磁盘故障，从而导致数据损坏或丢失（第七章中介绍了一个诱因）。我们可以通过chunk的其他副本来修复损坏的chunk，但不能通过比较chunkserver间的副本来检测chunk是否损坏。除此之外，即使内容不同的副本中的数据也可能都是合法的：GFS中变更的语义（特别是前文中讨论过的record append）不会保证副本完全相同。因此，每个chunkserver必须能够通过维护校验和的方式独立的验证副本中数据的完整性。 一个chunk被划分为64KB的block。每个block有其对应的32位校验和。就像其他元数据一样，校验和也在内存中保存且会被通过日志的方式持久化存储。校验和与用户数据是分开存储的。 对于读取操作，无论请求来自client还是其他chunkserver，chunkserver都会在返回任何数据前校验所有包含待读取数据的block的校验和。因此，chunkserver不会将损坏的数据传给其他机器。如果一个block中数据和记录中低的校验和不匹配，那么chunkserver会给请求者返回一个错误，并向master报告校验和不匹配。随后，请求者会从其他副本读取数据，而master会从该chunk的其他副本克隆这个chunk。当该chunk新的合法的副本被安置后，master会通知报告了校验和不匹配的chunkserver删除那份损坏的副本。 校验和对读取性能的影响很小。因为我们的大部分读操作至少会读跨几个block的内容，我们只需要读取并校验相对少量的额外数据。GFS客户端代码通过尝试将读取的数据与需要校验的block边界对其的方式，进一步地减小了校验开销。除此之外，chunkserver上校验和的查找与比较不需要I/O操作，且校验和计算操作经常与其他操作在I/O上重叠，因此几乎不存在额外的I/O开销。 因为向chunk末尾append数据的操作在我们的工作负载中占主要地位，所以我们对这种写入场景的校验和计算做了大量优化。在append操作时，我们仅增量更新上一个block剩余部分的校验和，并为append的新block计算新校验和。即使最后一个block已经损坏且目前没被检测到，增量更新后的该block的新校验和也不会与block中存储的数据匹配。在下一次读取该block时，GFS会像往常一样检测到数据损坏。 相反，如果write操作覆盖了一个chunk已存在的范围，那么我们必须读取并验证这个范围的头一个和最后一个block，再执行write操作，最后计算并记录新的校验和。如果我们没有在写入前校验头一个和最后一个block，新的校验和可能会掩盖这两个block中没被覆写的区域中存在的数据损坏问题。 chunkserver可以在空闲期间扫描并验证非活动的chunk的内容。这样可以让我们检测到很少被读取的chunk中的数据损坏。一旦检测到数据损坏，master可以创建一个新的未损坏的副本并删除损坏的副本。这样可以防止master将chunk的非活动的但是已损坏的副本识别成数据合法的副本。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:7:2","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"5.3 诊断工具 全面且详细的诊断日志以极小的开销为问题定位、调试和性能分析提供了很大的帮助。如果没有日志，理解机器间短暂且不重复的交互将变得非常困难。GFS服务器会生成用来记录重要事件（如chunkserver上线或离线）和所有RPC请求与响应的诊断日志。这些诊断日志可以随意删除，不会影响到系统正确性。不过，如果磁盘空间允许，我们将尽可能地保持这些日志。 RPC日志包括通过网络收发的请求和响应中除读写的文件数据之外的详细内容。在诊断问题时，我们可以通过整合不同机器中的日志并将请求与响应匹配的方式，重建整个交互历史。同样，这些日志也可用来跟踪压力测试、性能分析等情况。 因为日志是顺序且异步写入的，因此日志对性能的影响非常小，并带来了很大的好处。其中最近的事件也会在内存中保存，以便在持续的在线监控中使用。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:7:3","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"6. 性能测试 在本章中，我们将展示一些小批量的benchmark，以说明在GFS架构和实现中的瓶颈。我们还将展示一些Google在真是集群中使用时的一些指标。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:8:0","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"6.1 小批量benchmark 我们在一个由1个master、2个master副本、16个chunkserver和16个client组成的GFS集群中测量性能表现。该配置的选择仅为了便于测试。通常一个GFS集群会由数百个chunkserver和数百个client组成。 所有的机器都采用双核1.4GHz的奔腾III处理器、2GB内存、两块5400转的80GB磁盘和100Mbpc全双工以太网，并连接到一台HP2524交换机。其中所有的19台GFS服务器都连接到同一台交换机，所有的16台client机器都连接到另一台交换机。这两个交换机之间通过1Gbps连接。 图3 总吞吐量（上面的曲线表示在网络拓扑中的理论极限。下面的曲线表示测量到的吞吐量。测量结果曲线显示了95%置信区间的误差柱，在一些情况下，由于测量值的方差很低，置信区间在图中难以辨认。图3 总吞吐量（上面的曲线表示在网络拓扑中的理论极限。下面的曲线表示测量到的吞吐量。测量结果曲线显示了95%置信区间的误差柱，在一些情况下，由于测量值的方差很低，置信区间在图中难以辨认。） \" 图3 总吞吐量（上面的曲线表示在网络拓扑中的理论极限。下面的曲线表示测量到的吞吐量。测量结果曲线显示了95%置信区间的误差柱，在一些情况下，由于测量值的方差很低，置信区间在图中难以辨认。 6.1.1 read操作 $N$个client同时从GFS读取数据。每个client从320GB的数据集中随机选取4MB的区域读取。读操作将重复256次，即每个client最终将读取1GB的数据。chunkserver总计有32GB内存，因此我们预测读操作中最多10%命中Linux缓冲区缓存。我们的测试结果应该接近冷缓存的结果。 **图3(a)**展示了$N$个client的总读取速率和理论速率上限。整体的理论速率在$125MB/s$时达到峰值，此时两个交换机间的$1Gbps$的链路达到饱和；或者每个client的理论速率在$12.5MB/s$时达到峰值，此时它的$100Mbps$的网络接口达到饱和。当仅有一台client在读取时，我们观测到其读取速率为$10MB/s$，在每台client理论上限的80%。当16个client一起读取时，总读取速率达到了$94MB/s$，大致达到了理论上限$125MB/s$的75%，平均每个client的读取速率为$6MB/s$。因为reader的数量增加导致多个reader从同一个chunkserver读取的概率增加，所以读取速率从理论值的80%下降到了75%。 6.1.2 write操作 $N$个client同时向$N$个不同的文件写入。每个client通过一系列的$1MB$的写操作向一个新文件写入总计$1GB$数据。**图3(b)**展示了整体的写入速率和理论速率上限。因为我们需要将每个字节写入16个chunkserver中的三个，每个chunkserver的连接输入速率为$12.5MB/s$，所以整体的理论写入速率上限为$67MB/s$。 译注：$67MB/s$的理论写入速率上限的计算方式为如下。因为集群中总计有16个chunkserver，每个chunkserver的$100Mbps$全双工连接为输入速率为$12.5MB/s$。数据有3份副本。因此，当所有chunkserver的连接输入全部饱和时，写入的速率为$12.5MB/s \\times 16 \\div 3 = 67MB/s$。根据在章节3.2中对数据流的介绍可知，在数据写入时，client仅与chunkserver中的primary副本间有一次完整的数据传输，其他secondary副本数据均通过chunkserver递交。因此在本实验的集群中，每个chunkserver的连接输入饱和时，两个交换机建的数据传输速率为$67MB/s$，即数据写入的速率。小于交换机间的最大传输速率$1Gbps$，因此不会因交换机间的连接产生瓶颈。 实验观测到的每个client的写入速率为$6.3MB/s$，大概是理论上限的一半。网络栈是造成这一现象的罪魁祸首。在我们使用流水线的方式将数据推送到chunk副本时，网络栈的表现不是很好。数据从一个副本传递给另一个副本的时延降低了整体的写入速率。 16个client的整体写入速率达到了$35MB/s$，大概是理论上限的一半。与读取相同，当client的数量增加时，更有可能出现多个client并发地向同一个chunkserver写入的情况。此外，因为write操作需要向3份不同的副本写入，所以16个writer比16个reader更有可能出现碰撞的情况。write操作比我们预想的要慢。但是在实际环境中，这并不是主要问题。即使它增加了单个client的时延，但是在有大量client的情况下它并没有显著影响系统的整体写入带宽。 6.1.3 record append操作 **图3(c)**展示了record append操作的性能表现。$N$个client同时向同一个文件append数据。其性能受存储该文件最后一个chunk的chunkserver的网络带宽限制，与client的数量无关。当仅有1个client时，record append的速率为$6.0MB/s$，当client的数量增加到16个时，速率下降到$4.8MB/s$。网络拥塞和不同client的网络传输速率不同是造成record append速率下降的主要原因。 在实际环境中，我们的应用程序往往会并发地向多个这样的文件追加数据。换句话说，即$N$个client同时地向$M$个共享的文件append数据，其中$N$与$M$均为数十或数百。因此，实验中出现的chunkserver的网络拥塞在实际环境中并不是大问题，因个client可以在chunkserver忙着处理一个文件时向另一个文件写入数据。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:8:1","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"6.2 现实中的集群 现在我们来考察在Google中使用的两个集群，它们代表了其他类似的集群。集群A是数百个工程师常用来研究或开发的集群。其中典型的任务由人启动并运行几个小时。这些任务会读几MB到几TB的数据，对其分析处理，并将结果写回到集群中。集群B主要用于生产数据的处理。其中的任务持续时间更长，会不断地生成数TB的数据集，且偶尔才会有人工干预。在这两种情况中，每个任务都有许多过程进程组成，这些进程包括许多机器对许多文件同时的读写操作。 6.2.1 存储 正如表2所示，两个集群都有数百个chunkserver，有数TB的磁盘存储空间，且大部分存储空间都被使用，但还没满。其中“已使用空间”包括所有chunk的副本占用的空间。几乎所有文件都以3份副本存储。因此，集群分别存储了$18TB$和$52TB$的数据。 这两个集群中的文件数相似，但集群B中停用文件（dead file）比例更大。停用文件即为被删除或被新副本替换后还未被回收其存储空间的文件。同样，集群B中chunk数量更多，因为其中文件一般更大。 表2 两个GFS集群的特征 集群 A B Chunkserver数量 342 227 可用磁盘空间 已使用空间 72 TB 55 TB 180 TB 155 TB 文件数 停用文件数 chunk数 735 k 22 k 992 k 737 k 232 k 1550 k chunkserver元数据大小 master元数据大小 13 GB 48 MB 21 GB 60 MB 6.2.2 元数据 在chunkserver中，总共存储了数十GB的元数据，其中大部分是用户数据的每64KB大小的block的校验和。除此之外，chunkserver中的保存元数据只有章节4.5中讨论的chunk版本号。大部分的文件元数据是文件名，我们对其采用前缀压缩的形式存储。其他的文件元数据包括文件所有权和权限、文件到chunk的映射、每个chunk当前的版本号。除此之外，我们还存储了chunk当前的副本位置和chunk的引用计数（以实现写入时拷贝等）。 无论是chunkserver还是master，每个服务器中仅有50MB到100MB元数据。因此，服务器恢复的速度很快。服务器只需要几秒钟的时间从磁盘读取元数据，随后就能应答查询请求。然而，master的恢复稍微有些慢，其通常需要30到60秒才能恢复，因为master需要从所有的chunkserver获取chunk的位置信息。 6.2.3 读写速率 表3展示了不同时间段的读写速率。两个集群在测量开始后均运行了大概一周的时间。（集群最近已因升级到新版本的GFS而重启过。） 从重启后，集群的平均写入速率小于$30MB/s$。当我们测量时，集群B正在执行以大概$100MB/s$写入生成的数据的活动，因为需要将数据传递给三份副本，该活动造成了$300MB/s$的网络负载。 读操作的速率比写操作的速率要高得多。正如我们假设的那样，整体负载主要有读操作组成而非写操作。在测量时两个集群都在执行高负荷的读操作。实际上，集群A已经维持$580MB/s$的读操作一周了。集群A的网络配置能够支持$750MB/s$的读操作，所以集群A在高效利用其资源。集群B能够支持峰值在$1300MB/s$的读操作，但集群B的应用程序仅使用了$380MB/s$。 表3 两个GFS集群的性能指标 集群 A B 读速率（过去一分钟） 读速率（过去一小时） 读速率（重启后至今） 583 MB/s 562 MB/s 589 MB/s 380 MB/s 384 MB/s 49 MB/s 写速率（过去一分钟） 写速率（过去一小时） 写速率（重启后至今） 1 MB/s 2 MB/s 25 MB/s 101 MB/s 117 MB/s 13 MB/s master操作数（过去一分钟） master操作数（过去一小时） master操作数（重启后至今） 325 Ops/s 381 Ops/s 202 Ops/s 533 Ops/s 518 Ops/s 347 Ops/s 6.2.4 master的负载 表3中还展示了向master发送操作指令的速率，该速率大概在美妙200到500次左右。master可以在该速率下轻松地工作，因此这不会成为负载的瓶颈。 GFS在早期版本中，在某些负载场景下，master偶尔会成为瓶颈。当时master会消耗大量的时间来扫描包含成百上千个文件的目录以寻找指定文件。在那之后，我们修改了master中的数据结构，允许其通过二分查找的方式高效地搜索命名空间。目前，master已经可以轻松地支持每秒上千次的文件访问。如果有必要，我们还可以通过在命名空间数据结构前放置名称缓存的方式进一步加快速度。 6.2.5 恢复时间 当chunkserver故障后，一些chunk的副本数会变得不饱和，系统必须克隆这些块的副本以使副本数重新饱和。恢复所有chunk需要的时间取决于资源的数量。在一次实验中，我们杀掉集群B中的一个chunkserver。该chunkserver上有大概15000个chunk，总计约600GB的数据。为了限制重分配副本对正在运行的应用程序的影响并提供更灵活的调度策略，我们的默认参数限制了集群中只能有91个并发的克隆操作（该值为集群中chunkserver数量的40%）。其中，每个克隆操作的速率上限为$6.25MB/s$（$50Mbps$）。所有的chunk在23.2分钟内完成恢复，有效地复制速率为$440MB/s$。 在另一个实验中，我们杀掉了两台均包含16000个chunk和660GB数据的chunkserver。这两个chunkserver的故障导致了266个chunk仅剩一分副本。这266个块在克隆时有着更高的优先级，在2分钟内即恢复到至少两份副本的状态，此时可以保证集群中即使再有一台chunkserver故障也不会发生数据丢失。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:8:2","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"6.3 负载分解 在本节中，我们将详细介绍两个GFS集群中的工作负载。这两个集群与章节6.2中的类似但并不完全相同。集群X用来研究和开发，集群Y用来处理生产数据。 6.3.1 方法和注意事项 这些实验结果仅包含来自client的请求，因此结果反映了我们的应用程序为整个文件系统带来的负载情况。结果中不包括用来处理client请求的内部请求和内部的后台活动，如chunkserver间传递write数据和副本重分配等。 I/O操作的统计数据来源于GFS通过RPC请求日志启发式重建得到的信息。例如，GFS的client代码可能将一个read操作分解为多个RPC请求以提高并行性，通过日志启发式重建后，我们可以推断出原read操作。因为我们的访问模式是高度一致化的，所以我们期望的错误都在数据噪声中。应用程序中显式的日志可能会提供更加准确的数据，但是重新编译并重启上千个正在运行的client是现实的，且从上千台机器上采集数据结果也非常困难。 需要注意的一点是，不要过度地推广我们的负载情况。因为Google对GFS和它的应用程序具有绝对的控制权，所以应用程序会面向GFS优化，而GFS也正是为这些应用程序设计的。虽然这种应用程序与文件系统间的互相影响在一般情况下也存在，但是这种影响在我们的例子中可能会更明显。 6.3.2 chunkserver的负载 表4展示了各种大小的操作占比。读操作的大小呈双峰分布。64KB以下的小规模read来自client从大文件查找小片数据的seek密集操作。超过512KB的大规模read来自读取整个文件的线性读取。 在集群Y中，大量的read没有返回任何数据。在我们的应用程序中（特别是生产系统中的应用程序），经常将文件作为生产者-消费者队列使用。在多个生产者并发地向同一个文件支架数据的同时，会有一个消费者读末尾的数据。偶尔当消费者超过生产者时，read即不会返回数据。集群X中这种情况出现的较少，因为在集群X中的应用程序通常为短期运行的数据分析程序，而非长期运行的分布式应用程序。 write也呈同样的双峰分布。超过256KB的大规模write操作通常是由writer中的大量的缓冲区造成的。小于64KB的小规模写操作通常来自于那些缓冲区小、创建检查点操作或者同步操作更频繁、或者是仅生成少量数据的writer。 对于record append操作，集群Y中大规模的record append操作比集群X中要高很多。因为我们的生产系统使用了集群Y，生产系统的应用程序会更激进地面向GFS优化。 表4 各种大小的操作占比（%） 对于read操作，数据大小为实际读取和传输的数据大小，而非请求读取的总大小。 操作类型 read write record append 集群 X Y X Y X Y 0K 0.4 2.6 0 0 0 0 1B...1K 0.1 4.1 6.6 4.9 0.2 9.2 1K...8K 65.2 38.5 0.4 1.0 18.9 15.2 8K...64K 29.9 45.1 17.8 43.0 78.0 2.8 64K...128K 0.1 0.7 2.3 1.9 \u003c .1 4.3 128K...256K 0.2 0.3 31.6 0.4 \u003c .1 10.6 256K...512K 0.1 0.1 4.2 7.7 \u003c .1 31.2 512K...1M 3.9 6.9 35.5 28.7 2.2 25.5 1M...inf 0.1 1.8 1.5 12.3 0.7 2.2 表5中展示了不同大小的操作中传输数据的总量的占比。对于所有类型的操作，超过256KB的大规模操作通常都是字节传输导致的。小于64KB的小规模read操作通常来自seek操作，这些读操作传输了很小但很重要的数据。 6.3.3 append vs write record append操作在我们的系统中被大量使用，尤其是我们的生产系统。在集群X中，write操作和record append操作的操作次数比例为8:1，字节传输比例为108:1。在集群Y中，这二者的比例分别为2.5:1和3.7:1。这些数据显示了对于两个集群来说，record append操作的规模通常比write打。然而。在集群X中，测量期间record append的使用量非常的少。因此。这个测量结果可能受一两个有特定缓冲区大小的应用程序影响较大。 正如我们预期的那样，我们的数据变更负载主要来自于append而非overwrite。我们测量了primary副本上overwrite的数据总量。测量值很接近client故意overwrite数据而不append的情况。对于集群X，overwrite的操作总量低于变更操作的0.0003%，字节数占比低于总量的0.0001%。对于集群Y，这两个数据均为0.05%。尽管这个比例已经很小了，但仍比我们预期的要高。大部分的overwrite都是由client因错误或超时而重试造成的。这本质上是由重试机制造成的而非工作负载。 表5 各种大小的操作字节传输量占比（%） 对于read操作，数据大小为实际读取和传输的数据大小，而非请求读取的总大小。二者的区别为，读取请求可能会试图读取超过文件末尾的内容。在我们的设计中，这不是常见的负载。 操作类型 read write record append 集群 X Y X Y X Y 1B...1K \u003c .1 \u003c .1 \u003c .1 \u003c .1 \u003c .1 \u003c .1 1K...8K 13.8 3.9 \u003c .1 \u003c .1 \u003c .1 0.1 8K...64K 11.4 9.3 2.4 5.9 2.3 0.3 64K...128K 0.3 0.7 0.3 0.3 22.7 1.2 128K...256K 0.8 0.6 16.5 0.2 \u003c .1 5.8 256K...512K 1.4 0.3 3.4 7.7 \u003c .1 38.4 512K...1M 65.9 55.1 74.1 58.0 .1 46.8 1M...inf 6.4 30.1 3.3 28.0 53.9 7.4 6.3.4 master的负载 表6展示了对master的各种类型的请求占比。其中，大部分请求来自read操作询问chunk位置的请求（FindLocation）和数据变更操作询问租约持有者（FindLeaseLocker）。 集群X与集群Y中Delete请求量差异很大，因为集群Y存储被经常重新生成或者移动的生产数据。一些Delete请求量的差异还隐藏在Open请求中，因为打开并从头写入文件时（Unix中以“w”模式打开文件），会隐式地删除旧版本的文件。 FindMatchingFiles是用来支持“ls”或类似文件系统操作的模式匹配请求。不像给master的其他请求，FindMatchingFiles请求可能处理很大一部分命名空间，因此这种请求开销很高。在集群Y中，这种请求更加频繁，因为自动化的数据处理任务常通过检查部分文件系统的方式来了解应用程序的全局状态。相反，使用集群X的应用程序会被用户更明确地控制，通常会提交知道所需的文件名。 表6 master请求类型占比（%） 集群 X Y Open 26.1 16.3 Delete 0.7 1.5 FindLocation 64.3 65.8 FindLeaseHolder 7.8 13.4 FindMatchingFiles 0.6 2.2 All otder combined 0.5 0.8 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:8:3","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"7. 开发经历 在构建和部署GFS的过程中，我们经历了很多问题。其中，有些是操作问题，有些是技术问题。 最初，GFS的构思是将其作为我们生产系统的后端文件系统。随着时间推移，GFS的用途演变为包括了研究和开发任务。GFS开始时几乎不支持权限、配额之类的功能，但现在这些功能都变为GFS包含的基本功能。虽然生产系统有着良好的纪律并被良好地控制着，但用户有时却没有。因此，其需要更多的基础设施来防止用户互相干扰。 我们最大的一些问题是磁盘问题和Linux相关问题。我们的许多磁盘都想Linux驱动程序声称它们支持很多版本的IDE（译注：本文IDE指集成设备电路Intergated Drive Electronics）协议，但事实上，它们可能只能可靠地响应最近几个版本的协议。因为这些协议都非常相似，所以大部分时间驱动器都能正常工作。但协议版本偶尔不匹配就会导致驱动器和内核中所认为的驱动器的状态不一致。由于内核中的问题，数据会无法察觉地损坏。这个问题驱动我们通过校验和的方式检测数据是否损坏，同时我们修改了内核去处理协议不匹配的问题。 早些时候，由于*fsync()*的开销，我们在Linux2.2内核中遇到了一些问题。这个函数的开销和文件成正比，而不是和修改的部分大小成正比。这对我们使用较大的操作日志造成了问题（特别是在我们实现检查点机制以前）。我们曾经通过同步写入的方式来解决这个问题，直到迁移到Linux2.4。 另一个Linux的问题是一个读写锁。当任何地址空间的线程从磁盘换入页（读锁）或者通过*mmap()函数修改地址空间（写锁）时，都必须持有这一个读写锁。我们发现系统在轻负载下的一个瞬间会出现超时问题，所以我们努力地去寻找资源瓶颈和零星的硬件故障。最终，我们发现在磁盘线程正在换入之前映射的文件时，这个读写锁阻塞了网络主线程，导致其无法将新数据映射到内存。因为我们主要受网络接口限制而非受内存拷贝带宽限制，所以我们用pread()替换了mmap()*函数，其代价是多了一次额外的拷贝操作。 尽管偶尔会有问题发生，Linux代码的可用性还是帮助了我们一次又一次地探索和理解系统行为。当时机合适时，我们会改进内核并将这些改进与开源社区分享。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:9:0","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"8. 相关工作 就像其他大型的分布式文件系统一样（如AFS[5]），GFS提供了与位置无关的命名空间，这可以允许数据为了负载均衡和容错地移动，这一操作对client是透明的。但与AFS不同，GFS将文件数据通过类似xFS[1]和Swtift[3]的方式分散到了存储服务器上，以释放集群整体性能并提高容错能力。 因为磁盘相对廉价且副本的方式比复杂的RAID[9]的方式简单很多，所以GFS仅通过副本的方式作为冗余，因此GFS会比xFS或Swift消耗更多的原始存储空间。与类似AFS、xFS、Frangipani[12]和Intermezzo[6]的文件系统不同，GFS在系文件系统接口下没有提供任何的缓存。在我们的目标工作负载中，一个应用程序几乎不会重用数据，因为其或者流式地处理一个大型数据集，或者每次仅在大型数据及中随机地seek并读取很小一部分的数据。 一些分布式文件系统移除了集中式的服务器，并依赖分布式算法来实现一致性和管理，如Frangipani、xFS、Minnesota’s GFS[11]和GPFS[10]。我们选择了集中式的方法来简化设计、增强可靠性，同时还获得了灵活性。集中式的master还大大简化了复杂的chunk分配操作和重分配副本的策略，因为master已经有了大部分相关信息，且由master来控制如何变化。我们通过保持master的状态大小很小并在其他机器上有充足的副本的方式来提高容错能力。可伸缩性和高可用性（对于read操作来说）目前通过影子master服务器机制提供。master状态的变化会通过追加到预写日志的方式进行持久化。因此我们可以通过适配像Harp[7]中的主拷贝模式（primary-copy scheme）的方法，来提供比当前的一致性有更强保证的高可用性。 我们遇到了一个类似Lustre[8]的问题，即为大量client提供整体的性能。然而，我们通过将重点放在我们的应用程序的需求而不是构架一个兼容POSIX文件系统的方式，大幅简化了这个问题。除此之外，GFS假设大量的设备是不可靠的，因此容错是我们设计中的中心问题。 GFS非常接近NASD架构[4]。NASD架构基于通过网络连接的磁盘驱动器，而GFS使用一般的商用机器作为chunkserver，就像NASD的原型那样。与NASD不同是，我们的chunkserver懒式分配固定大小的chunk，而不是可变长的对象。另外，GFS实现了如负载均衡、副本重分配和恢复等在生产环境中需要的特性。 与Minnesota’s GFS或NASD不同，我们不希望改变存储设备的模型。我们着重解决由已有的商用设备组成的复杂的分布式系统的日常数据处理问题。 对生产者-消费者队列的原子性record append操作解决了类似于River的分布式队列问题。River[2]使用了分布在不同机器上的基于内存的队列和谨慎的数据流控制，而GFS采用了可以被多个生产者并发追加的持久化文件。River的模型支持M:N的分布式队列，但缺少持久化存储带来的容错能力。而GFS仅支持M:1的高效的队列。多个消费者可一个读相同的文件，但必须相互协调载入的分区。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:10:0","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"9. 结论 Google File System论证了在产品级硬件上支持大规模数据处理负载的必要特性。虽然很多设计是为我们特殊的场景定制的，但很多设计可能适用于规模和预算相似的数据处理任务。 我们根据我们当前和预期的应用程序负载和技术环境，重新考察了传统文件系统设计中的假设。我们的考察结果指向了完全不同的设计。我们视设备故障为平常事件而非异常事件。我们优化了大部分操作为追加（可能是并发追加）和读取（通常为顺序读取）的大文件。我们还扩展并放宽了标准文件系统接口来改进整个系统。 我们的系统通过持续的监控、备份关键数据、自动且快速的恢复来提供容错能力。chunk副本让我们能够容忍chunkserver故障。这些故障的频率让我们设计了一种新的在线修复机制：周期性地对client不可见的修复损坏数据，并尽快补充丢失的副本。另外，我们通过校验和的方式来检测磁盘或IDE子系统级别的数据损坏，因为GFS系统中磁盘数量很多，这类问题是非常普遍的。 我们的设计为并发执行多种任务的reader和writer提供了很高的整体吞吐量。为了实现这一点，我们将通过master进行的文件系统控制和通过chunkserver、client的数据传输分离开来。我们还通过选取较大的chunk大小和chunk租约（将数据变更授权给primary副本）的方式最小化了master对一般操作的参与度。这种方式让master变得简单，且中心化的master不会成为系统瓶颈。我们相信，通过改进网络栈，会减少当前对单个client的写入吞吐量的限制。 GFS成功地满足了我们的存储需求，并已经在Google内部作为研究、开发和生产数据处理的存储平台使用。GFS是让我们能够进一步创新并攻克web规模问题的重要工具。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:11:0","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"致谢 感谢以下对本系统或本论文做出了贡献的人。Brain Bershad（我们的指导者）和给我我们珍贵的评论和建议的匿名评审员。Anurag Acharya、Jeff Dean和David Desjardins为系统的早期设计做出了贡献。Fay Chang致力于chunkserver间副本比较的研究。Guy Edjlali致力于存储配额的研究。Markus Gutschke致力于测试框架与安全性增强的研究。Fay Chang、Urs Hoelzle、Max Ibel、Sharon Perl、Rob Pike和Debby Wallach对本论文早期的草稿做出了评论。我们在Google的许多勇敢的同事，他们信任这个新文件系统并给我们提出了很多很有用的反馈。Yoshka在早期的测试中提供了帮助。 ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:12:0","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["Paper Reading"],"content":"参考文献 [1] Thomas Anderson, Michael Dahlin, Jeanna Neefe, David Patterson, Drew Roselli, and Randolph Wang. Serverless networkfile systems. In Proceedings of the 15th ACM Symposium on Operating System Principles, pages 109–126, Copper Mountain Resort, Colorado, December 1995. [2] Remzi H. Arpaci-Dusseau, Eric Anderson, Noah Treuhaft, David E. Culler, Joseph M. Hellerstein, David Patterson, and Kathy Yelick. Cluster I/O with River: Making the fast case common. In Proceedings of the Sixth Workshop on Input/Output in Parallel and Distributed Systems (IOPADS ’99), pages 10–22, Atlanta, Georgia, May 1999. [3] Luis-Felipe Cabrera and Darrell D. E. Long. Swift: Using distributed diskstriping to provide high I/O data rates. Computer Systems, 4(4):405–436, 1991. [4] Garth A. Gibson, David F. Nagle, Khalil Amiri, Jeff Butler, Fay W. Chang, Howard Gobioff, Charles Hardin, ErikRiedel, David Rochberg, and Jim Zelenka. A cost-effective, high-bandwidth storage architecture. In Proceedings of the 8th Architectural Support for Programming Languages and Operating Systems, pages 92–103, San Jose, California, October 1998. [5] John Howard, Michael Kazar, Sherri Menees, David Nichols, Mahadev Satyanarayanan, Robert Sidebotham, and Michael West. Scale and performance in a distributed file system. ACM Transactions on Computer Systems, 6(1):51–81, February 1988. [6] InterMezzo. http://www.inter-mezzo.org, 2003. [7] Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Williams. Replication in the Harp file system. In 13th Symposium on Operating System Principles, pages 226–238, Pacific Grove, CA, October 1991. [8] Lustre. http://www.lustreorg, 2003. [9] David A. Patterson, Garth A. Gibson, and Randy H. Katz. A case for redundant arrays of inexpensive disks (RAID). In Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data, pages 109–116, Chicago, Illinois, September 1988. [10] FrankSchmuckand Roger Haskin. GPFS: A shared-diskfile system for large computing clusters. In Proceedings of the First USENIX Conference on File and Storage Technologies, pages 231–244, Monterey, California, January 2002. [11] Steven R. Soltis, Thomas M. Ruwart, and Matthew T. O’Keefe. The Gobal File System. In Proceedings of the Fifth NASA Goddard Space Flight Center Conference on Mass Storage Systems and Technologies, College Park, Maryland, September 1996. [12] Chandramohan A. Thekkath, Timothy Mann, and Edward K. Lee. Frangipani: A scalable distributed file system. In Proceedings of the 16th ACM Symposium on Operating System Principles, pages 224–237, Saint-Malo, France, October 1997. ","date":"2020-07-21","objectID":"/posts/paper-reading/gfs-sosp2003/:13:0","tags":["GFS","Translation"],"title":"《The Google File System》论文翻译（GFS-SOSP2003）","uri":"/posts/paper-reading/gfs-sosp2003/"},{"categories":["CCF CSP"],"content":"CCF CSP 201403 T1 相反数 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t1/:0:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T1 相反数","uri":"/posts/csp/csp-201403-t1/"},{"categories":["CCF CSP"],"content":"题目 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t1/:1:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T1 相反数","uri":"/posts/csp/csp-201403-t1/"},{"categories":["CCF CSP"],"content":"问题描述 有 N 个非零且各不相同的整数。请你编一个程序求出它们中有多少对相反数(a 和 -a 为一对相反数)。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t1/:1:1","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T1 相反数","uri":"/posts/csp/csp-201403-t1/"},{"categories":["CCF CSP"],"content":"输入格式 第一行包含一个正整数 N。(1 ≤ N ≤ 500)。 第二行为 N 个用单个空格隔开的非零整数,每个数的绝对值不超过1000,保证这些整数各不相同。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t1/:1:2","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T1 相反数","uri":"/posts/csp/csp-201403-t1/"},{"categories":["CCF CSP"],"content":"输出格式 只输出一个整数,即这 N 个数中包含多少对相反数。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t1/:1:3","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T1 相反数","uri":"/posts/csp/csp-201403-t1/"},{"categories":["CCF CSP"],"content":"样例输入 5 1 2 3 -1 -2 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t1/:1:4","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T1 相反数","uri":"/posts/csp/csp-201403-t1/"},{"categories":["CCF CSP"],"content":"样例输出 2 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t1/:1:5","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T1 相反数","uri":"/posts/csp/csp-201403-t1/"},{"categories":["CCF CSP"],"content":"时间限制 1.0s ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t1/:1:6","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T1 相反数","uri":"/posts/csp/csp-201403-t1/"},{"categories":["CCF CSP"],"content":"内存限制 256.0MB ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t1/:1:7","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T1 相反数","uri":"/posts/csp/csp-201403-t1/"},{"categories":["CCF CSP"],"content":"题解 用set存出现过的数，如果找到相反数答案+1。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t1/:2:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T1 相反数","uri":"/posts/csp/csp-201403-t1/"},{"categories":["CCF CSP"],"content":"代码 #include \u003ciostream\u003e#include \u003cset\u003e using namespace std; set\u003cint\u003e all; int main() { int n, x, ans = 0; all.clear(); cin \u003e\u003e n; while (n--) { cin \u003e\u003e x; all.insert(x); if (all.find(-x) != end(all)) ans++; } cout \u003c\u003c ans; return 0; } ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t1/:3:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T1 相反数","uri":"/posts/csp/csp-201403-t1/"},{"categories":["CCF CSP"],"content":"CCF CSP 201403 T2 窗口 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t2/:0:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T2 窗口","uri":"/posts/csp/csp-201403-t2/"},{"categories":["CCF CSP"],"content":"题目 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t2/:1:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T2 窗口","uri":"/posts/csp/csp-201403-t2/"},{"categories":["CCF CSP"],"content":"问题描述 在某图形操作系统中,有 N 个窗口,每个窗口都是一个两边与坐标轴分别平行的矩形区域。窗口的边界上的点也属于该窗口。窗口之间有层次的区别,在多于一个窗口重叠的区域里,只会显示位于顶层的窗口里的内容。 当你点击屏幕上一个点的时候,你就选择了处于被点击位置的最顶层窗口,并且这个窗口就会被移到所有窗口的最顶层,而剩余的窗口的层次顺序不变。如果你点击的位置不属于任何窗口,则系统会忽略你这次点击。 现在我们希望你写一个程序模拟点击窗口的过程。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t2/:1:1","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T2 窗口","uri":"/posts/csp/csp-201403-t2/"},{"categories":["CCF CSP"],"content":"输入格式 输入的第一行有两个正整数,即 N 和 M。(1 ≤ N ≤ 10,1 ≤ M ≤ 10) 接下来 N 行按照从最下层到最顶层的顺序给出 N 个窗口的位置。 每行包含四个非负整数 x1, y1, x2, y2,表示该窗口的一对顶点坐标分别为 (x1, y1) 和 (x2, y2)。保证 x1 \u003c x2,y1 2。 接下来 M 行每行包含两个非负整数 x, y,表示一次鼠标点击的坐标。 题目中涉及到的所有点和矩形的顶点的 x, y 坐标分别不超过2559和1439。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t2/:1:2","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T2 窗口","uri":"/posts/csp/csp-201403-t2/"},{"categories":["CCF CSP"],"content":"输出格式 输出包括 M 行,每一行表示一次鼠标点击的结果。如果该次鼠标点击选择了一个窗口,则输出这个窗口的编号(窗口按照输入中的顺序从 1 编号到 N);如果没有,则输出\"IGNORED”(不含双引号)。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t2/:1:3","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T2 窗口","uri":"/posts/csp/csp-201403-t2/"},{"categories":["CCF CSP"],"content":"样例输入 3 4 0 0 4 4 1 1 5 5 2 2 6 6 1 1 0 0 4 4 0 5 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t2/:1:4","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T2 窗口","uri":"/posts/csp/csp-201403-t2/"},{"categories":["CCF CSP"],"content":"样例输出 2 1 1 IGNORED ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t2/:1:5","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T2 窗口","uri":"/posts/csp/csp-201403-t2/"},{"categories":["CCF CSP"],"content":"样例说明 第一次点击的位置同时属于第 1 和第 2 个窗口,但是由于第 2 个窗口在上面,它被选择并且被置于顶层。 第二次点击的位置只属于第 1 个窗口,因此该次点击选择了此窗口并将其置于顶层。现在的三个窗口的层次关系与初始状态恰好相反了。 第三次点击的位置同时属于三个窗口的范围,但是由于现在第 1 个窗口处于顶层,它被选择。 最后点击的 (0, 5) 不属于任何窗口。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t2/:1:6","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T2 窗口","uri":"/posts/csp/csp-201403-t2/"},{"categories":["CCF CSP"],"content":"时间限制 1.0s ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t2/:1:7","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T2 窗口","uri":"/posts/csp/csp-201403-t2/"},{"categories":["CCF CSP"],"content":"内存限制 256.0MB ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t2/:1:8","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T2 窗口","uri":"/posts/csp/csp-201403-t2/"},{"categories":["CCF CSP"],"content":"题解 简单模拟题，注意边界条件，将被点击的窗口移动到最前。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t2/:2:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T2 窗口","uri":"/posts/csp/csp-201403-t2/"},{"categories":["CCF CSP"],"content":"代码 #include \u003ciostream\u003e#include \u003cvector\u003e using namespace std; struct point { int x, y; point(int _x, int _y) { this-\u003ex = _x; this-\u003ey = _y; } }; struct rect { point s, t; int id = 0; rect(int _x1, int _y1, int _x2, int _y2, int _id) : s(_x1, _y1), t(_x2, _y2) { this-\u003eid = _id; } }; vector\u003crect\u003e windows; bool in(point p, rect r) { if (p.x \u003e= r.s.x \u0026\u0026 p.x \u003c= r.t.x \u0026\u0026 p.y \u003e= r.s.y \u0026\u0026 p.y \u003c= r.t.y) return true; return false; } int main() { int n, m; windows.clear(); cin \u003e\u003e n \u003e\u003e m; for (int i = 1; i \u003c= n; i++) { int x1, y1, x2, y2; cin \u003e\u003e x1 \u003e\u003e y1 \u003e\u003e x2 \u003e\u003e y2; windows.insert(windows.begin(), rect(x1, y1, x2, y2, i)); } while (m--) { int x, y; cin \u003e\u003e x \u003e\u003e y; point p = {x, y}; bool ignored = true; for (vector\u003crect\u003e::iterator it = windows.begin(); it != windows.end(); it++) { if (in(p, *it)) { cout \u003c\u003c it-\u003eid \u003c\u003c endl; ignored = false; rect window = *it; windows.erase(it); windows.insert(windows.begin(), window); break; } } if (ignored) cout \u003c\u003c \"IGNORED\" \u003c\u003c endl; } return 0; } ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t2/:3:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T2 窗口","uri":"/posts/csp/csp-201403-t2/"},{"categories":["CCF CSP"],"content":"CCF CSP 201403 T3 命令行选项 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t3/:0:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T3 命令行选项","uri":"/posts/csp/csp-201403-t3/"},{"categories":["CCF CSP"],"content":"题目 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t3/:1:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T3 命令行选项","uri":"/posts/csp/csp-201403-t3/"},{"categories":["CCF CSP"],"content":"问题描述 请你写一个命令行分析程序,用以分析给定的命令行里包含哪些选项。每个命令行由若干个字符串组成,它们之间恰好由一个空格分隔。这些字符串中的第一个为该命令行工具的名字,由小写字母组成,你的程序不用对它进行处理。在工具名字之后可能会包含若干选项,然后可能会包含一 些不是选项的参数。 选项有两类:带参数的选项和不带参数的选项。一个合法的无参数选项的形式是一个减号后面跟单个小写字母,如”-a” 或”-b”。而带参数选项则由两个由空格分隔的字符串构成,前者的格式要求与无参数选项相同,后者则是该选项的参数,是由小写字母,数字和减号组成的非空字符串。 该命令行工具的作者提供给你一个格式字符串以指定他的命令行工具需要接受哪些选项。这个字符串由若干小写字母和冒号组成,其中的每个小写字母表示一个该程序接受的选项。如果该小写字母后面紧跟了一个冒号,它就表示一个带参数的选项,否则则为不带参数的选项。例如, “abⓜ” 表示该程序接受三种选项,即”-a”(不带参数),\"-b”(带参数), 以及”-m”(带参数)。 命令行工具的作者准备了若干条命令行用以测试你的程序。对于每个命令行,你的工具应当一直向后分析。当你的工具遇到某个字符串既不是合法的选项,又不是某个合法选项的参数时,分析就停止。命令行剩余的未分析部分不构成该命令的选项,因此你的程序应当忽略它们。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t3/:1:1","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T3 命令行选项","uri":"/posts/csp/csp-201403-t3/"},{"categories":["CCF CSP"],"content":"输入格式 输入的第一行是一个格式字符串,它至少包含一个字符,且长度不超过 52。格式字符串只包含小写字母和冒号,保证每个小写字母至多出现一次,不会有两个相邻的冒号,也不会以冒号开头。 输入的第二行是一个正整数 N(1 ≤ N ≤ 20),表示你需要处理的命令行的个数。 接下来有 N 行,每行是一个待处理的命令行,它包括不超过 256 个字符。该命令行一定是若干个由单个空格分隔的字符串构成,每个字符串里只包含小写字母,数字和减号。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t3/:1:2","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T3 命令行选项","uri":"/posts/csp/csp-201403-t3/"},{"categories":["CCF CSP"],"content":"输出格式 输出有 N 行。其中第 i 行以\"Case i:” 开始,然后应当有恰好一个空格,然后应当按照字母升序输出该命令行中用到的所有选项的名称,对于带参数的选项,在输出它的名称之后还要输出它的参数。如果一个选项在命令行中出现了多次,只输出一次。如果一个带参数的选项在命令行中出 现了多次,只输出最后一次出现时所带的参数。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t3/:1:3","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T3 命令行选项","uri":"/posts/csp/csp-201403-t3/"},{"categories":["CCF CSP"],"content":"样例输入 albw:x 4 ls -a -l -a documents -b ls ls -w 10 -x -w 15 ls -a -b -c -d -e -l ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t3/:1:4","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T3 命令行选项","uri":"/posts/csp/csp-201403-t3/"},{"categories":["CCF CSP"],"content":"样例输出 Case 1: -a -l Case 2: Case 3: -w 15 -x Case 4: -a -b ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t3/:1:5","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T3 命令行选项","uri":"/posts/csp/csp-201403-t3/"},{"categories":["CCF CSP"],"content":"时间限制 1.0s ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t3/:1:6","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T3 命令行选项","uri":"/posts/csp/csp-201403-t3/"},{"categories":["CCF CSP"],"content":"内存限制 256.0MB ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t3/:1:7","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T3 命令行选项","uri":"/posts/csp/csp-201403-t3/"},{"categories":["CCF CSP"],"content":"题解 字符串题，据说CSP现场编译器还不支持C++11的正则表达式。 认真读题，冷静Debug。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t3/:2:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T3 命令行选项","uri":"/posts/csp/csp-201403-t3/"},{"categories":["CCF CSP"],"content":"代码 #include \u003ciostream\u003e#include \u003cstring\u003e#include \u003cmap\u003e#include \u003ccstring\u003e#include \u003cvector\u003e using namespace std; map\u003cchar, bool\u003e hasV; map\u003cchar, string\u003e *mapper(string str) { map\u003cchar, string\u003e *paras = new map\u003cchar, string\u003e(); vector\u003cstring\u003e *cmd = new vector\u003cstring\u003e(); paras-\u003eclear(); cmd-\u003eclear(); int s, t; s = 0; while ((t = str.find(\" \", s)) != string::npos) { cmd-\u003epush_back(str.substr(s, t - s)); s = t + 1; } cmd-\u003epush_back(str.substr(s, str.length() - s)); for (int i = 1; i \u003c cmd-\u003esize(); i++) { string now = cmd-\u003eat(i); if (now.length() != 2 || now[0] != '-') break; char k = now[1]; if (hasV.find(k) == hasV.end()) break; if (!hasV[k]) { (*paras)[k] = \"\"; continue; } if (i == cmd-\u003esize() - 1) break; string v = cmd-\u003eat(i + 1); (*paras)[k] = v; i++; } return paras; } int main() { hasV.clear(); string pattern; int n; getline(cin, pattern); char last = 0; for (char c : pattern) { if (c == ':') { hasV[last] = true; continue; } hasV[c] = false; last = c; } cin \u003e\u003e n; getchar(); for (int k = 1; k \u003c= n; k++) { cout \u003c\u003c \"Case \" \u003c\u003c k \u003c\u003c \":\" \u003c\u003c \" \"; string str; getline(cin, str); map\u003cchar, string\u003e *paras = mapper(str); for (map\u003cchar, string\u003e::iterator it = paras-\u003ebegin(); it != paras-\u003eend(); it++) { cout \u003c\u003c '-' \u003c\u003c it-\u003efirst \u003c\u003c ' '; if (it-\u003esecond != \"\") cout \u003c\u003c it-\u003esecond \u003c\u003c ' '; } cout \u003c\u003c endl; } return 0; } ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t3/:3:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T3 命令行选项","uri":"/posts/csp/csp-201403-t3/"},{"categories":["CCF CSP"],"content":"CCF CSP 201403 T4 无线网络 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t4/:0:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T4 无线网络","uri":"/posts/csp/csp-201403-t4/"},{"categories":["CCF CSP"],"content":"题目 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t4/:1:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T4 无线网络","uri":"/posts/csp/csp-201403-t4/"},{"categories":["CCF CSP"],"content":"问题描述 目前在一个很大的平面房间里有 n 个无线路由器,每个无线路由器都固定在某个点上。任何两个无线路由器只要距离不超过 r 就能互相建立网络连接。 除此以外,另有 m 个可以摆放无线路由器的位置。你可以在这些位置中选择至多 k 个增设新的路由器。 你的目标是使得第 1 个路由器和第 2 个路由器之间的网络连接经过尽量少的中转路由器。请问在最优方案下中转路由器的最少个数是多少? ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t4/:1:1","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T4 无线网络","uri":"/posts/csp/csp-201403-t4/"},{"categories":["CCF CSP"],"content":"输入格式 第一行包含四个正整数 n,m,k,r。(2 ≤ n ≤ 100,1 ≤ k ≤ m ≤ 100, 1 ≤ r ≤ 108)。 接下来 n 行,每行包含两个整数 xi 和 yi,表示一个已经放置好的无线 路由器在 (xi, yi) 点处。输入数据保证第 1 和第 2 个路由器在仅有这 n 个路由器的情况下已经可以互相连接(经过一系列的中转路由器)。 接下来 m 行,每行包含两个整数 xi 和 yi,表示 (xi, yi) 点处可以增设 一个路由器。 输入中所有的坐标的绝对值不超过 108,保证输入中的坐标各不相同。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t4/:1:2","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T4 无线网络","uri":"/posts/csp/csp-201403-t4/"},{"categories":["CCF CSP"],"content":"输出格式 输出只有一个数,即在指定的位置中增设 k 个路由器后,从第 1 个路 由器到第 2 个路由器最少经过的中转路由器的个数。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t4/:1:3","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T4 无线网络","uri":"/posts/csp/csp-201403-t4/"},{"categories":["CCF CSP"],"content":"样例输入 5 3 1 3 0 0 5 5 0 3 0 5 3 5 3 3 4 4 3 0 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t4/:1:4","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T4 无线网络","uri":"/posts/csp/csp-201403-t4/"},{"categories":["CCF CSP"],"content":"样例输出 2 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t4/:1:5","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T4 无线网络","uri":"/posts/csp/csp-201403-t4/"},{"categories":["CCF CSP"],"content":"时间限制 1.0s ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t4/:1:6","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T4 无线网络","uri":"/posts/csp/csp-201403-t4/"},{"categories":["CCF CSP"],"content":"内存限制 256.0MB ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t4/:1:7","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T4 无线网络","uri":"/posts/csp/csp-201403-t4/"},{"categories":["CCF CSP"],"content":"题解 这道题是一道限制经过某类点最大次数的最短路问题,这类问题可以通过二维距离解决。由于图的边权均为1，因此直接使用二维距离BFS即可，如dis[i][j]表示从0号路由器到i号路由器，在新增了j个路由器时的最少总路由器数。 之前在网上查看大多数的题解都是直接在一维的距离上使用BFS，由于这道题数据比较弱，可以AC，但是实际上这样如下问题: 当在BFS过程中，已经过的新增路由器已经耗尽了可以添加的最大次数，而未经过的路由器中存在添加后可以缩短的路径长度大于已添加某一新增路由器能够缩短的路径长度。由于dis只有一维，BFS对每个路由器只会遍历一次，因此不会跳过在前面的新增路由器，导致得到的答案大于最优解。 如果上面这段话比较绕嘴，可以通过如下的一组数据来演示： 13 2 1 1 0 0 6 0 0 1 1 1 2 1 2 0 3 0 4 0 4 1 4 2 5 2 6 2 6 1 1 0 5 0 将这组数据绘制成图，虚线表示0号和1号路由器，白点表示已有的路由器，黑点表示可以添加的路由器。可以互相连通的路由器中间使用直线连接： originorigin \" origin 在这组数据中，k=1。如果dis只有一维，在遇到第一个黑点时会经过，这样到黑点后的第一个路由器的距离为2，我们设这个点为x点。当绕过黑点的路径到达该点时，该点已被遍历，不会再次遍历。而到达第二个黑点时，由于第一个黑点已经耗尽了增加路由器的次数，因此只能绕更远的路。最终导致得到的答案为9。其路径如下图所示； wrongwrong \" wrong 而使用二维距离时，dis[i][j]表示从0号路由器到i号路由器，在新增了j个路由器时的最少总路由器数。因此对于我们命名为x的点时，有两种不同的状态：dis[x][1]=2与dis[x][0]=4。这样，可以通过dis[x][0]=4继续遍历，经过第二个黑点找到到达终点的真正最短路7。其路径如下图所示： correctcorrect \" correct 通过比较网上一维BFS的结果与二维BFS在这组数据下得到的结果，证明这个问题确实存在。 ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t4/:2:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T4 无线网络","uri":"/posts/csp/csp-201403-t4/"},{"categories":["CCF CSP"],"content":"代码 这里为了复习最短路的写法，使用SPFA的方式写的。 #include \u003ciostream\u003e#include \u003ccstring\u003e#include \u003cvector\u003e#include \u003cqueue\u003e #define ll long long #define N 205 using namespace std; struct point { ll x, y; }; queue\u003cint\u003e q; bool inq[N]; vector\u003cint\u003e nxt[N]; point pos[N]; // index \u003c n : 已有 int dis[N][N]; bool reach(point a, point b, ll r) { if ((a.x - b.x) * (a.x - b.x) + (a.y - b.y) * (a.y - b.y) \u003c= r * r) return true; return false; } int main() { int n, m, k; ll r; cin \u003e\u003e n \u003e\u003e m \u003e\u003e k \u003e\u003e r; memset(inq, 0, sizeof(inq)); memset(dis, 0x7f, sizeof(dis)); while (!q.empty()) q.pop(); for (int i = 0; i \u003c n + m; i++) cin \u003e\u003e pos[i].x \u003e\u003e pos[i].y; for (int i = 0; i \u003c n + m; i++) for (int j = 0; j \u003c n + m; j++) if (reach(pos[i], pos[j], r)) nxt[i].push_back(j); q.push(0); inq[0] = true; dis[0][0] = 0; while (!q.empty()) { int s = q.front(); q.pop(); inq[s] = false; if (s == 1) break; for (int c = 0; c \u003c= k; c++) for (vector\u003cint\u003e::iterator it = nxt[s].begin(); it != nxt[s].end(); it++) { int t = *it; int ct = (s \u003c n) ? (c) : (c + 1); if (ct \u003e k) continue; if (dis[t][ct] \u003e dis[s][c] + 1) { dis[t][ct] = dis[s][c] + 1; if (!inq[t]) { q.push(t); inq[t] = true; } } } } int ans = m + n + 1; for (int c = 0; c \u003c= k; c++) if (dis[1][c] \u003c ans) ans = dis[1][c]; cout \u003c\u003c ans - 1; return 0; } ","date":"2019-07-30","objectID":"/posts/csp/csp-201403-t4/:3:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201403 T4 无线网络","uri":"/posts/csp/csp-201403-t4/"},{"categories":["Docker"],"content":"带LDAP认证与WebUI的Docker私有仓库搭建 ","date":"2019-07-22","objectID":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/:0:0","tags":["Deployment"],"title":"带LDAP认证与WebUI的Docker私有仓库搭建","uri":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/"},{"categories":["Docker"],"content":"1. 获取docker仓库并运行 docker pull registry:latest docker run -d -p 127.0.0.1:5000:5000 --name registry-localhost -v /opt/docker-registry:/var/lib/registry --restart=always registry:latest ","date":"2019-07-22","objectID":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/:1:0","tags":["Deployment"],"title":"带LDAP认证与WebUI的Docker私有仓库搭建","uri":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/"},{"categories":["Docker"],"content":"2. 获取WebUI并运行 Docker Registry的WebUI有很多，这里使用konradkleine/docker-registry-frontend:v2 docker pull konradkleine/docker-registry-frontend:v2 docker run -d -e ENV_DOCKER_REGISTRY_HOST=barricade.ivic.org.cn -e ENV_DOCKER_REGISTRY_PORT=80 -p 127.0.0.1:8081:80 --restart=always --name frontend-localhost konradkleine/docker-registry-frontend:v2 ","date":"2019-07-22","objectID":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/:2:0","tags":["Deployment"],"title":"带LDAP认证与WebUI的Docker私有仓库搭建","uri":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/"},{"categories":["Docker"],"content":"3. 搭建带有LDAP认证的Nginx服务 预编译版本的nginx默认不带有LDAP认证所需模块，需要手动编译nginx与拓展模块。 ","date":"2019-07-22","objectID":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/:3:0","tags":["Deployment"],"title":"带LDAP认证与WebUI的Docker私有仓库搭建","uri":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/"},{"categories":["Docker"],"content":"3.1 编译安装nginx依赖 # 编译安装pcre wget ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre-8.42.tar.gz tar -zxf pcre-8.42.tar.gz cd pcre-8.42 ./configure make sudo make install # 编译安装zlib wget http://zlib.net/zlib-1.2.11.tar.gz tar -zxf zlib-1.2.11.tar.gz cd zlib-1.2.11 ./configure make sudo make install # 如果没有安装openssl，请自行安装openssl # ... # 安装ldap依赖库 # Ubuntu/Debian sudo apt install libldap2-dev # CentOS yum install openldap-devel ","date":"2019-07-22","objectID":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/:3:1","tags":["Deployment"],"title":"带LDAP认证与WebUI的Docker私有仓库搭建","uri":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/"},{"categories":["Docker"],"content":"3.2 下载nginx与拓展模块源码 # 下载解压nginx源码 wget https://nginx.org/download/nginx-1.15.11.tar.gz tar zxf nginx-1.15.11.tar.gz # 下载解压nginx-auth-ldap wget https://github.com/kvspb/nginx-auth-ldap/archive/master.zip unzip master.zip ","date":"2019-07-22","objectID":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/:3:2","tags":["Deployment"],"title":"带LDAP认证与WebUI的Docker私有仓库搭建","uri":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/"},{"categories":["Docker"],"content":"3.3 编译安装nginx # 编译nginx cd nginx-1.15.11 ./configure --sbin-path=/usr/local/nginx/nginx --conf-path=/etc/nginx/nginx.conf --pid-path=/usr/local/nginx/nginx.pid --with-pcre=../pcre-8.42 --with-zlib=../zlib-1.2.11 --with-http_ssl_module --with-stream --with-mail=dynamic --add-module=~/nginx-auth-ldap-master make sudo make install # 创建软连接 sudo ln -s /usr/local/nginx/nginx /usr/bin/nginx ","date":"2019-07-22","objectID":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/:3:3","tags":["Deployment"],"title":"带LDAP认证与WebUI的Docker私有仓库搭建","uri":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/"},{"categories":["Docker"],"content":"3.4 编写nginx配置文件/etc/nginx/nginx.conf 注意替换ldap_server中参数，同时注意其他位置关于host与port的配置 worker_processes 1; events { worker_connections 1024; } http { upstream docker-registry { server localhost:5000; } ldap_server ldapserver { url ldap://\u003cLDAP-SERVER-HOST\u003e:\u003cLDAP-SERVER-PORT\u003e/\u003cOU=...,DC=...\u003e?samaccountname?sub?(objectClass=user); binddn \u003cBINDDN\u003e; binddn_passwd \u003cPASSWORD-FOR-BINDDN\u003e; group_attribute uniquemember; group_attribute_is_dn on; } server { listen 80; error_log /var/log/nginx/error.log debug; access_log /var/log/nginx/access.log; client_max_body_size 0; chunked_transfer_encoding on; location / { return 301 http://barricade.ivic.org.cn:80/v2; } location /v2/ { # Do not allow connections from docker 1.5 and earlier # docker pre-1.6.0 did not properly set the user agent on ping, catch \"Go *\" user agents if ($http_user_agent ~ \"^(docker\\/1\\.(3|4|5(?!\\.[0-9]-dev))|Go ).*$\" ) { return 404; } auth_ldap \"Forbidden\"; auth_ldap_servers ldapserver; add_header 'Docker-Distribution-Api-Version' 'registry/2.0' always; proxy_pass http://docker-registry; proxy_set_header Host $http_host; # required for docker client's sake proxy_set_header X-Real-IP $remote_addr; # pass on real client's IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_read_timeout 900; } } server { listen 8080; error_log /var/log/nginx/error.log debug; access_log /var/log/nginx/access.log; client_max_body_size 0; chunked_transfer_encoding on; location / { auth_ldap \"Forbidden\"; auth_ldap_servers ldapserver; proxy_pass http://localhost:8081; } } } ","date":"2019-07-22","objectID":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/:3:4","tags":["Deployment"],"title":"带LDAP认证与WebUI的Docker私有仓库搭建","uri":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/"},{"categories":["Docker"],"content":"3.5 配置Nginx服务管理与自启动 编写服务/lib/systemd/system/nginx.service [Unit] Description=nginx - high performance web server Documentation=http://nginx.org/en/docs/ After=network.target [Service] Type=forking ExecStartPre=/usr/local/nginx/nginx -t -c /etc/nginx/nginx.conf ExecStart=/usr/local/nginx/nginx -c /etc/nginx/nginx.conf ExecReload=/usr/local/nginx/nginx -s reload ExecStop=/usr/local/nginx/nginx -s quit PrivateTmp=true [Install] WantedBy=multi-user.target 启动Nginx服务并设置自启动 sudo systemctl daemon-reload sudo systemctl start nginx sudo systemctl status nginx sudo systemctl enable nginx ","date":"2019-07-22","objectID":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/:3:5","tags":["Deployment"],"title":"带LDAP认证与WebUI的Docker私有仓库搭建","uri":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/"},{"categories":["Docker"],"content":"4. 客户端配置 docker仅支持https的仓库，在内网中无需使用https，因此需要将仓库加入docker的白名单。 ","date":"2019-07-22","objectID":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/:4:0","tags":["Deployment"],"title":"带LDAP认证与WebUI的Docker私有仓库搭建","uri":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/"},{"categories":["Docker"],"content":"4.1 编写/etc/docker/daemon.json { \"insecure-registries\":[ \"barricade.ivic.org.cn\" ] } ","date":"2019-07-22","objectID":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/:4:1","tags":["Deployment"],"title":"带LDAP认证与WebUI的Docker私有仓库搭建","uri":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/"},{"categories":["Docker"],"content":"4.2 重启docker服务 sudo systemctl restart docker ","date":"2019-07-22","objectID":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/:4:2","tags":["Deployment"],"title":"带LDAP认证与WebUI的Docker私有仓库搭建","uri":"/posts/deployment/private-docker-registry-with-ldap-auth-and-webui/"},{"categories":["CCF CSP"],"content":"CCF CSP 201312 T1 出现次数最多的数 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t1/:0:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T1 出现次数最多的数","uri":"/posts/csp/csp-201312-t1/"},{"categories":["CCF CSP"],"content":"题目 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t1/:1:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T1 出现次数最多的数","uri":"/posts/csp/csp-201312-t1/"},{"categories":["CCF CSP"],"content":"问题描述 给定n个正整数，找出它们中出现次数最多的数。如果这样的数有多个，请输出其中最小的一个。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t1/:1:1","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T1 出现次数最多的数","uri":"/posts/csp/csp-201312-t1/"},{"categories":["CCF CSP"],"content":"输入格式 输入的第一行只有一个正整数n(1 ≤ n ≤ 1000)，表示数字的个数。 输入的第二行有n个整数s1, s2, …, sn (1 ≤ si ≤ 10000, 1 ≤ i ≤ n)。相邻的数用空格分隔。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t1/:1:2","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T1 出现次数最多的数","uri":"/posts/csp/csp-201312-t1/"},{"categories":["CCF CSP"],"content":"输出格式 输出这n个次数中出现次数最多的数。如果这样的数有多个，输出其中最小的一个。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t1/:1:3","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T1 出现次数最多的数","uri":"/posts/csp/csp-201312-t1/"},{"categories":["CCF CSP"],"content":"样例输入 6 10 1 10 20 30 20 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t1/:1:4","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T1 出现次数最多的数","uri":"/posts/csp/csp-201312-t1/"},{"categories":["CCF CSP"],"content":"样例输出 10 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t1/:1:5","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T1 出现次数最多的数","uri":"/posts/csp/csp-201312-t1/"},{"categories":["CCF CSP"],"content":"时间限制 1.0s ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t1/:1:6","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T1 出现次数最多的数","uri":"/posts/csp/csp-201312-t1/"},{"categories":["CCF CSP"],"content":"内存限制 256.0MB ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t1/:1:7","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T1 出现次数最多的数","uri":"/posts/csp/csp-201312-t1/"},{"categories":["CCF CSP"],"content":"题解 使用map统计数字出现次数。遍历map找到出现次数最多的最小的数即可。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t1/:2:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T1 出现次数最多的数","uri":"/posts/csp/csp-201312-t1/"},{"categories":["CCF CSP"],"content":"代码 #include \u003ciostream\u003e#include \u003cmap\u003e using namespace std; map\u003cint, int\u003e cnt; int main() { int n; int maxcnt = 0, ans = 0; cnt.clear(); cin \u003e\u003e n; while (n--) { int x; cin \u003e\u003e x; cnt[x]++; for (map\u003cint, int\u003e::iterator it = cnt.begin(); it != cnt.end(); it++) { if (it-\u003esecond \u003e maxcnt) { maxcnt = it-\u003esecond; ans = it-\u003efirst; } else if (it-\u003esecond == maxcnt \u0026\u0026 it-\u003efirst \u003c ans) { ans = it-\u003efirst; } } } cout \u003c\u003c ans; return 0; } ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t1/:3:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T1 出现次数最多的数","uri":"/posts/csp/csp-201312-t1/"},{"categories":["CCF CSP"],"content":"CCF CSP 201312 T2 ISBN号码 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t2/:0:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T2 ISBN号码","uri":"/posts/csp/csp-201312-t2/"},{"categories":["CCF CSP"],"content":"题目 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t2/:1:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T2 ISBN号码","uri":"/posts/csp/csp-201312-t2/"},{"categories":["CCF CSP"],"content":"问题描述 每一本正式出版的图书都有一个ISBN号码与之对应，ISBN码包括9位数字、1位识别码和3位分隔符，其规定格式如“x-xxx-xxxxx-x”，其中符号“-”是分隔符（键盘上的减号），最后一位是识别码，例如0-670-82162-4就是一个标准的ISBN码。ISBN码的首位数字表示书籍的出版语言，例如0代表英语；第一个分隔符“-”之后的三位数字代表出版社，例如670代表维京出版社；第二个分隔之后的五位数字代表该书在出版社的编号；最后一位为识别码。 识别码的计算方法如下： 首位数字乘以1加上次位数字乘以2……以此类推，用所得的结果mod 11，所得的余数即为识别码，如果余数为10，则识别码为大写字母X。例如ISBN号码0-670-82162-4中的识别码4是这样得到的：对067082162这9个数字，从左至右，分别乘以1，2，…，9，再求和，即0×1+6×2+……+2×9=158，然后取158 mod 11的结果4作为识别码。 编写程序判断输入的ISBN号码中识别码是否正确，如果正确，则仅输出“Right”；如果错误，则输出是正确的ISBN号码。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t2/:1:1","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T2 ISBN号码","uri":"/posts/csp/csp-201312-t2/"},{"categories":["CCF CSP"],"content":"输入格式 输入只有一行，是一个字符序列，表示一本书的ISBN号码（保证输入符合ISBN号码的格式要求）。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t2/:1:2","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T2 ISBN号码","uri":"/posts/csp/csp-201312-t2/"},{"categories":["CCF CSP"],"content":"输出格式 输出一行，假如输入的ISBN号码的识别码正确，那么输出“Right”，否则，按照规定的格式，输出正确的ISBN号码（包括分隔符“-”）。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t2/:1:3","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T2 ISBN号码","uri":"/posts/csp/csp-201312-t2/"},{"categories":["CCF CSP"],"content":"样例输入 0-670-82162-4 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t2/:1:4","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T2 ISBN号码","uri":"/posts/csp/csp-201312-t2/"},{"categories":["CCF CSP"],"content":"样例输出 Right ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t2/:1:5","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T2 ISBN号码","uri":"/posts/csp/csp-201312-t2/"},{"categories":["CCF CSP"],"content":"样例输入 0-670-82162-0 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t2/:1:6","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T2 ISBN号码","uri":"/posts/csp/csp-201312-t2/"},{"categories":["CCF CSP"],"content":"样例输出 0-670-82162-4 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t2/:1:7","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T2 ISBN号码","uri":"/posts/csp/csp-201312-t2/"},{"categories":["CCF CSP"],"content":"时间限制 1.0s ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t2/:1:8","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T2 ISBN号码","uri":"/posts/csp/csp-201312-t2/"},{"categories":["CCF CSP"],"content":"内存限制 256.0MB ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t2/:1:9","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T2 ISBN号码","uri":"/posts/csp/csp-201312-t2/"},{"categories":["CCF CSP"],"content":"题解 遍历输入的字符串进行处理即可。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t2/:2:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T2 ISBN号码","uri":"/posts/csp/csp-201312-t2/"},{"categories":["CCF CSP"],"content":"代码 #include \u003ciostream\u003e#include \u003cstring\u003e using namespace std; int main() { string isbn; cin \u003e\u003e isbn; int sum = 0; int i = 0; for (char c : isbn) { if (c == '-') continue; i++; if (i == 10) { if ((sum \u003c 10 \u0026\u0026 c - '0' == sum) || (sum == 10 \u0026\u0026 c == 'X')) { cout \u003c\u003c \"Right\"; break; } cout \u003c\u003c isbn.substr(0, 12); if (sum \u003c 10) cout \u003c\u003c sum; else cout \u003c\u003c \"X\"; break; } sum += (c - '0') * i; sum %= 11; } return 0; } ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t2/:3:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T2 ISBN号码","uri":"/posts/csp/csp-201312-t2/"},{"categories":["CCF CSP"],"content":"CCF CSP 201312 T3 最大的矩形 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t3/:0:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T3 最大的矩形","uri":"/posts/csp/csp-201312-t3/"},{"categories":["CCF CSP"],"content":"题目 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t3/:1:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T3 最大的矩形","uri":"/posts/csp/csp-201312-t3/"},{"categories":["CCF CSP"],"content":"问题描述 在横轴上放了n个相邻的矩形，每个矩形的宽度是1，而第i（1 ≤ i ≤ n）个矩形的高度是hi。这n个矩形构成了一个直方图。例如，下图中六个矩形的高度就分别是3, 1, 6, 5, 2, 3。 Figure 1Figure 1 \" Figure 1 请找出能放在给定直方图里面积最大的矩形，它的边要与坐标轴平行。对于上面给出的例子，最大矩形如下图所示的阴影部分，面积是10。 Figure 2Figure 2 \" Figure 2 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t3/:1:1","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T3 最大的矩形","uri":"/posts/csp/csp-201312-t3/"},{"categories":["CCF CSP"],"content":"输入格式 第一行包含一个整数n，即矩形的数量(1 ≤ n ≤ 1000)。 第二行包含n 个整数h1, h2, … , hn，相邻的数之间由空格分隔。(1 ≤ hi ≤ 10000)。hi是第i个矩形的高度。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t3/:1:2","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T3 最大的矩形","uri":"/posts/csp/csp-201312-t3/"},{"categories":["CCF CSP"],"content":"输出格式 输出一行，包含一个整数，即给定直方图内的最大矩形的面积。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t3/:1:3","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T3 最大的矩形","uri":"/posts/csp/csp-201312-t3/"},{"categories":["CCF CSP"],"content":"样例输入 6 3 1 6 5 2 3 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t3/:1:4","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T3 最大的矩形","uri":"/posts/csp/csp-201312-t3/"},{"categories":["CCF CSP"],"content":"样例输出 10 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t3/:1:5","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T3 最大的矩形","uri":"/posts/csp/csp-201312-t3/"},{"categories":["CCF CSP"],"content":"时间限制 1.0s ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t3/:1:6","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T3 最大的矩形","uri":"/posts/csp/csp-201312-t3/"},{"categories":["CCF CSP"],"content":"内存限制 256.0MB ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t3/:1:7","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T3 最大的矩形","uri":"/posts/csp/csp-201312-t3/"},{"categories":["CCF CSP"],"content":"题解 动态规划题。 定义f[i][j]为以第i个矩形为左边界、第j个矩形为右边界的最大矩形面积。显然，当左右边界确定时，最大矩形面积由从左边界到右边界中最低的矩形高度决定。若定义minh[i][j]为左边界到右边界中最低的矩形高度，则有f[i][j] = minh[i][j] * (j-i+1)。因此此题只需要统计举行高度h[i]在任意区间的最小值。 数组任意区间最小值有很多方法统计，对于静态的值我们可以直接使用动态规划的方法在$O(n^2)$的时间复杂度内统计，根据本题的数据规模与时间限制来看是可以接受的。 $$\\text{minh}[i][j]=\\begin{cases} \\text{h}[i] \u0026 \\text{if }i=j \\\\ \\min(\\text{minh}[i][j-1],\\text{h}[j]) \u0026 \\text{if }i\\lt j \\end{cases}$$ 根据动态转移方程可以看出我们可以在读入的同时求解，只需要扫描一次，降低常数。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t3/:2:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T3 最大的矩形","uri":"/posts/csp/csp-201312-t3/"},{"categories":["CCF CSP"],"content":"代码 #include \u003ciostream\u003e#include \u003ccstring\u003e #define N 1050 using namespace std; int h[N]; int minh[N][N]; int main() { memset(minh, 0x7f, sizeof(minh)); int n, ans = 0; cin \u003e\u003e n; for (int j = 0; j \u003c n; j++) { cin \u003e\u003e h[j]; minh[j][j] = h[j]; if (minh[j][j] \u003e ans) ans = minh[j][j]; for (int i = 0; i \u003c j; i++) { minh[i][j] = min(minh[i][j-1], h[j]); if (minh[i][j] * (j - i + 1) \u003e ans) ans = minh[i][j] * (j - i + 1); } } cout \u003c\u003c ans; return 0; } ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t3/:3:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T3 最大的矩形","uri":"/posts/csp/csp-201312-t3/"},{"categories":["CCF CSP"],"content":"CCF CSP 201312 T4 有趣的数 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t4/:0:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T4 有趣的数","uri":"/posts/csp/csp-201312-t4/"},{"categories":["CCF CSP"],"content":"题目 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t4/:1:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T4 有趣的数","uri":"/posts/csp/csp-201312-t4/"},{"categories":["CCF CSP"],"content":"问题描述 我们把一个数称为有趣的，当且仅当： 它的数字只包含0, 1, 2, 3，且这四个数字都出现过至少一次。 所有的0都出现在所有的1之前，而所有的2都出现在所有的3之前。 最高位数字不为0。 因此，符合我们定义的最小的有趣的数是2013。除此以外，4位的有趣的数还有两个：2031和2301。 请计算恰好有n位的有趣的数的个数。由于答案可能非常大，只需要输出答案除以1000000007的余数。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t4/:1:1","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T4 有趣的数","uri":"/posts/csp/csp-201312-t4/"},{"categories":["CCF CSP"],"content":"输入格式 输入只有一行，包括恰好一个正整数n (4 ≤ n ≤ 1000)。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t4/:1:2","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T4 有趣的数","uri":"/posts/csp/csp-201312-t4/"},{"categories":["CCF CSP"],"content":"输出格式 输出只有一行，包括恰好n 位的整数中有趣的数的个数除以1000000007的余数。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t4/:1:3","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T4 有趣的数","uri":"/posts/csp/csp-201312-t4/"},{"categories":["CCF CSP"],"content":"样例输入 4 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t4/:1:4","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T4 有趣的数","uri":"/posts/csp/csp-201312-t4/"},{"categories":["CCF CSP"],"content":"样例输出 3 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t4/:1:5","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T4 有趣的数","uri":"/posts/csp/csp-201312-t4/"},{"categories":["CCF CSP"],"content":"时间限制 1.0s ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t4/:1:6","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T4 有趣的数","uri":"/posts/csp/csp-201312-t4/"},{"categories":["CCF CSP"],"content":"内存限制 256.0MB ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t4/:1:7","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T4 有趣的数","uri":"/posts/csp/csp-201312-t4/"},{"categories":["CCF CSP"],"content":"题解 递推问题。 首先观察规则，我们可以总结出如下的规则： 首位必须为2； 所有0都在任意1前； 所有2都在任意3前。 0、1、2、3必须出现至少一次。 在计算时，我们其实只需要考虑前三条规则。 我们设f[i][b0][b1][b2][b3]为长度为i的、满足规则123的有趣的数的个数，其中b0、b1、b2、b3表示0、1、2、3是否在这个数中出现。 例如，“满足规则123的、0、1、2、3都出现的、长度为4的有趣的数的个数为3”可表示为f[4][1][1][1][1]=3。 考虑规则123，动态转移方程可表示为： f[i][0][0][1][0] = 1; f[i][1][0][1][0] = f[i - 1][1][0][1][0] * 2 + f[i - 1][0][0][1][0]; f[i][1][1][1][0] = f[i - 1][1][1][1][0] * 2 + f[i - 1][1][0][1][0]; f[i][0][0][1][1] = f[i - 1][0][0][1][1] + f[i - 1][0][0][1][0]; f[i][1][0][1][1] = f[i - 1][1][0][1][1] * 2 + f[i - 1][0][0][1][1] + f[i - 1][1][0][1][0]; f[i][1][1][1][1] = f[i - 1][1][1][1][1] * 2 + f[i - 1][1][0][1][1] + f[i - 1][1][1][1][0]; 其余均为0。 可见，其实有效的状态只有6种，可以按照6中状态进行压缩，由于( 我懒 )本题空间足够，也可以不压缩。 最后输出的答案即f[n][1][1][1][1]。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t4/:2:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T4 有趣的数","uri":"/posts/csp/csp-201312-t4/"},{"categories":["CCF CSP"],"content":"代码 #include \u003ciostream\u003e#include \u003ccstring\u003e #define N 1050 #define ll long long using namespace std; ll f[N][2][2][2][2]; ll m = 1000000007; int main() { int n; memset(f, 0, sizeof(f)); cin \u003e\u003e n; f[1][0][0][1][0] = 1; for (int i = 2; i \u003c= n; i++) { f[i][0][0][1][0] = 1; f[i][1][0][1][0] = ((f[i - 1][1][0][1][0] * 2) % m + f[i - 1][0][0][1][0]) % m; f[i][1][1][1][0] = ((f[i - 1][1][1][1][0] * 2) % m + f[i - 1][1][0][1][0]) % m; f[i][0][0][1][1] = (f[i - 1][0][0][1][1] + f[i - 1][0][0][1][0]) % m; f[i][1][0][1][1] = ((f[i - 1][1][0][1][1] * 2) % m + f[i - 1][0][0][1][1] + f[i - 1][1][0][1][0]) % m; f[i][1][1][1][1] = ((f[i - 1][1][1][1][1] * 2) % m + f[i - 1][1][0][1][1] + f[i - 1][1][1][1][0]) % m; } cout \u003c\u003c f[n][1][1][1][1]; return 0; } ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t4/:3:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T4 有趣的数","uri":"/posts/csp/csp-201312-t4/"},{"categories":["CCF CSP"],"content":"CCF CSP 201312 T5 I’m stuck! ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t5/:0:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T5 I'm stuck!","uri":"/posts/csp/csp-201312-t5/"},{"categories":["CCF CSP"],"content":"题目 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t5/:1:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T5 I'm stuck!","uri":"/posts/csp/csp-201312-t5/"},{"categories":["CCF CSP"],"content":"问题描述 给定一个R行C列的地图，地图的每一个方格可能是’#', ‘+’, ‘-', ‘|’, ‘.', ‘S’, ‘T'七个字符中的一个，分别表示如下意思： ‘#': 任何时候玩家都不能移动到此方格； ‘+’: 当玩家到达这一方格后，下一步可以向上下左右四个方向相邻的任意一个非’#‘方格移动一格； ‘-': 当玩家到达这一方格后，下一步可以向左右两个方向相邻的一个非’#‘方格移动一格； ‘|’: 当玩家到达这一方格后，下一步可以向上下两个方向相邻的一个非’#‘方格移动一格； ‘.': 当玩家到达这一方格后，下一步只能向下移动一格。如果下面相邻的方格为’#'，则玩家不能再移动； ‘S’: 玩家的初始位置，地图中只会有一个初始位置。玩家到达这一方格后，下一步可以向上下左右四个方向相邻的任意一个非’#‘方格移动一格； ‘T’: 玩家的目标位置，地图中只会有一个目标位置。玩家到达这一方格后，可以选择完成任务，也可以选择不完成任务继续移动。如果继续移动下一步可以向上下左右四个方向相邻的任意一个非’#‘方格移动一格。 此外，玩家不能移动出地图。 请找出满足下面两个性质的方格个数： 玩家可以从初始位置移动到此方格； 玩家不可以从此方格移动到目标位置。 它的数字只包含0, 1, 2, 3，且这四个数字都出现过至少一次。 所有的0都出现在所有的1之前，而所有的2都出现在所有的3之前。 最高位数字不为0。 因此，符合我们定义的最小的有趣的数是2013。除此以外，4位的有趣的数还有两个：2031和2301。 请计算恰好有n位的有趣的数的个数。由于答案可能非常大，只需要输出答案除以1000000007的余数。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t5/:1:1","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T5 I'm stuck!","uri":"/posts/csp/csp-201312-t5/"},{"categories":["CCF CSP"],"content":"输入格式 输入的第一行包括两个整数R 和C，分别表示地图的行和列数。(1 ≤ R, C ≤ 50)。 接下来的R行每行都包含C个字符。它们表示地图的格子。地图上恰好有一个’S'和一个’T’。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t5/:1:2","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T5 I'm stuck!","uri":"/posts/csp/csp-201312-t5/"},{"categories":["CCF CSP"],"content":"输出格式 如果玩家在初始位置就已经不能到达终点了，就输出“I’m stuck!”（不含双引号）。否则的话，输出满足性质的方格的个数。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t5/:1:3","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T5 I'm stuck!","uri":"/posts/csp/csp-201312-t5/"},{"categories":["CCF CSP"],"content":"样例输入 5 5 --+-+ ..|#. ..|## S-+-T ####. ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t5/:1:4","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T5 I'm stuck!","uri":"/posts/csp/csp-201312-t5/"},{"categories":["CCF CSP"],"content":"样例输出 2 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t5/:1:5","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T5 I'm stuck!","uri":"/posts/csp/csp-201312-t5/"},{"categories":["CCF CSP"],"content":"样例说明 如果把满足性质的方格在地图上用’X'标记出来的话，地图如下所示： --+-+ ..|#X ..|## S-+-T ####X ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t5/:1:6","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T5 I'm stuck!","uri":"/posts/csp/csp-201312-t5/"},{"categories":["CCF CSP"],"content":"时间限制 1.0s ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t5/:1:7","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T5 I'm stuck!","uri":"/posts/csp/csp-201312-t5/"},{"categories":["CCF CSP"],"content":"内存限制 256.0MB ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t5/:1:8","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T5 I'm stuck!","uri":"/posts/csp/csp-201312-t5/"},{"categories":["CCF CSP"],"content":"题解 小模拟题。 按照规则正向遍历再反向遍历即可。 $$ \\lbrace \\text{满足性质的点集} \\rbrace = \\lbrace \\text{正向遍历可达的点集} \\rbrace - \\lbrace \\text{反向遍历可达的点集} \\rbrace $$ 需要注意的是，反向遍历时检查的是相邻点的类型而不是当前点。这样代码可能稍微有点复杂( 比如下面我的 )，可能直接写成图能更好复用代码一点。 ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t5/:2:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T5 I'm stuck!","uri":"/posts/csp/csp-201312-t5/"},{"categories":["CCF CSP"],"content":"代码 #include \u003ciostream\u003e#include \u003cqueue\u003e#include \u003cvector\u003e#include \u003ccstring\u003e#include \u003cset\u003e #define N 55 using namespace std; struct pos { int r, c; bool operator\u003c(const pos \u0026x) const { return (this-\u003er == x.r) ? ( this-\u003ec \u003c x.c) : ( this-\u003er \u003c x.r); } }; struct forwR { bool stuck; }; char m[N][N]; set\u003cpos\u003e *forw(int r, int c, pos s, pos t) { bool b[N][N]; queue\u003cpos\u003e q; memset(b, 0, sizeof(b)); while (!q.empty()) q.pop(); q.push(s); b[s.r][s.c] = true; while (!q.empty()) { pos cur = q.front(); q.pop(); vector\u003cpos\u003e rp; rp.clear(); switch (m[cur.r][cur.c]) { case '+': case 'S': case 'T': rp = { {0, -1}, {0, 1}, {-1, 0}, {1, 0}}; break; case '-': rp = { {0, -1}, {0, 1}}; break; case '|': rp = { {1, 0}, {-1, 0}}; break; case '.': rp = { {1, 0}}; break; } for (pos dp : rp) if (m[cur.r + dp.r][cur.c + dp.c] != '#' \u0026\u0026 !b[cur.r + dp.r][cur.c + dp.c]) { q.push(pos{cur.r + dp.r, cur.c + dp.c}); b[cur.r + dp.r][cur.c + dp.c] = true; } } if (!b[t.r][t.c]) return NULL; set\u003cpos\u003e *result = new set\u003cpos\u003e(); result-\u003eclear(); for (int ir = 1; ir \u003c= r; ir++) { for (int ic = 1; ic \u003c= c; ic++) { if (b[ir][ic]) result-\u003einsert(pos{ir, ic}); } } return result; } set\u003cpos\u003e rev(int r, int c, pos s) { bool b[N][N]; queue\u003cpos\u003e q; memset(b, 0, sizeof(b)); while (!q.empty()) q.pop(); q.push(s); b[s.r][s.c] = true; while (!q.empty()) { pos cur = q.front(); q.pop(); if (!b[cur.r - 1][cur.c] \u0026\u0026 (m[cur.r - 1][cur.c] == '.' || m[cur.r - 1][cur.c] == '|' || m[cur.r - 1][cur.c] == '+' || m[cur.r - 1][cur.c] == 'S' || m[cur.r - 1][cur.c] == 'T')) { q.push({cur.r - 1, cur.c}); b[cur.r - 1][cur.c] = true; } if (!b[cur.r + 1][cur.c] \u0026\u0026 (m[cur.r + 1][cur.c] == '|' || m[cur.r + 1][cur.c] == '+' || m[cur.r + 1][cur.c] == 'S' || m[cur.r + 1][cur.c] == 'T')) { q.push({cur.r + 1, cur.c}); b[cur.r + 1][cur.c] = true; } if (!b[cur.r][cur.c - 1] \u0026\u0026 (m[cur.r][cur.c - 1] == '-' || m[cur.r][cur.c - 1] == '+' || m[cur.r][cur.c - 1] == 'S' || m[cur.r][cur.c - 1] == 'T')) { q.push({cur.r, cur.c - 1}); b[cur.r][cur.c - 1] = true; } if (!b[cur.r][cur.c + 1] \u0026\u0026 (m[cur.r][cur.c + 1] == '-' || m[cur.r][cur.c + 1] == '+' || m[cur.r][cur.c + 1] == 'S' || m[cur.r][cur.c + 1] == 'T')) { q.push({cur.r, cur.c + 1}); b[cur.r][cur.c + 1] = true; } } set\u003cpos\u003e result; result.clear(); for (int i = 1; i \u003c= r; i++) for (int j = 1; j \u003c= c; j++) { if (b[i][j]) result.insert({i, j}); } return result; } int main() { int r, c; cin \u003e\u003e r \u003e\u003e c; pos s, t; for (int i = 0; i \u003c N; i++) for (int j = 0; j \u003c N; j++) { m[i][j] = '#'; } for (int i = 1; i \u003c= r; i++) for (int j = 1; j \u003c= c; j++) { cin \u003e\u003e m[i][j]; if (m[i][j] == 'S') s = pos{i, j}; else if (m[i][j] == 'T') t = pos{i, j}; } set\u003cpos\u003e *pre = forw(r, c, s, t); if (pre == NULL) { cout \u003c\u003c \"I'm stuck!\"; return 0; } set\u003cpos\u003e post = rev(r, c, t); int ans = 0; for (pos p : *pre) { if (post.find(p) == end(post)) ans++; } cout \u003c\u003c ans; return 0; } ","date":"2019-07-10","objectID":"/posts/csp/csp-201312-t5/:3:0","tags":["CCF CSP","Algorithm"],"title":"CSP 201312 T5 I'm stuck!","uri":"/posts/csp/csp-201312-t5/"}]